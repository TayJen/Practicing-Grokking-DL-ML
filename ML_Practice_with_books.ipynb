{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.30250000000000005 Prediction: 0.25\n",
      "Error: 0.3019502500000001 Prediction: 0.2505\n",
      "Error: 0.30140100000000003 Prediction: 0.251\n",
      "Error: 0.30085225 Prediction: 0.2515\n",
      "Error: 0.30030400000000007 Prediction: 0.252\n",
      "Error: 0.2997562500000001 Prediction: 0.2525\n",
      "Error: 0.29920900000000006 Prediction: 0.253\n",
      "Error: 0.29866224999999996 Prediction: 0.2535\n",
      "Error: 0.29811600000000005 Prediction: 0.254\n",
      "Error: 0.2975702500000001 Prediction: 0.2545\n",
      "Error: 0.29702500000000004 Prediction: 0.255\n",
      "Error: 0.29648025 Prediction: 0.2555\n",
      "Error: 0.29593600000000003 Prediction: 0.256\n",
      "Error: 0.2953922500000001 Prediction: 0.2565\n",
      "Error: 0.294849 Prediction: 0.257\n",
      "Error: 0.29430625 Prediction: 0.2575\n",
      "Error: 0.293764 Prediction: 0.258\n",
      "Error: 0.2932222500000001 Prediction: 0.2585\n",
      "Error: 0.292681 Prediction: 0.259\n",
      "Error: 0.29214025 Prediction: 0.2595\n",
      "Error: 0.2916 Prediction: 0.26\n",
      "Error: 0.2910602500000001 Prediction: 0.2605\n",
      "Error: 0.29052100000000003 Prediction: 0.261\n",
      "Error: 0.28998225 Prediction: 0.2615\n",
      "Error: 0.28944400000000003 Prediction: 0.262\n",
      "Error: 0.2889062500000001 Prediction: 0.2625\n",
      "Error: 0.28836900000000004 Prediction: 0.263\n",
      "Error: 0.28783224999999996 Prediction: 0.2635\n",
      "Error: 0.28729600000000005 Prediction: 0.264\n",
      "Error: 0.2867602500000001 Prediction: 0.2645\n",
      "Error: 0.286225 Prediction: 0.265\n",
      "Error: 0.28569025 Prediction: 0.2655\n",
      "Error: 0.285156 Prediction: 0.266\n",
      "Error: 0.2846222500000001 Prediction: 0.2665\n",
      "Error: 0.28408900000000004 Prediction: 0.267\n",
      "Error: 0.28355624999999995 Prediction: 0.2675\n",
      "Error: 0.28302400000000005 Prediction: 0.268\n",
      "Error: 0.2824922500000001 Prediction: 0.2685\n",
      "Error: 0.281961 Prediction: 0.269\n",
      "Error: 0.28143025 Prediction: 0.2695\n",
      "Error: 0.28090000000000004 Prediction: 0.27\n",
      "Error: 0.2803702500000001 Prediction: 0.2705\n",
      "Error: 0.279841 Prediction: 0.271\n",
      "Error: 0.27931225 Prediction: 0.2715\n",
      "Error: 0.27878400000000003 Prediction: 0.272\n",
      "Error: 0.2782562500000001 Prediction: 0.2725\n",
      "Error: 0.277729 Prediction: 0.273\n",
      "Error: 0.27720225 Prediction: 0.2735\n",
      "Error: 0.27667600000000003 Prediction: 0.274\n",
      "Error: 0.2761502500000001 Prediction: 0.2745\n",
      "Error: 0.275625 Prediction: 0.275\n",
      "Error: 0.27510025 Prediction: 0.2755\n",
      "Error: 0.27457600000000004 Prediction: 0.276\n",
      "Error: 0.27405225000000005 Prediction: 0.2765\n",
      "Error: 0.273529 Prediction: 0.277\n",
      "Error: 0.27300624999999995 Prediction: 0.2775\n",
      "Error: 0.272484 Prediction: 0.278\n",
      "Error: 0.27196225000000007 Prediction: 0.2785\n",
      "Error: 0.27144100000000004 Prediction: 0.279\n",
      "Error: 0.27092025 Prediction: 0.2795\n",
      "Error: 0.27040000000000003 Prediction: 0.28\n",
      "Error: 0.2698802500000001 Prediction: 0.2805\n",
      "Error: 0.269361 Prediction: 0.281\n",
      "Error: 0.26884224999999995 Prediction: 0.28150000000000003\n",
      "Error: 0.268324 Prediction: 0.28200000000000003\n",
      "Error: 0.2678062500000001 Prediction: 0.28250000000000003\n",
      "Error: 0.267289 Prediction: 0.28300000000000003\n",
      "Error: 0.26677224999999993 Prediction: 0.28350000000000003\n",
      "Error: 0.266256 Prediction: 0.28400000000000003\n",
      "Error: 0.26574025000000007 Prediction: 0.28450000000000003\n",
      "Error: 0.265225 Prediction: 0.28500000000000003\n",
      "Error: 0.26471025 Prediction: 0.28550000000000003\n",
      "Error: 0.264196 Prediction: 0.28600000000000003\n",
      "Error: 0.26368225000000006 Prediction: 0.28650000000000003\n",
      "Error: 0.263169 Prediction: 0.28700000000000003\n",
      "Error: 0.26265625 Prediction: 0.28750000000000003\n",
      "Error: 0.262144 Prediction: 0.28800000000000003\n",
      "Error: 0.26163225000000007 Prediction: 0.28850000000000003\n",
      "Error: 0.261121 Prediction: 0.28900000000000003\n",
      "Error: 0.26061024999999993 Prediction: 0.28950000000000004\n",
      "Error: 0.2601 Prediction: 0.29000000000000004\n",
      "Error: 0.2595902500000001 Prediction: 0.29050000000000004\n",
      "Error: 0.259081 Prediction: 0.29100000000000004\n",
      "Error: 0.25857224999999995 Prediction: 0.29150000000000004\n",
      "Error: 0.258064 Prediction: 0.29200000000000004\n",
      "Error: 0.25755625000000004 Prediction: 0.29250000000000004\n",
      "Error: 0.257049 Prediction: 0.29300000000000004\n",
      "Error: 0.25654224999999997 Prediction: 0.29350000000000004\n",
      "Error: 0.256036 Prediction: 0.29400000000000004\n",
      "Error: 0.25553025000000007 Prediction: 0.29450000000000004\n",
      "Error: 0.255025 Prediction: 0.29500000000000004\n",
      "Error: 0.25452024999999995 Prediction: 0.29550000000000004\n",
      "Error: 0.254016 Prediction: 0.29600000000000004\n",
      "Error: 0.25351225000000005 Prediction: 0.29650000000000004\n",
      "Error: 0.253009 Prediction: 0.29700000000000004\n",
      "Error: 0.25250624999999993 Prediction: 0.29750000000000004\n",
      "Error: 0.252004 Prediction: 0.29800000000000004\n",
      "Error: 0.25150225000000004 Prediction: 0.29850000000000004\n",
      "Error: 0.251001 Prediction: 0.29900000000000004\n",
      "Error: 0.2505002499999999 Prediction: 0.29950000000000004\n",
      "Error: 0.25 Prediction: 0.30000000000000004\n",
      "Error: 0.24950025 Prediction: 0.30050000000000004\n",
      "Error: 0.249001 Prediction: 0.30100000000000005\n",
      "Error: 0.24850225 Prediction: 0.30150000000000005\n",
      "Error: 0.248004 Prediction: 0.30200000000000005\n",
      "Error: 0.24750625 Prediction: 0.30250000000000005\n",
      "Error: 0.247009 Prediction: 0.30300000000000005\n",
      "Error: 0.24651225 Prediction: 0.30350000000000005\n",
      "Error: 0.24601599999999998 Prediction: 0.30400000000000005\n",
      "Error: 0.24552025 Prediction: 0.30450000000000005\n",
      "Error: 0.245025 Prediction: 0.30500000000000005\n",
      "Error: 0.24453025 Prediction: 0.30550000000000005\n",
      "Error: 0.244036 Prediction: 0.30600000000000005\n",
      "Error: 0.24354225 Prediction: 0.30650000000000005\n",
      "Error: 0.243049 Prediction: 0.30700000000000005\n",
      "Error: 0.24255625 Prediction: 0.30750000000000005\n",
      "Error: 0.242064 Prediction: 0.30800000000000005\n",
      "Error: 0.24157225 Prediction: 0.30850000000000005\n",
      "Error: 0.241081 Prediction: 0.30900000000000005\n",
      "Error: 0.24059025 Prediction: 0.30950000000000005\n",
      "Error: 0.24009999999999998 Prediction: 0.31000000000000005\n",
      "Error: 0.23961025 Prediction: 0.31050000000000005\n",
      "Error: 0.239121 Prediction: 0.31100000000000005\n",
      "Error: 0.23863225 Prediction: 0.31150000000000005\n",
      "Error: 0.238144 Prediction: 0.31200000000000006\n",
      "Error: 0.23765624999999999 Prediction: 0.31250000000000006\n",
      "Error: 0.237169 Prediction: 0.31300000000000006\n",
      "Error: 0.23668224999999998 Prediction: 0.31350000000000006\n",
      "Error: 0.236196 Prediction: 0.31400000000000006\n",
      "Error: 0.23571024999999998 Prediction: 0.31450000000000006\n",
      "Error: 0.235225 Prediction: 0.31500000000000006\n",
      "Error: 0.23474024999999998 Prediction: 0.31550000000000006\n",
      "Error: 0.234256 Prediction: 0.31600000000000006\n",
      "Error: 0.23377225 Prediction: 0.31650000000000006\n",
      "Error: 0.233289 Prediction: 0.31700000000000006\n",
      "Error: 0.23280625 Prediction: 0.31750000000000006\n",
      "Error: 0.23232399999999997 Prediction: 0.31800000000000006\n",
      "Error: 0.23184224999999997 Prediction: 0.31850000000000006\n",
      "Error: 0.23136099999999998 Prediction: 0.31900000000000006\n",
      "Error: 0.23088024999999998 Prediction: 0.31950000000000006\n",
      "Error: 0.2304 Prediction: 0.32000000000000006\n",
      "Error: 0.22992025 Prediction: 0.32050000000000006\n",
      "Error: 0.22944099999999998 Prediction: 0.32100000000000006\n",
      "Error: 0.22896224999999998 Prediction: 0.32150000000000006\n",
      "Error: 0.228484 Prediction: 0.32200000000000006\n",
      "Error: 0.22800625 Prediction: 0.32250000000000006\n",
      "Error: 0.22752899999999998 Prediction: 0.32300000000000006\n",
      "Error: 0.22705224999999998 Prediction: 0.32350000000000007\n",
      "Error: 0.22657599999999997 Prediction: 0.32400000000000007\n",
      "Error: 0.22610024999999997 Prediction: 0.32450000000000007\n",
      "Error: 0.225625 Prediction: 0.32500000000000007\n",
      "Error: 0.22515024999999997 Prediction: 0.32550000000000007\n",
      "Error: 0.224676 Prediction: 0.32600000000000007\n",
      "Error: 0.22420224999999996 Prediction: 0.32650000000000007\n",
      "Error: 0.22372899999999998 Prediction: 0.32700000000000007\n",
      "Error: 0.22325625 Prediction: 0.32750000000000007\n",
      "Error: 0.22278399999999998 Prediction: 0.32800000000000007\n",
      "Error: 0.22231225 Prediction: 0.32850000000000007\n",
      "Error: 0.22184099999999998 Prediction: 0.32900000000000007\n",
      "Error: 0.22137024999999996 Prediction: 0.32950000000000007\n",
      "Error: 0.22089999999999999 Prediction: 0.33000000000000007\n",
      "Error: 0.22043024999999997 Prediction: 0.33050000000000007\n",
      "Error: 0.21996099999999996 Prediction: 0.33100000000000007\n",
      "Error: 0.21949224999999997 Prediction: 0.3315000000000001\n",
      "Error: 0.21902399999999997 Prediction: 0.3320000000000001\n",
      "Error: 0.21855624999999998 Prediction: 0.3325000000000001\n",
      "Error: 0.21808899999999998 Prediction: 0.3330000000000001\n",
      "Error: 0.21762224999999996 Prediction: 0.3335000000000001\n",
      "Error: 0.21715599999999996 Prediction: 0.3340000000000001\n",
      "Error: 0.21669024999999997 Prediction: 0.3345000000000001\n",
      "Error: 0.21622499999999997 Prediction: 0.3350000000000001\n",
      "Error: 0.21576024999999996 Prediction: 0.3355000000000001\n",
      "Error: 0.21529599999999996 Prediction: 0.3360000000000001\n",
      "Error: 0.21483224999999997 Prediction: 0.3365000000000001\n",
      "Error: 0.21436899999999998 Prediction: 0.3370000000000001\n",
      "Error: 0.21390624999999996 Prediction: 0.3375000000000001\n",
      "Error: 0.21344399999999997 Prediction: 0.3380000000000001\n",
      "Error: 0.21298224999999996 Prediction: 0.3385000000000001\n",
      "Error: 0.21252099999999996 Prediction: 0.3390000000000001\n",
      "Error: 0.21206024999999998 Prediction: 0.3395000000000001\n",
      "Error: 0.21159999999999995 Prediction: 0.3400000000000001\n",
      "Error: 0.21114024999999997 Prediction: 0.3405000000000001\n",
      "Error: 0.21068099999999998 Prediction: 0.3410000000000001\n",
      "Error: 0.21022224999999997 Prediction: 0.3415000000000001\n",
      "Error: 0.20976399999999998 Prediction: 0.3420000000000001\n",
      "Error: 0.20930624999999997 Prediction: 0.3425000000000001\n",
      "Error: 0.20884899999999998 Prediction: 0.3430000000000001\n",
      "Error: 0.20839224999999997 Prediction: 0.3435000000000001\n",
      "Error: 0.20793599999999995 Prediction: 0.3440000000000001\n",
      "Error: 0.20748024999999998 Prediction: 0.3445000000000001\n",
      "Error: 0.20702499999999996 Prediction: 0.3450000000000001\n",
      "Error: 0.20657024999999996 Prediction: 0.3455000000000001\n",
      "Error: 0.20611599999999997 Prediction: 0.3460000000000001\n",
      "Error: 0.20566224999999996 Prediction: 0.3465000000000001\n",
      "Error: 0.20520899999999997 Prediction: 0.3470000000000001\n",
      "Error: 0.20475624999999997 Prediction: 0.3475000000000001\n",
      "Error: 0.20430399999999996 Prediction: 0.3480000000000001\n",
      "Error: 0.20385224999999996 Prediction: 0.3485000000000001\n",
      "Error: 0.20340099999999997 Prediction: 0.3490000000000001\n",
      "Error: 0.20295024999999997 Prediction: 0.3495000000000001\n",
      "Error: 0.20249999999999996 Prediction: 0.3500000000000001\n",
      "Error: 0.20205024999999996 Prediction: 0.3505000000000001\n",
      "Error: 0.20160099999999995 Prediction: 0.3510000000000001\n",
      "Error: 0.20115224999999995 Prediction: 0.3515000000000001\n",
      "Error: 0.20070399999999997 Prediction: 0.3520000000000001\n",
      "Error: 0.20025624999999997 Prediction: 0.3525000000000001\n",
      "Error: 0.19980899999999996 Prediction: 0.3530000000000001\n",
      "Error: 0.19936224999999996 Prediction: 0.3535000000000001\n",
      "Error: 0.19891599999999995 Prediction: 0.3540000000000001\n",
      "Error: 0.19847024999999996 Prediction: 0.3545000000000001\n",
      "Error: 0.19802499999999995 Prediction: 0.3550000000000001\n",
      "Error: 0.19758024999999996 Prediction: 0.3555000000000001\n",
      "Error: 0.19713599999999995 Prediction: 0.3560000000000001\n",
      "Error: 0.19669224999999996 Prediction: 0.3565000000000001\n",
      "Error: 0.19624899999999995 Prediction: 0.3570000000000001\n",
      "Error: 0.19580624999999996 Prediction: 0.3575000000000001\n",
      "Error: 0.19536399999999995 Prediction: 0.3580000000000001\n",
      "Error: 0.19492224999999996 Prediction: 0.3585000000000001\n",
      "Error: 0.19448099999999996 Prediction: 0.3590000000000001\n",
      "Error: 0.19404024999999994 Prediction: 0.3595000000000001\n",
      "Error: 0.19359999999999997 Prediction: 0.3600000000000001\n",
      "Error: 0.19316024999999995 Prediction: 0.3605000000000001\n",
      "Error: 0.19272099999999995 Prediction: 0.3610000000000001\n",
      "Error: 0.19228224999999996 Prediction: 0.3615000000000001\n",
      "Error: 0.19184399999999996 Prediction: 0.3620000000000001\n",
      "Error: 0.19140624999999994 Prediction: 0.3625000000000001\n",
      "Error: 0.19096899999999994 Prediction: 0.3630000000000001\n",
      "Error: 0.19053224999999996 Prediction: 0.3635000000000001\n",
      "Error: 0.19009599999999996 Prediction: 0.3640000000000001\n",
      "Error: 0.18966024999999995 Prediction: 0.3645000000000001\n",
      "Error: 0.18922499999999995 Prediction: 0.3650000000000001\n",
      "Error: 0.18879024999999994 Prediction: 0.3655000000000001\n",
      "Error: 0.18835599999999994 Prediction: 0.3660000000000001\n",
      "Error: 0.18792224999999996 Prediction: 0.3665000000000001\n",
      "Error: 0.18748899999999996 Prediction: 0.3670000000000001\n",
      "Error: 0.18705624999999995 Prediction: 0.3675000000000001\n",
      "Error: 0.18662399999999996 Prediction: 0.3680000000000001\n",
      "Error: 0.18619224999999995 Prediction: 0.3685000000000001\n",
      "Error: 0.18576099999999995 Prediction: 0.3690000000000001\n",
      "Error: 0.18533024999999995 Prediction: 0.3695000000000001\n",
      "Error: 0.18489999999999995 Prediction: 0.3700000000000001\n",
      "Error: 0.18447024999999995 Prediction: 0.3705000000000001\n",
      "Error: 0.18404099999999995 Prediction: 0.3710000000000001\n",
      "Error: 0.18361224999999995 Prediction: 0.3715000000000001\n",
      "Error: 0.18318399999999996 Prediction: 0.3720000000000001\n",
      "Error: 0.18275624999999995 Prediction: 0.3725000000000001\n",
      "Error: 0.18232899999999994 Prediction: 0.3730000000000001\n",
      "Error: 0.18190224999999993 Prediction: 0.3735000000000001\n",
      "Error: 0.18147599999999994 Prediction: 0.3740000000000001\n",
      "Error: 0.18105024999999994 Prediction: 0.3745000000000001\n",
      "Error: 0.18062499999999995 Prediction: 0.3750000000000001\n",
      "Error: 0.18020024999999995 Prediction: 0.3755000000000001\n",
      "Error: 0.17977599999999994 Prediction: 0.3760000000000001\n",
      "Error: 0.17935224999999994 Prediction: 0.3765000000000001\n",
      "Error: 0.17892899999999995 Prediction: 0.3770000000000001\n",
      "Error: 0.17850624999999995 Prediction: 0.3775000000000001\n",
      "Error: 0.17808399999999994 Prediction: 0.3780000000000001\n",
      "Error: 0.17766224999999994 Prediction: 0.3785000000000001\n",
      "Error: 0.17724099999999995 Prediction: 0.3790000000000001\n",
      "Error: 0.17682024999999993 Prediction: 0.3795000000000001\n",
      "Error: 0.17639999999999995 Prediction: 0.3800000000000001\n",
      "Error: 0.17598024999999995 Prediction: 0.3805000000000001\n",
      "Error: 0.17556099999999994 Prediction: 0.3810000000000001\n",
      "Error: 0.17514224999999994 Prediction: 0.3815000000000001\n",
      "Error: 0.17472399999999993 Prediction: 0.3820000000000001\n",
      "Error: 0.17430624999999994 Prediction: 0.3825000000000001\n",
      "Error: 0.17388899999999993 Prediction: 0.3830000000000001\n",
      "Error: 0.17347224999999994 Prediction: 0.3835000000000001\n",
      "Error: 0.17305599999999993 Prediction: 0.3840000000000001\n",
      "Error: 0.17264024999999994 Prediction: 0.3845000000000001\n",
      "Error: 0.17222499999999993 Prediction: 0.3850000000000001\n",
      "Error: 0.17181024999999994 Prediction: 0.3855000000000001\n",
      "Error: 0.17139599999999994 Prediction: 0.3860000000000001\n",
      "Error: 0.17098224999999995 Prediction: 0.3865000000000001\n",
      "Error: 0.17056899999999994 Prediction: 0.3870000000000001\n",
      "Error: 0.17015624999999993 Prediction: 0.3875000000000001\n",
      "Error: 0.16974399999999992 Prediction: 0.3880000000000001\n",
      "Error: 0.16933224999999993 Prediction: 0.3885000000000001\n",
      "Error: 0.16892099999999993 Prediction: 0.3890000000000001\n",
      "Error: 0.16851024999999994 Prediction: 0.3895000000000001\n",
      "Error: 0.16809999999999994 Prediction: 0.3900000000000001\n",
      "Error: 0.16769024999999993 Prediction: 0.3905000000000001\n",
      "Error: 0.16728099999999993 Prediction: 0.3910000000000001\n",
      "Error: 0.16687224999999994 Prediction: 0.3915000000000001\n",
      "Error: 0.16646399999999995 Prediction: 0.3920000000000001\n",
      "Error: 0.16605624999999993 Prediction: 0.3925000000000001\n",
      "Error: 0.16564899999999994 Prediction: 0.3930000000000001\n",
      "Error: 0.16524224999999992 Prediction: 0.3935000000000001\n",
      "Error: 0.16483599999999993 Prediction: 0.39400000000000013\n",
      "Error: 0.16443024999999994 Prediction: 0.39450000000000013\n",
      "Error: 0.16402499999999992 Prediction: 0.39500000000000013\n",
      "Error: 0.16362024999999994 Prediction: 0.39550000000000013\n",
      "Error: 0.16321599999999994 Prediction: 0.39600000000000013\n",
      "Error: 0.16281224999999994 Prediction: 0.39650000000000013\n",
      "Error: 0.16240899999999994 Prediction: 0.39700000000000013\n",
      "Error: 0.16200624999999994 Prediction: 0.39750000000000013\n",
      "Error: 0.16160399999999994 Prediction: 0.39800000000000013\n",
      "Error: 0.16120224999999994 Prediction: 0.39850000000000013\n",
      "Error: 0.16080099999999992 Prediction: 0.39900000000000013\n",
      "Error: 0.16040024999999994 Prediction: 0.39950000000000013\n",
      "Error: 0.15999999999999992 Prediction: 0.40000000000000013\n",
      "Error: 0.15960024999999992 Prediction: 0.40050000000000013\n",
      "Error: 0.15920099999999993 Prediction: 0.40100000000000013\n",
      "Error: 0.15880224999999992 Prediction: 0.40150000000000013\n",
      "Error: 0.15840399999999993 Prediction: 0.40200000000000014\n",
      "Error: 0.15800624999999993 Prediction: 0.40250000000000014\n",
      "Error: 0.15760899999999992 Prediction: 0.40300000000000014\n",
      "Error: 0.15721224999999991 Prediction: 0.40350000000000014\n",
      "Error: 0.15681599999999993 Prediction: 0.40400000000000014\n",
      "Error: 0.15642024999999993 Prediction: 0.40450000000000014\n",
      "Error: 0.1560249999999999 Prediction: 0.40500000000000014\n",
      "Error: 0.15563024999999991 Prediction: 0.40550000000000014\n",
      "Error: 0.15523599999999993 Prediction: 0.40600000000000014\n",
      "Error: 0.15484224999999993 Prediction: 0.40650000000000014\n",
      "Error: 0.15444899999999992 Prediction: 0.40700000000000014\n",
      "Error: 0.15405624999999992 Prediction: 0.40750000000000014\n",
      "Error: 0.1536639999999999 Prediction: 0.40800000000000014\n",
      "Error: 0.15327224999999992 Prediction: 0.40850000000000014\n",
      "Error: 0.15288099999999993 Prediction: 0.40900000000000014\n",
      "Error: 0.1524902499999999 Prediction: 0.40950000000000014\n",
      "Error: 0.15209999999999993 Prediction: 0.41000000000000014\n",
      "Error: 0.15171024999999994 Prediction: 0.41050000000000014\n",
      "Error: 0.15132099999999993 Prediction: 0.41100000000000014\n",
      "Error: 0.15093224999999993 Prediction: 0.41150000000000014\n",
      "Error: 0.15054399999999993 Prediction: 0.41200000000000014\n",
      "Error: 0.15015624999999994 Prediction: 0.41250000000000014\n",
      "Error: 0.14976899999999993 Prediction: 0.41300000000000014\n",
      "Error: 0.1493822499999999 Prediction: 0.41350000000000015\n",
      "Error: 0.14899599999999993 Prediction: 0.41400000000000015\n",
      "Error: 0.14861024999999992 Prediction: 0.41450000000000015\n",
      "Error: 0.1482249999999999 Prediction: 0.41500000000000015\n",
      "Error: 0.14784024999999992 Prediction: 0.41550000000000015\n",
      "Error: 0.14745599999999992 Prediction: 0.41600000000000015\n",
      "Error: 0.14707224999999993 Prediction: 0.41650000000000015\n",
      "Error: 0.14668899999999993 Prediction: 0.41700000000000015\n",
      "Error: 0.14630624999999992 Prediction: 0.41750000000000015\n",
      "Error: 0.14592399999999991 Prediction: 0.41800000000000015\n",
      "Error: 0.14554224999999993 Prediction: 0.41850000000000015\n",
      "Error: 0.14516099999999993 Prediction: 0.41900000000000015\n",
      "Error: 0.14478024999999992 Prediction: 0.41950000000000015\n",
      "Error: 0.14439999999999992 Prediction: 0.42000000000000015\n",
      "Error: 0.1440202499999999 Prediction: 0.42050000000000015\n",
      "Error: 0.1436409999999999 Prediction: 0.42100000000000015\n",
      "Error: 0.14326224999999992 Prediction: 0.42150000000000015\n",
      "Error: 0.14288399999999993 Prediction: 0.42200000000000015\n",
      "Error: 0.14250624999999992 Prediction: 0.42250000000000015\n",
      "Error: 0.14212899999999992 Prediction: 0.42300000000000015\n",
      "Error: 0.1417522499999999 Prediction: 0.42350000000000015\n",
      "Error: 0.14137599999999992 Prediction: 0.42400000000000015\n",
      "Error: 0.1410002499999999 Prediction: 0.42450000000000015\n",
      "Error: 0.14062499999999992 Prediction: 0.42500000000000016\n",
      "Error: 0.1402502499999999 Prediction: 0.42550000000000016\n",
      "Error: 0.13987599999999992 Prediction: 0.42600000000000016\n",
      "Error: 0.1395022499999999 Prediction: 0.42650000000000016\n",
      "Error: 0.13912899999999992 Prediction: 0.42700000000000016\n",
      "Error: 0.13875624999999991 Prediction: 0.42750000000000016\n",
      "Error: 0.13838399999999992 Prediction: 0.42800000000000016\n",
      "Error: 0.13801224999999992 Prediction: 0.42850000000000016\n",
      "Error: 0.1376409999999999 Prediction: 0.42900000000000016\n",
      "Error: 0.13727024999999993 Prediction: 0.42950000000000016\n",
      "Error: 0.1368999999999999 Prediction: 0.43000000000000016\n",
      "Error: 0.1365302499999999 Prediction: 0.43050000000000016\n",
      "Error: 0.13616099999999992 Prediction: 0.43100000000000016\n",
      "Error: 0.13579224999999992 Prediction: 0.43150000000000016\n",
      "Error: 0.1354239999999999 Prediction: 0.43200000000000016\n",
      "Error: 0.1350562499999999 Prediction: 0.43250000000000016\n",
      "Error: 0.13468899999999992 Prediction: 0.43300000000000016\n",
      "Error: 0.13432224999999992 Prediction: 0.43350000000000016\n",
      "Error: 0.1339559999999999 Prediction: 0.43400000000000016\n",
      "Error: 0.1335902499999999 Prediction: 0.43450000000000016\n",
      "Error: 0.1332249999999999 Prediction: 0.43500000000000016\n",
      "Error: 0.1328602499999999 Prediction: 0.43550000000000016\n",
      "Error: 0.13249599999999992 Prediction: 0.43600000000000017\n",
      "Error: 0.13213224999999992 Prediction: 0.43650000000000017\n",
      "Error: 0.13176899999999991 Prediction: 0.43700000000000017\n",
      "Error: 0.13140624999999992 Prediction: 0.43750000000000017\n",
      "Error: 0.1310439999999999 Prediction: 0.43800000000000017\n",
      "Error: 0.13068224999999992 Prediction: 0.43850000000000017\n",
      "Error: 0.1303209999999999 Prediction: 0.43900000000000017\n",
      "Error: 0.12996024999999992 Prediction: 0.43950000000000017\n",
      "Error: 0.1295999999999999 Prediction: 0.44000000000000017\n",
      "Error: 0.12924024999999992 Prediction: 0.44050000000000017\n",
      "Error: 0.1288809999999999 Prediction: 0.44100000000000017\n",
      "Error: 0.12852224999999992 Prediction: 0.44150000000000017\n",
      "Error: 0.12816399999999992 Prediction: 0.44200000000000017\n",
      "Error: 0.1278062499999999 Prediction: 0.44250000000000017\n",
      "Error: 0.1274489999999999 Prediction: 0.44300000000000017\n",
      "Error: 0.1270922499999999 Prediction: 0.44350000000000017\n",
      "Error: 0.1267359999999999 Prediction: 0.4440000000000002\n",
      "Error: 0.12638024999999992 Prediction: 0.4445000000000002\n",
      "Error: 0.12602499999999991 Prediction: 0.4450000000000002\n",
      "Error: 0.1256702499999999 Prediction: 0.4455000000000002\n",
      "Error: 0.1253159999999999 Prediction: 0.4460000000000002\n",
      "Error: 0.12496224999999991 Prediction: 0.4465000000000002\n",
      "Error: 0.12460899999999991 Prediction: 0.4470000000000002\n",
      "Error: 0.1242562499999999 Prediction: 0.4475000000000002\n",
      "Error: 0.1239039999999999 Prediction: 0.4480000000000002\n",
      "Error: 0.1235522499999999 Prediction: 0.4485000000000002\n",
      "Error: 0.12320099999999991 Prediction: 0.4490000000000002\n",
      "Error: 0.12285024999999991 Prediction: 0.4495000000000002\n",
      "Error: 0.1224999999999999 Prediction: 0.4500000000000002\n",
      "Error: 0.1221502499999999 Prediction: 0.4505000000000002\n",
      "Error: 0.12180099999999991 Prediction: 0.4510000000000002\n",
      "Error: 0.1214522499999999 Prediction: 0.4515000000000002\n",
      "Error: 0.1211039999999999 Prediction: 0.4520000000000002\n",
      "Error: 0.12075624999999991 Prediction: 0.4525000000000002\n",
      "Error: 0.1204089999999999 Prediction: 0.4530000000000002\n",
      "Error: 0.12006224999999991 Prediction: 0.4535000000000002\n",
      "Error: 0.1197159999999999 Prediction: 0.4540000000000002\n",
      "Error: 0.1193702499999999 Prediction: 0.4545000000000002\n",
      "Error: 0.11902499999999991 Prediction: 0.4550000000000002\n",
      "Error: 0.1186802499999999 Prediction: 0.4555000000000002\n",
      "Error: 0.1183359999999999 Prediction: 0.4560000000000002\n",
      "Error: 0.11799224999999991 Prediction: 0.4565000000000002\n",
      "Error: 0.1176489999999999 Prediction: 0.4570000000000002\n",
      "Error: 0.1173062499999999 Prediction: 0.4575000000000002\n",
      "Error: 0.1169639999999999 Prediction: 0.4580000000000002\n",
      "Error: 0.1166222499999999 Prediction: 0.4585000000000002\n",
      "Error: 0.1162809999999999 Prediction: 0.4590000000000002\n",
      "Error: 0.1159402499999999 Prediction: 0.4595000000000002\n",
      "Error: 0.1155999999999999 Prediction: 0.4600000000000002\n",
      "Error: 0.1152602499999999 Prediction: 0.4605000000000002\n",
      "Error: 0.1149209999999999 Prediction: 0.4610000000000002\n",
      "Error: 0.1145822499999999 Prediction: 0.4615000000000002\n",
      "Error: 0.1142439999999999 Prediction: 0.4620000000000002\n",
      "Error: 0.1139062499999999 Prediction: 0.4625000000000002\n",
      "Error: 0.1135689999999999 Prediction: 0.4630000000000002\n",
      "Error: 0.1132322499999999 Prediction: 0.4635000000000002\n",
      "Error: 0.1128959999999999 Prediction: 0.4640000000000002\n",
      "Error: 0.1125602499999999 Prediction: 0.4645000000000002\n",
      "Error: 0.11222499999999991 Prediction: 0.4650000000000002\n",
      "Error: 0.1118902499999999 Prediction: 0.4655000000000002\n",
      "Error: 0.1115559999999999 Prediction: 0.4660000000000002\n",
      "Error: 0.1112222499999999 Prediction: 0.4665000000000002\n",
      "Error: 0.1108889999999999 Prediction: 0.4670000000000002\n",
      "Error: 0.1105562499999999 Prediction: 0.4675000000000002\n",
      "Error: 0.1102239999999999 Prediction: 0.4680000000000002\n",
      "Error: 0.1098922499999999 Prediction: 0.4685000000000002\n",
      "Error: 0.1095609999999999 Prediction: 0.4690000000000002\n",
      "Error: 0.1092302499999999 Prediction: 0.4695000000000002\n",
      "Error: 0.1088999999999999 Prediction: 0.4700000000000002\n",
      "Error: 0.1085702499999999 Prediction: 0.4705000000000002\n",
      "Error: 0.1082409999999999 Prediction: 0.4710000000000002\n",
      "Error: 0.1079122499999999 Prediction: 0.4715000000000002\n",
      "Error: 0.1075839999999999 Prediction: 0.4720000000000002\n",
      "Error: 0.1072562499999999 Prediction: 0.4725000000000002\n",
      "Error: 0.1069289999999999 Prediction: 0.4730000000000002\n",
      "Error: 0.1066022499999999 Prediction: 0.4735000000000002\n",
      "Error: 0.1062759999999999 Prediction: 0.4740000000000002\n",
      "Error: 0.1059502499999999 Prediction: 0.4745000000000002\n",
      "Error: 0.1056249999999999 Prediction: 0.4750000000000002\n",
      "Error: 0.1053002499999999 Prediction: 0.4755000000000002\n",
      "Error: 0.1049759999999999 Prediction: 0.4760000000000002\n",
      "Error: 0.1046522499999999 Prediction: 0.4765000000000002\n",
      "Error: 0.1043289999999999 Prediction: 0.4770000000000002\n",
      "Error: 0.1040062499999999 Prediction: 0.4775000000000002\n",
      "Error: 0.1036839999999999 Prediction: 0.4780000000000002\n",
      "Error: 0.10336224999999989 Prediction: 0.4785000000000002\n",
      "Error: 0.1030409999999999 Prediction: 0.4790000000000002\n",
      "Error: 0.1027202499999999 Prediction: 0.4795000000000002\n",
      "Error: 0.1023999999999999 Prediction: 0.4800000000000002\n",
      "Error: 0.1020802499999999 Prediction: 0.4805000000000002\n",
      "Error: 0.1017609999999999 Prediction: 0.4810000000000002\n",
      "Error: 0.1014422499999999 Prediction: 0.4815000000000002\n",
      "Error: 0.1011239999999999 Prediction: 0.4820000000000002\n",
      "Error: 0.1008062499999999 Prediction: 0.4825000000000002\n",
      "Error: 0.1004889999999999 Prediction: 0.4830000000000002\n",
      "Error: 0.1001722499999999 Prediction: 0.4835000000000002\n",
      "Error: 0.0998559999999999 Prediction: 0.4840000000000002\n",
      "Error: 0.0995402499999999 Prediction: 0.4845000000000002\n",
      "Error: 0.0992249999999999 Prediction: 0.4850000000000002\n",
      "Error: 0.0989102499999999 Prediction: 0.4855000000000002\n",
      "Error: 0.09859599999999989 Prediction: 0.4860000000000002\n",
      "Error: 0.09828224999999989 Prediction: 0.4865000000000002\n",
      "Error: 0.09796899999999989 Prediction: 0.4870000000000002\n",
      "Error: 0.0976562499999999 Prediction: 0.4875000000000002\n",
      "Error: 0.09734399999999989 Prediction: 0.4880000000000002\n",
      "Error: 0.09703224999999989 Prediction: 0.4885000000000002\n",
      "Error: 0.09672099999999989 Prediction: 0.4890000000000002\n",
      "Error: 0.09641024999999989 Prediction: 0.4895000000000002\n",
      "Error: 0.0960999999999999 Prediction: 0.4900000000000002\n",
      "Error: 0.0957902499999999 Prediction: 0.4905000000000002\n",
      "Error: 0.0954809999999999 Prediction: 0.4910000000000002\n",
      "Error: 0.09517224999999989 Prediction: 0.4915000000000002\n",
      "Error: 0.09486399999999989 Prediction: 0.4920000000000002\n",
      "Error: 0.0945562499999999 Prediction: 0.4925000000000002\n",
      "Error: 0.09424899999999989 Prediction: 0.4930000000000002\n",
      "Error: 0.0939422499999999 Prediction: 0.4935000000000002\n",
      "Error: 0.0936359999999999 Prediction: 0.4940000000000002\n",
      "Error: 0.09333024999999989 Prediction: 0.4945000000000002\n",
      "Error: 0.0930249999999999 Prediction: 0.4950000000000002\n",
      "Error: 0.09272024999999989 Prediction: 0.4955000000000002\n",
      "Error: 0.0924159999999999 Prediction: 0.4960000000000002\n",
      "Error: 0.0921122499999999 Prediction: 0.4965000000000002\n",
      "Error: 0.09180899999999989 Prediction: 0.4970000000000002\n",
      "Error: 0.0915062499999999 Prediction: 0.4975000000000002\n",
      "Error: 0.0912039999999999 Prediction: 0.4980000000000002\n",
      "Error: 0.09090224999999989 Prediction: 0.4985000000000002\n",
      "Error: 0.09060099999999989 Prediction: 0.4990000000000002\n",
      "Error: 0.09030024999999989 Prediction: 0.4995000000000002\n",
      "Error: 0.0899999999999999 Prediction: 0.5000000000000002\n",
      "Error: 0.08970024999999993 Prediction: 0.5005000000000002\n",
      "Error: 0.08940099999999995 Prediction: 0.5010000000000001\n",
      "Error: 0.08910225 Prediction: 0.5015000000000001\n",
      "Error: 0.08880400000000002 Prediction: 0.502\n",
      "Error: 0.08850625000000006 Prediction: 0.5025\n",
      "Error: 0.08820900000000009 Prediction: 0.5029999999999999\n",
      "Error: 0.08791225000000012 Prediction: 0.5034999999999998\n",
      "Error: 0.08761600000000015 Prediction: 0.5039999999999998\n",
      "Error: 0.08732025000000018 Prediction: 0.5044999999999997\n",
      "Error: 0.08702500000000021 Prediction: 0.5049999999999997\n",
      "Error: 0.08673025000000026 Prediction: 0.5054999999999996\n",
      "Error: 0.08643600000000029 Prediction: 0.5059999999999996\n",
      "Error: 0.08614225000000032 Prediction: 0.5064999999999995\n",
      "Error: 0.08584900000000034 Prediction: 0.5069999999999995\n",
      "Error: 0.08555625000000038 Prediction: 0.5074999999999994\n",
      "Error: 0.08526400000000041 Prediction: 0.5079999999999993\n",
      "Error: 0.08497225000000044 Prediction: 0.5084999999999993\n",
      "Error: 0.08468100000000048 Prediction: 0.5089999999999992\n",
      "Error: 0.0843902500000005 Prediction: 0.5094999999999992\n",
      "Error: 0.08410000000000054 Prediction: 0.5099999999999991\n",
      "Error: 0.08381025000000057 Prediction: 0.5104999999999991\n",
      "Error: 0.0835210000000006 Prediction: 0.510999999999999\n",
      "Error: 0.08323225000000063 Prediction: 0.511499999999999\n",
      "Error: 0.08294400000000066 Prediction: 0.5119999999999989\n",
      "Error: 0.0826562500000007 Prediction: 0.5124999999999988\n",
      "Error: 0.08236900000000072 Prediction: 0.5129999999999988\n",
      "Error: 0.08208225000000074 Prediction: 0.5134999999999987\n",
      "Error: 0.08179600000000078 Prediction: 0.5139999999999987\n",
      "Error: 0.08151025000000081 Prediction: 0.5144999999999986\n",
      "Error: 0.08122500000000084 Prediction: 0.5149999999999986\n",
      "Error: 0.08094025000000087 Prediction: 0.5154999999999985\n",
      "Error: 0.0806560000000009 Prediction: 0.5159999999999985\n",
      "Error: 0.08037225000000094 Prediction: 0.5164999999999984\n",
      "Error: 0.08008900000000096 Prediction: 0.5169999999999983\n",
      "Error: 0.079806250000001 Prediction: 0.5174999999999983\n",
      "Error: 0.07952400000000102 Prediction: 0.5179999999999982\n",
      "Error: 0.07924225000000104 Prediction: 0.5184999999999982\n",
      "Error: 0.07896100000000107 Prediction: 0.5189999999999981\n",
      "Error: 0.0786802500000011 Prediction: 0.5194999999999981\n",
      "Error: 0.07840000000000114 Prediction: 0.519999999999998\n",
      "Error: 0.07812025000000117 Prediction: 0.520499999999998\n",
      "Error: 0.07784100000000119 Prediction: 0.5209999999999979\n",
      "Error: 0.07756225000000122 Prediction: 0.5214999999999979\n",
      "Error: 0.07728400000000125 Prediction: 0.5219999999999978\n",
      "Error: 0.07700625000000128 Prediction: 0.5224999999999977\n",
      "Error: 0.07672900000000131 Prediction: 0.5229999999999977\n",
      "Error: 0.07645225000000133 Prediction: 0.5234999999999976\n",
      "Error: 0.07617600000000137 Prediction: 0.5239999999999976\n",
      "Error: 0.07590025000000139 Prediction: 0.5244999999999975\n",
      "Error: 0.07562500000000141 Prediction: 0.5249999999999975\n",
      "Error: 0.07535025000000145 Prediction: 0.5254999999999974\n",
      "Error: 0.07507600000000147 Prediction: 0.5259999999999974\n",
      "Error: 0.0748022500000015 Prediction: 0.5264999999999973\n",
      "Error: 0.07452900000000152 Prediction: 0.5269999999999972\n",
      "Error: 0.07425625000000155 Prediction: 0.5274999999999972\n",
      "Error: 0.07398400000000158 Prediction: 0.5279999999999971\n",
      "Error: 0.0737122500000016 Prediction: 0.5284999999999971\n",
      "Error: 0.07344100000000163 Prediction: 0.528999999999997\n",
      "Error: 0.07317025000000166 Prediction: 0.529499999999997\n",
      "Error: 0.07290000000000169 Prediction: 0.5299999999999969\n",
      "Error: 0.07263025000000171 Prediction: 0.5304999999999969\n",
      "Error: 0.07236100000000174 Prediction: 0.5309999999999968\n",
      "Error: 0.07209225000000177 Prediction: 0.5314999999999968\n",
      "Error: 0.07182400000000179 Prediction: 0.5319999999999967\n",
      "Error: 0.07155625000000182 Prediction: 0.5324999999999966\n",
      "Error: 0.07128900000000185 Prediction: 0.5329999999999966\n",
      "Error: 0.07102225000000187 Prediction: 0.5334999999999965\n",
      "Error: 0.0707560000000019 Prediction: 0.5339999999999965\n",
      "Error: 0.07049025000000192 Prediction: 0.5344999999999964\n",
      "Error: 0.07022500000000195 Prediction: 0.5349999999999964\n",
      "Error: 0.06996025000000197 Prediction: 0.5354999999999963\n",
      "Error: 0.069696000000002 Prediction: 0.5359999999999963\n",
      "Error: 0.06943225000000203 Prediction: 0.5364999999999962\n",
      "Error: 0.06916900000000205 Prediction: 0.5369999999999961\n",
      "Error: 0.06890625000000207 Prediction: 0.5374999999999961\n",
      "Error: 0.0686440000000021 Prediction: 0.537999999999996\n",
      "Error: 0.06838225000000213 Prediction: 0.538499999999996\n",
      "Error: 0.06812100000000215 Prediction: 0.5389999999999959\n",
      "Error: 0.06786025000000218 Prediction: 0.5394999999999959\n",
      "Error: 0.0676000000000022 Prediction: 0.5399999999999958\n",
      "Error: 0.06734025000000222 Prediction: 0.5404999999999958\n",
      "Error: 0.06708100000000225 Prediction: 0.5409999999999957\n",
      "Error: 0.06682225000000228 Prediction: 0.5414999999999957\n",
      "Error: 0.0665640000000023 Prediction: 0.5419999999999956\n",
      "Error: 0.06630625000000231 Prediction: 0.5424999999999955\n",
      "Error: 0.06604900000000234 Prediction: 0.5429999999999955\n",
      "Error: 0.06579225000000237 Prediction: 0.5434999999999954\n",
      "Error: 0.06553600000000238 Prediction: 0.5439999999999954\n",
      "Error: 0.06528025000000241 Prediction: 0.5444999999999953\n",
      "Error: 0.06502500000000244 Prediction: 0.5449999999999953\n",
      "Error: 0.06477025000000246 Prediction: 0.5454999999999952\n",
      "Error: 0.06451600000000249 Prediction: 0.5459999999999952\n",
      "Error: 0.0642622500000025 Prediction: 0.5464999999999951\n",
      "Error: 0.06400900000000254 Prediction: 0.546999999999995\n",
      "Error: 0.06375625000000255 Prediction: 0.547499999999995\n",
      "Error: 0.06350400000000257 Prediction: 0.5479999999999949\n",
      "Error: 0.06325225000000259 Prediction: 0.5484999999999949\n",
      "Error: 0.06300100000000262 Prediction: 0.5489999999999948\n",
      "Error: 0.06275025000000264 Prediction: 0.5494999999999948\n",
      "Error: 0.06250000000000266 Prediction: 0.5499999999999947\n",
      "Error: 0.062250250000002685 Prediction: 0.5504999999999947\n",
      "Error: 0.06200100000000271 Prediction: 0.5509999999999946\n",
      "Error: 0.06175225000000273 Prediction: 0.5514999999999946\n",
      "Error: 0.06150400000000275 Prediction: 0.5519999999999945\n",
      "Error: 0.061256250000002774 Prediction: 0.5524999999999944\n",
      "Error: 0.0610090000000028 Prediction: 0.5529999999999944\n",
      "Error: 0.060762250000002814 Prediction: 0.5534999999999943\n",
      "Error: 0.06051600000000284 Prediction: 0.5539999999999943\n",
      "Error: 0.06027025000000286 Prediction: 0.5544999999999942\n",
      "Error: 0.06002500000000288 Prediction: 0.5549999999999942\n",
      "Error: 0.0597802500000029 Prediction: 0.5554999999999941\n",
      "Error: 0.05953600000000292 Prediction: 0.555999999999994\n",
      "Error: 0.05929225000000295 Prediction: 0.556499999999994\n",
      "Error: 0.05904900000000297 Prediction: 0.5569999999999939\n",
      "Error: 0.05880625000000299 Prediction: 0.5574999999999939\n",
      "Error: 0.058564000000003 Prediction: 0.5579999999999938\n",
      "Error: 0.058322250000003024 Prediction: 0.5584999999999938\n",
      "Error: 0.05808100000000305 Prediction: 0.5589999999999937\n",
      "Error: 0.05784025000000307 Prediction: 0.5594999999999937\n",
      "Error: 0.057600000000003086 Prediction: 0.5599999999999936\n",
      "Error: 0.0573602500000031 Prediction: 0.5604999999999936\n",
      "Error: 0.05712100000000313 Prediction: 0.5609999999999935\n",
      "Error: 0.056882250000003146 Prediction: 0.5614999999999934\n",
      "Error: 0.056644000000003164 Prediction: 0.5619999999999934\n",
      "Error: 0.05640625000000318 Prediction: 0.5624999999999933\n",
      "Error: 0.0561690000000032 Prediction: 0.5629999999999933\n",
      "Error: 0.05593225000000322 Prediction: 0.5634999999999932\n",
      "Error: 0.05569600000000324 Prediction: 0.5639999999999932\n",
      "Error: 0.055460250000003264 Prediction: 0.5644999999999931\n",
      "Error: 0.05522500000000328 Prediction: 0.5649999999999931\n",
      "Error: 0.0549902500000033 Prediction: 0.565499999999993\n",
      "Error: 0.054756000000003316 Prediction: 0.565999999999993\n",
      "Error: 0.05452225000000334 Prediction: 0.5664999999999929\n",
      "Error: 0.054289000000003355 Prediction: 0.5669999999999928\n",
      "Error: 0.05405625000000337 Prediction: 0.5674999999999928\n",
      "Error: 0.05382400000000339 Prediction: 0.5679999999999927\n",
      "Error: 0.05359225000000341 Prediction: 0.5684999999999927\n",
      "Error: 0.053361000000003427 Prediction: 0.5689999999999926\n",
      "Error: 0.053130250000003446 Prediction: 0.5694999999999926\n",
      "Error: 0.052900000000003465 Prediction: 0.5699999999999925\n",
      "Error: 0.052670250000003485 Prediction: 0.5704999999999925\n",
      "Error: 0.0524410000000035 Prediction: 0.5709999999999924\n",
      "Error: 0.05221225000000352 Prediction: 0.5714999999999923\n",
      "Error: 0.051984000000003534 Prediction: 0.5719999999999923\n",
      "Error: 0.05175625000000355 Prediction: 0.5724999999999922\n",
      "Error: 0.05152900000000357 Prediction: 0.5729999999999922\n",
      "Error: 0.05130225000000359 Prediction: 0.5734999999999921\n",
      "Error: 0.051076000000003605 Prediction: 0.5739999999999921\n",
      "Error: 0.05085025000000362 Prediction: 0.574499999999992\n",
      "Error: 0.05062500000000364 Prediction: 0.574999999999992\n",
      "Error: 0.05040025000000365 Prediction: 0.5754999999999919\n",
      "Error: 0.05017600000000367 Prediction: 0.5759999999999919\n",
      "Error: 0.04995225000000369 Prediction: 0.5764999999999918\n",
      "Error: 0.0497290000000037 Prediction: 0.5769999999999917\n",
      "Error: 0.04950625000000372 Prediction: 0.5774999999999917\n",
      "Error: 0.049284000000003735 Prediction: 0.5779999999999916\n",
      "Error: 0.04906225000000375 Prediction: 0.5784999999999916\n",
      "Error: 0.04884100000000377 Prediction: 0.5789999999999915\n",
      "Error: 0.048620250000003785 Prediction: 0.5794999999999915\n",
      "Error: 0.0484000000000038 Prediction: 0.5799999999999914\n",
      "Error: 0.04818025000000382 Prediction: 0.5804999999999914\n",
      "Error: 0.04796100000000383 Prediction: 0.5809999999999913\n",
      "Error: 0.047742250000003844 Prediction: 0.5814999999999912\n",
      "Error: 0.04752400000000386 Prediction: 0.5819999999999912\n",
      "Error: 0.04730625000000387 Prediction: 0.5824999999999911\n",
      "Error: 0.04708900000000389 Prediction: 0.5829999999999911\n",
      "Error: 0.046872250000003904 Prediction: 0.583499999999991\n",
      "Error: 0.04665600000000392 Prediction: 0.583999999999991\n",
      "Error: 0.04644025000000394 Prediction: 0.5844999999999909\n",
      "Error: 0.04622500000000395 Prediction: 0.5849999999999909\n",
      "Error: 0.046010250000003965 Prediction: 0.5854999999999908\n",
      "Error: 0.04579600000000398 Prediction: 0.5859999999999908\n",
      "Error: 0.045582250000003995 Prediction: 0.5864999999999907\n",
      "Error: 0.045369000000004 Prediction: 0.5869999999999906\n",
      "Error: 0.04515625000000402 Prediction: 0.5874999999999906\n",
      "Error: 0.044944000000004036 Prediction: 0.5879999999999905\n",
      "Error: 0.04473225000000405 Prediction: 0.5884999999999905\n",
      "Error: 0.044521000000004064 Prediction: 0.5889999999999904\n",
      "Error: 0.044310250000004076 Prediction: 0.5894999999999904\n",
      "Error: 0.04410000000000409 Prediction: 0.5899999999999903\n",
      "Error: 0.0438902500000041 Prediction: 0.5904999999999903\n",
      "Error: 0.04368100000000411 Prediction: 0.5909999999999902\n",
      "Error: 0.043472250000004126 Prediction: 0.5914999999999901\n",
      "Error: 0.04326400000000414 Prediction: 0.5919999999999901\n",
      "Error: 0.043056250000004154 Prediction: 0.59249999999999\n",
      "Error: 0.04284900000000417 Prediction: 0.59299999999999\n",
      "Error: 0.04264225000000418 Prediction: 0.5934999999999899\n",
      "Error: 0.04243600000000419 Prediction: 0.5939999999999899\n",
      "Error: 0.0422302500000042 Prediction: 0.5944999999999898\n",
      "Error: 0.04202500000000422 Prediction: 0.5949999999999898\n",
      "Error: 0.04182025000000423 Prediction: 0.5954999999999897\n",
      "Error: 0.04161600000000424 Prediction: 0.5959999999999896\n",
      "Error: 0.04141225000000425 Prediction: 0.5964999999999896\n",
      "Error: 0.04120900000000426 Prediction: 0.5969999999999895\n",
      "Error: 0.041006250000004275 Prediction: 0.5974999999999895\n",
      "Error: 0.04080400000000429 Prediction: 0.5979999999999894\n",
      "Error: 0.0406022500000043 Prediction: 0.5984999999999894\n",
      "Error: 0.04040100000000431 Prediction: 0.5989999999999893\n",
      "Error: 0.04020025000000432 Prediction: 0.5994999999999893\n",
      "Error: 0.04000000000000434 Prediction: 0.5999999999999892\n",
      "Error: 0.039800250000004346 Prediction: 0.6004999999999892\n",
      "Error: 0.039601000000004355 Prediction: 0.6009999999999891\n",
      "Error: 0.039402250000004364 Prediction: 0.601499999999989\n",
      "Error: 0.03920400000000438 Prediction: 0.601999999999989\n",
      "Error: 0.03900625000000439 Prediction: 0.6024999999999889\n",
      "Error: 0.0388090000000044 Prediction: 0.6029999999999889\n",
      "Error: 0.03861225000000441 Prediction: 0.6034999999999888\n",
      "Error: 0.03841600000000442 Prediction: 0.6039999999999888\n",
      "Error: 0.03822025000000443 Prediction: 0.6044999999999887\n",
      "Error: 0.038025000000004444 Prediction: 0.6049999999999887\n",
      "Error: 0.03783025000000445 Prediction: 0.6054999999999886\n",
      "Error: 0.03763600000000446 Prediction: 0.6059999999999885\n",
      "Error: 0.03744225000000447 Prediction: 0.6064999999999885\n",
      "Error: 0.03724900000000448 Prediction: 0.6069999999999884\n",
      "Error: 0.03705625000000449 Prediction: 0.6074999999999884\n",
      "Error: 0.0368640000000045 Prediction: 0.6079999999999883\n",
      "Error: 0.03667225000000451 Prediction: 0.6084999999999883\n",
      "Error: 0.03648100000000452 Prediction: 0.6089999999999882\n",
      "Error: 0.03629025000000453 Prediction: 0.6094999999999882\n",
      "Error: 0.03610000000000454 Prediction: 0.6099999999999881\n",
      "Error: 0.03591025000000454 Prediction: 0.610499999999988\n",
      "Error: 0.035721000000004555 Prediction: 0.610999999999988\n",
      "Error: 0.03553225000000456 Prediction: 0.6114999999999879\n",
      "Error: 0.03534400000000457 Prediction: 0.6119999999999879\n",
      "Error: 0.03515625000000458 Prediction: 0.6124999999999878\n",
      "Error: 0.03496900000000459 Prediction: 0.6129999999999878\n",
      "Error: 0.034782250000004594 Prediction: 0.6134999999999877\n",
      "Error: 0.0345960000000046 Prediction: 0.6139999999999877\n",
      "Error: 0.03441025000000461 Prediction: 0.6144999999999876\n",
      "Error: 0.03422500000000462 Prediction: 0.6149999999999876\n",
      "Error: 0.03404025000000463 Prediction: 0.6154999999999875\n",
      "Error: 0.03385600000000464 Prediction: 0.6159999999999874\n",
      "Error: 0.03367225000000464 Prediction: 0.6164999999999874\n",
      "Error: 0.033489000000004654 Prediction: 0.6169999999999873\n",
      "Error: 0.03330625000000466 Prediction: 0.6174999999999873\n",
      "Error: 0.033124000000004664 Prediction: 0.6179999999999872\n",
      "Error: 0.032942250000004676 Prediction: 0.6184999999999872\n",
      "Error: 0.03276100000000468 Prediction: 0.6189999999999871\n",
      "Error: 0.03258025000000469 Prediction: 0.6194999999999871\n",
      "Error: 0.032400000000004696 Prediction: 0.619999999999987\n",
      "Error: 0.0322202500000047 Prediction: 0.620499999999987\n",
      "Error: 0.032041000000004705 Prediction: 0.6209999999999869\n",
      "Error: 0.03186225000000471 Prediction: 0.6214999999999868\n",
      "Error: 0.03168400000000472 Prediction: 0.6219999999999868\n",
      "Error: 0.031506250000004725 Prediction: 0.6224999999999867\n",
      "Error: 0.031329000000004735 Prediction: 0.6229999999999867\n",
      "Error: 0.03115225000000474 Prediction: 0.6234999999999866\n",
      "Error: 0.030976000000004746 Prediction: 0.6239999999999866\n",
      "Error: 0.03080025000000475 Prediction: 0.6244999999999865\n",
      "Error: 0.030625000000004756 Prediction: 0.6249999999999865\n",
      "Error: 0.03045025000000476 Prediction: 0.6254999999999864\n",
      "Error: 0.030276000000004768 Prediction: 0.6259999999999863\n",
      "Error: 0.03010225000000477 Prediction: 0.6264999999999863\n",
      "Error: 0.029929000000004778 Prediction: 0.6269999999999862\n",
      "Error: 0.029756250000004782 Prediction: 0.6274999999999862\n",
      "Error: 0.029584000000004787 Prediction: 0.6279999999999861\n",
      "Error: 0.029412250000004792 Prediction: 0.6284999999999861\n",
      "Error: 0.029241000000004798 Prediction: 0.628999999999986\n",
      "Error: 0.029070250000004804 Prediction: 0.629499999999986\n",
      "Error: 0.028900000000004807 Prediction: 0.6299999999999859\n",
      "Error: 0.02873025000000481 Prediction: 0.6304999999999858\n",
      "Error: 0.028561000000004815 Prediction: 0.6309999999999858\n",
      "Error: 0.02839225000000482 Prediction: 0.6314999999999857\n",
      "Error: 0.028224000000004825 Prediction: 0.6319999999999857\n",
      "Error: 0.02805625000000483 Prediction: 0.6324999999999856\n",
      "Error: 0.027889000000004834 Prediction: 0.6329999999999856\n",
      "Error: 0.027722250000004837 Prediction: 0.6334999999999855\n",
      "Error: 0.02755600000000484 Prediction: 0.6339999999999855\n",
      "Error: 0.027390250000004845 Prediction: 0.6344999999999854\n",
      "Error: 0.02722500000000485 Prediction: 0.6349999999999854\n",
      "Error: 0.02706025000000485 Prediction: 0.6354999999999853\n",
      "Error: 0.026896000000004854 Prediction: 0.6359999999999852\n",
      "Error: 0.026732250000004856 Prediction: 0.6364999999999852\n",
      "Error: 0.02656900000000486 Prediction: 0.6369999999999851\n",
      "Error: 0.026406250000004863 Prediction: 0.6374999999999851\n",
      "Error: 0.026244000000004868 Prediction: 0.637999999999985\n",
      "Error: 0.02608225000000487 Prediction: 0.638499999999985\n",
      "Error: 0.02592100000000487 Prediction: 0.6389999999999849\n",
      "Error: 0.025760250000004873 Prediction: 0.6394999999999849\n",
      "Error: 0.025600000000004876 Prediction: 0.6399999999999848\n",
      "Error: 0.02544025000000488 Prediction: 0.6404999999999847\n",
      "Error: 0.025281000000004883 Prediction: 0.6409999999999847\n",
      "Error: 0.025122250000004884 Prediction: 0.6414999999999846\n",
      "Error: 0.024964000000004885 Prediction: 0.6419999999999846\n",
      "Error: 0.024806250000004887 Prediction: 0.6424999999999845\n",
      "Error: 0.02464900000000489 Prediction: 0.6429999999999845\n",
      "Error: 0.024492250000004892 Prediction: 0.6434999999999844\n",
      "Error: 0.024336000000004892 Prediction: 0.6439999999999844\n",
      "Error: 0.024180250000004896 Prediction: 0.6444999999999843\n",
      "Error: 0.024025000000004897 Prediction: 0.6449999999999843\n",
      "Error: 0.023870250000004898 Prediction: 0.6454999999999842\n",
      "Error: 0.023716000000004896 Prediction: 0.6459999999999841\n",
      "Error: 0.0235622500000049 Prediction: 0.6464999999999841\n",
      "Error: 0.023409000000004898 Prediction: 0.646999999999984\n",
      "Error: 0.0232562500000049 Prediction: 0.647499999999984\n",
      "Error: 0.023104000000004902 Prediction: 0.6479999999999839\n",
      "Error: 0.022952250000004903 Prediction: 0.6484999999999839\n",
      "Error: 0.0228010000000049 Prediction: 0.6489999999999838\n",
      "Error: 0.022650250000004903 Prediction: 0.6494999999999838\n",
      "Error: 0.0225000000000049 Prediction: 0.6499999999999837\n",
      "Error: 0.022350250000004904 Prediction: 0.6504999999999836\n",
      "Error: 0.022201000000004904 Prediction: 0.6509999999999836\n",
      "Error: 0.0220522500000049 Prediction: 0.6514999999999835\n",
      "Error: 0.021904000000004902 Prediction: 0.6519999999999835\n",
      "Error: 0.021756250000004904 Prediction: 0.6524999999999834\n",
      "Error: 0.021609000000004902 Prediction: 0.6529999999999834\n",
      "Error: 0.0214622500000049 Prediction: 0.6534999999999833\n",
      "Error: 0.0213160000000049 Prediction: 0.6539999999999833\n",
      "Error: 0.0211702500000049 Prediction: 0.6544999999999832\n",
      "Error: 0.021025000000004897 Prediction: 0.6549999999999832\n",
      "Error: 0.0208802500000049 Prediction: 0.6554999999999831\n",
      "Error: 0.020736000000004896 Prediction: 0.655999999999983\n",
      "Error: 0.020592250000004895 Prediction: 0.656499999999983\n",
      "Error: 0.020449000000004894 Prediction: 0.6569999999999829\n",
      "Error: 0.020306250000004893 Prediction: 0.6574999999999829\n",
      "Error: 0.02016400000000489 Prediction: 0.6579999999999828\n",
      "Error: 0.02002225000000489 Prediction: 0.6584999999999828\n",
      "Error: 0.019881000000004888 Prediction: 0.6589999999999827\n",
      "Error: 0.019740250000004886 Prediction: 0.6594999999999827\n",
      "Error: 0.019600000000004884 Prediction: 0.6599999999999826\n",
      "Error: 0.019460250000004883 Prediction: 0.6604999999999825\n",
      "Error: 0.01932100000000488 Prediction: 0.6609999999999825\n",
      "Error: 0.01918225000000488 Prediction: 0.6614999999999824\n",
      "Error: 0.019044000000004876 Prediction: 0.6619999999999824\n",
      "Error: 0.018906250000004874 Prediction: 0.6624999999999823\n",
      "Error: 0.01876900000000487 Prediction: 0.6629999999999823\n",
      "Error: 0.018632250000004867 Prediction: 0.6634999999999822\n",
      "Error: 0.018496000000004866 Prediction: 0.6639999999999822\n",
      "Error: 0.018360250000004862 Prediction: 0.6644999999999821\n",
      "Error: 0.01822500000000486 Prediction: 0.664999999999982\n",
      "Error: 0.018090250000004856 Prediction: 0.665499999999982\n",
      "Error: 0.017956000000004853 Prediction: 0.6659999999999819\n",
      "Error: 0.017822250000004848 Prediction: 0.6664999999999819\n",
      "Error: 0.017689000000004847 Prediction: 0.6669999999999818\n",
      "Error: 0.017556250000004842 Prediction: 0.6674999999999818\n",
      "Error: 0.01742400000000484 Prediction: 0.6679999999999817\n",
      "Error: 0.017292250000004835 Prediction: 0.6684999999999817\n",
      "Error: 0.01716100000000483 Prediction: 0.6689999999999816\n",
      "Error: 0.017030250000004826 Prediction: 0.6694999999999816\n",
      "Error: 0.01690000000000482 Prediction: 0.6699999999999815\n",
      "Error: 0.016770250000004816 Prediction: 0.6704999999999814\n",
      "Error: 0.01664100000000481 Prediction: 0.6709999999999814\n",
      "Error: 0.016512250000004808 Prediction: 0.6714999999999813\n",
      "Error: 0.016384000000004804 Prediction: 0.6719999999999813\n",
      "Error: 0.016256250000004798 Prediction: 0.6724999999999812\n",
      "Error: 0.016129000000004796 Prediction: 0.6729999999999812\n",
      "Error: 0.01600225000000479 Prediction: 0.6734999999999811\n",
      "Error: 0.015876000000004786 Prediction: 0.6739999999999811\n",
      "Error: 0.015750250000004778 Prediction: 0.674499999999981\n",
      "Error: 0.015625000000004774 Prediction: 0.674999999999981\n",
      "Error: 0.015500250000004769 Prediction: 0.6754999999999809\n",
      "Error: 0.015376000000004763 Prediction: 0.6759999999999808\n",
      "Error: 0.015252250000004757 Prediction: 0.6764999999999808\n",
      "Error: 0.015129000000004751 Prediction: 0.6769999999999807\n",
      "Error: 0.015006250000004747 Prediction: 0.6774999999999807\n",
      "Error: 0.01488400000000474 Prediction: 0.6779999999999806\n",
      "Error: 0.014762250000004733 Prediction: 0.6784999999999806\n",
      "Error: 0.014641000000004728 Prediction: 0.6789999999999805\n",
      "Error: 0.014520250000004722 Prediction: 0.6794999999999805\n",
      "Error: 0.014400000000004715 Prediction: 0.6799999999999804\n",
      "Error: 0.01428025000000471 Prediction: 0.6804999999999803\n",
      "Error: 0.014161000000004703 Prediction: 0.6809999999999803\n",
      "Error: 0.014042250000004695 Prediction: 0.6814999999999802\n",
      "Error: 0.013924000000004688 Prediction: 0.6819999999999802\n",
      "Error: 0.013806250000004681 Prediction: 0.6824999999999801\n",
      "Error: 0.013689000000004675 Prediction: 0.6829999999999801\n",
      "Error: 0.013572250000004667 Prediction: 0.68349999999998\n",
      "Error: 0.01345600000000466 Prediction: 0.68399999999998\n",
      "Error: 0.013340250000004652 Prediction: 0.6844999999999799\n",
      "Error: 0.013225000000004644 Prediction: 0.6849999999999798\n",
      "Error: 0.013110250000004637 Prediction: 0.6854999999999798\n",
      "Error: 0.01299600000000463 Prediction: 0.6859999999999797\n",
      "Error: 0.012882250000004623 Prediction: 0.6864999999999797\n",
      "Error: 0.012769000000004613 Prediction: 0.6869999999999796\n",
      "Error: 0.012656250000004607 Prediction: 0.6874999999999796\n",
      "Error: 0.012544000000004598 Prediction: 0.6879999999999795\n",
      "Error: 0.01243225000000459 Prediction: 0.6884999999999795\n",
      "Error: 0.012321000000004582 Prediction: 0.6889999999999794\n",
      "Error: 0.012210250000004573 Prediction: 0.6894999999999794\n",
      "Error: 0.012100000000004564 Prediction: 0.6899999999999793\n",
      "Error: 0.011990250000004556 Prediction: 0.6904999999999792\n",
      "Error: 0.011881000000004548 Prediction: 0.6909999999999792\n",
      "Error: 0.011772250000004538 Prediction: 0.6914999999999791\n",
      "Error: 0.011664000000004528 Prediction: 0.6919999999999791\n",
      "Error: 0.01155625000000452 Prediction: 0.692499999999979\n",
      "Error: 0.011449000000004511 Prediction: 0.692999999999979\n",
      "Error: 0.011342250000004502 Prediction: 0.6934999999999789\n",
      "Error: 0.011236000000004492 Prediction: 0.6939999999999789\n",
      "Error: 0.011130250000004482 Prediction: 0.6944999999999788\n",
      "Error: 0.011025000000004472 Prediction: 0.6949999999999787\n",
      "Error: 0.010920250000004463 Prediction: 0.6954999999999787\n",
      "Error: 0.010816000000004452 Prediction: 0.6959999999999786\n",
      "Error: 0.010712250000004442 Prediction: 0.6964999999999786\n",
      "Error: 0.010609000000004433 Prediction: 0.6969999999999785\n",
      "Error: 0.010506250000004422 Prediction: 0.6974999999999785\n",
      "Error: 0.010404000000004411 Prediction: 0.6979999999999784\n",
      "Error: 0.010302250000004402 Prediction: 0.6984999999999784\n",
      "Error: 0.01020100000000439 Prediction: 0.6989999999999783\n",
      "Error: 0.01010025000000438 Prediction: 0.6994999999999783\n",
      "Error: 0.01000000000000437 Prediction: 0.6999999999999782\n",
      "Error: 0.009900250000004359 Prediction: 0.7004999999999781\n",
      "Error: 0.009801000000004348 Prediction: 0.7009999999999781\n",
      "Error: 0.009702250000004338 Prediction: 0.701499999999978\n",
      "Error: 0.009604000000004326 Prediction: 0.701999999999978\n",
      "Error: 0.009506250000004315 Prediction: 0.7024999999999779\n",
      "Error: 0.009409000000004303 Prediction: 0.7029999999999779\n",
      "Error: 0.009312250000004291 Prediction: 0.7034999999999778\n",
      "Error: 0.00921600000000428 Prediction: 0.7039999999999778\n",
      "Error: 0.009120250000004267 Prediction: 0.7044999999999777\n",
      "Error: 0.009025000000004255 Prediction: 0.7049999999999776\n",
      "Error: 0.008930250000004244 Prediction: 0.7054999999999776\n",
      "Error: 0.008836000000004231 Prediction: 0.7059999999999775\n",
      "Error: 0.008742250000004219 Prediction: 0.7064999999999775\n",
      "Error: 0.008649000000004207 Prediction: 0.7069999999999774\n",
      "Error: 0.008556250000004194 Prediction: 0.7074999999999774\n",
      "Error: 0.008464000000004182 Prediction: 0.7079999999999773\n",
      "Error: 0.00837225000000417 Prediction: 0.7084999999999773\n",
      "Error: 0.008281000000004157 Prediction: 0.7089999999999772\n",
      "Error: 0.008190250000004144 Prediction: 0.7094999999999771\n",
      "Error: 0.008100000000004132 Prediction: 0.7099999999999771\n",
      "Error: 0.008010250000004118 Prediction: 0.710499999999977\n",
      "Error: 0.007921000000004105 Prediction: 0.710999999999977\n",
      "Error: 0.007832250000004091 Prediction: 0.7114999999999769\n",
      "Error: 0.007744000000004078 Prediction: 0.7119999999999769\n",
      "Error: 0.007656250000004064 Prediction: 0.7124999999999768\n",
      "Error: 0.007569000000004051 Prediction: 0.7129999999999768\n",
      "Error: 0.007482250000004037 Prediction: 0.7134999999999767\n",
      "Error: 0.0073960000000040235 Prediction: 0.7139999999999767\n",
      "Error: 0.00731025000000401 Prediction: 0.7144999999999766\n",
      "Error: 0.007225000000003996 Prediction: 0.7149999999999765\n",
      "Error: 0.007140250000003981 Prediction: 0.7154999999999765\n",
      "Error: 0.007056000000003967 Prediction: 0.7159999999999764\n",
      "Error: 0.006972250000003953 Prediction: 0.7164999999999764\n",
      "Error: 0.006889000000003938 Prediction: 0.7169999999999763\n",
      "Error: 0.006806250000003923 Prediction: 0.7174999999999763\n",
      "Error: 0.006724000000003908 Prediction: 0.7179999999999762\n",
      "Error: 0.006642250000003893 Prediction: 0.7184999999999762\n",
      "Error: 0.006561000000003879 Prediction: 0.7189999999999761\n",
      "Error: 0.006480250000003863 Prediction: 0.719499999999976\n",
      "Error: 0.006400000000003848 Prediction: 0.719999999999976\n",
      "Error: 0.006320250000003833 Prediction: 0.7204999999999759\n",
      "Error: 0.006241000000003817 Prediction: 0.7209999999999759\n",
      "Error: 0.006162250000003802 Prediction: 0.7214999999999758\n",
      "Error: 0.006084000000003786 Prediction: 0.7219999999999758\n",
      "Error: 0.006006250000003771 Prediction: 0.7224999999999757\n",
      "Error: 0.005929000000003755 Prediction: 0.7229999999999757\n",
      "Error: 0.005852250000003739 Prediction: 0.7234999999999756\n",
      "Error: 0.005776000000003723 Prediction: 0.7239999999999756\n",
      "Error: 0.005700250000003707 Prediction: 0.7244999999999755\n",
      "Error: 0.00562500000000369 Prediction: 0.7249999999999754\n",
      "Error: 0.005550250000003674 Prediction: 0.7254999999999754\n",
      "Error: 0.005476000000003658 Prediction: 0.7259999999999753\n",
      "Error: 0.005402250000003641 Prediction: 0.7264999999999753\n",
      "Error: 0.005329000000003624 Prediction: 0.7269999999999752\n",
      "Error: 0.005256250000003607 Prediction: 0.7274999999999752\n",
      "Error: 0.00518400000000359 Prediction: 0.7279999999999751\n",
      "Error: 0.005112250000003573 Prediction: 0.7284999999999751\n",
      "Error: 0.0050410000000035565 Prediction: 0.728999999999975\n",
      "Error: 0.004970250000003539 Prediction: 0.729499999999975\n",
      "Error: 0.004900000000003521 Prediction: 0.7299999999999749\n",
      "Error: 0.004830250000003504 Prediction: 0.7304999999999748\n",
      "Error: 0.004761000000003486 Prediction: 0.7309999999999748\n",
      "Error: 0.004692250000003469 Prediction: 0.7314999999999747\n",
      "Error: 0.004624000000003451 Prediction: 0.7319999999999747\n",
      "Error: 0.0045562500000034326 Prediction: 0.7324999999999746\n",
      "Error: 0.004489000000003415 Prediction: 0.7329999999999746\n",
      "Error: 0.0044222500000033966 Prediction: 0.7334999999999745\n",
      "Error: 0.004356000000003378 Prediction: 0.7339999999999745\n",
      "Error: 0.00429025000000336 Prediction: 0.7344999999999744\n",
      "Error: 0.0042250000000033415 Prediction: 0.7349999999999743\n",
      "Error: 0.004160250000003323 Prediction: 0.7354999999999743\n",
      "Error: 0.0040960000000033045 Prediction: 0.7359999999999742\n",
      "Error: 0.004032250000003285 Prediction: 0.7364999999999742\n",
      "Error: 0.003969000000003267 Prediction: 0.7369999999999741\n",
      "Error: 0.003906250000003247 Prediction: 0.7374999999999741\n",
      "Error: 0.003844000000003228 Prediction: 0.737999999999974\n",
      "Error: 0.003782250000003209 Prediction: 0.738499999999974\n",
      "Error: 0.0037210000000031896 Prediction: 0.7389999999999739\n",
      "Error: 0.00366025000000317 Prediction: 0.7394999999999738\n",
      "Error: 0.0036000000000031506 Prediction: 0.7399999999999738\n",
      "Error: 0.0035402500000031307 Prediction: 0.7404999999999737\n",
      "Error: 0.003481000000003111 Prediction: 0.7409999999999737\n",
      "Error: 0.0034222500000030912 Prediction: 0.7414999999999736\n",
      "Error: 0.003364000000003071 Prediction: 0.7419999999999736\n",
      "Error: 0.003306250000003051 Prediction: 0.7424999999999735\n",
      "Error: 0.0032490000000030307 Prediction: 0.7429999999999735\n",
      "Error: 0.0031922500000030104 Prediction: 0.7434999999999734\n",
      "Error: 0.0031360000000029897 Prediction: 0.7439999999999733\n",
      "Error: 0.003080250000002969 Prediction: 0.7444999999999733\n",
      "Error: 0.0030250000000029485 Prediction: 0.7449999999999732\n",
      "Error: 0.0029702500000029276 Prediction: 0.7454999999999732\n",
      "Error: 0.0029160000000029067 Prediction: 0.7459999999999731\n",
      "Error: 0.002862250000002886 Prediction: 0.7464999999999731\n",
      "Error: 0.0028090000000028648 Prediction: 0.746999999999973\n",
      "Error: 0.0027562500000028437 Prediction: 0.747499999999973\n",
      "Error: 0.002704000000002822 Prediction: 0.7479999999999729\n",
      "Error: 0.002652250000002801 Prediction: 0.7484999999999729\n",
      "Error: 0.002601000000002779 Prediction: 0.7489999999999728\n",
      "Error: 0.0025502500000027573 Prediction: 0.7494999999999727\n",
      "Error: 0.0025000000000027357 Prediction: 0.7499999999999727\n",
      "Error: 0.0024502500000027137 Prediction: 0.7504999999999726\n",
      "Error: 0.0024010000000026918 Prediction: 0.7509999999999726\n",
      "Error: 0.0023522500000026695 Prediction: 0.7514999999999725\n",
      "Error: 0.0023040000000026472 Prediction: 0.7519999999999725\n",
      "Error: 0.002256250000002625 Prediction: 0.7524999999999724\n",
      "Error: 0.0022090000000026025 Prediction: 0.7529999999999724\n",
      "Error: 0.00216225000000258 Prediction: 0.7534999999999723\n",
      "Error: 0.0021160000000025572 Prediction: 0.7539999999999722\n",
      "Error: 0.0020702500000025345 Prediction: 0.7544999999999722\n",
      "Error: 0.0020250000000025118 Prediction: 0.7549999999999721\n",
      "Error: 0.0019802500000024887 Prediction: 0.7554999999999721\n",
      "Error: 0.0019360000000024655 Prediction: 0.755999999999972\n",
      "Error: 0.0018922500000024423 Prediction: 0.756499999999972\n",
      "Error: 0.0018490000000024188 Prediction: 0.7569999999999719\n",
      "Error: 0.0018062500000023956 Prediction: 0.7574999999999719\n",
      "Error: 0.001764000000002372 Prediction: 0.7579999999999718\n",
      "Error: 0.0017222500000023482 Prediction: 0.7584999999999718\n",
      "Error: 0.0016810000000023245 Prediction: 0.7589999999999717\n",
      "Error: 0.0016402500000023007 Prediction: 0.7594999999999716\n",
      "Error: 0.0016000000000022767 Prediction: 0.7599999999999716\n",
      "Error: 0.0015602500000022525 Prediction: 0.7604999999999715\n",
      "Error: 0.0015210000000022283 Prediction: 0.7609999999999715\n",
      "Error: 0.001482250000002204 Prediction: 0.7614999999999714\n",
      "Error: 0.0014440000000021794 Prediction: 0.7619999999999714\n",
      "Error: 0.001406250000002155 Prediction: 0.7624999999999713\n",
      "Error: 0.0013690000000021302 Prediction: 0.7629999999999713\n",
      "Error: 0.0013322500000021056 Prediction: 0.7634999999999712\n",
      "Error: 0.0012960000000020806 Prediction: 0.7639999999999711\n",
      "Error: 0.0012602500000020557 Prediction: 0.7644999999999711\n",
      "Error: 0.0012250000000020305 Prediction: 0.764999999999971\n",
      "Error: 0.0011902500000020052 Prediction: 0.765499999999971\n",
      "Error: 0.00115600000000198 Prediction: 0.7659999999999709\n",
      "Error: 0.0011222500000019546 Prediction: 0.7664999999999709\n",
      "Error: 0.0010890000000019291 Prediction: 0.7669999999999708\n",
      "Error: 0.0010562500000019033 Prediction: 0.7674999999999708\n",
      "Error: 0.0010240000000018776 Prediction: 0.7679999999999707\n",
      "Error: 0.0009922500000018517 Prediction: 0.7684999999999707\n",
      "Error: 0.0009610000000018258 Prediction: 0.7689999999999706\n",
      "Error: 0.0009302500000017998 Prediction: 0.7694999999999705\n",
      "Error: 0.0009000000000017735 Prediction: 0.7699999999999705\n",
      "Error: 0.0008702500000017472 Prediction: 0.7704999999999704\n",
      "Error: 0.0008410000000017208 Prediction: 0.7709999999999704\n",
      "Error: 0.0008122500000016942 Prediction: 0.7714999999999703\n",
      "Error: 0.0007840000000016676 Prediction: 0.7719999999999703\n",
      "Error: 0.0007562500000016409 Prediction: 0.7724999999999702\n",
      "Error: 0.000729000000001614 Prediction: 0.7729999999999702\n",
      "Error: 0.000702250000001587 Prediction: 0.7734999999999701\n",
      "Error: 0.0006760000000015599 Prediction: 0.77399999999997\n",
      "Error: 0.0006502500000015327 Prediction: 0.77449999999997\n",
      "Error: 0.0006250000000015054 Prediction: 0.7749999999999699\n",
      "Error: 0.0006002500000014781 Prediction: 0.7754999999999699\n",
      "Error: 0.0005760000000014506 Prediction: 0.7759999999999698\n",
      "Error: 0.0005522500000014229 Prediction: 0.7764999999999698\n",
      "Error: 0.0005290000000013951 Prediction: 0.7769999999999697\n",
      "Error: 0.0005062500000013673 Prediction: 0.7774999999999697\n",
      "Error: 0.00048400000000133937 Prediction: 0.7779999999999696\n",
      "Error: 0.0004622500000013113 Prediction: 0.7784999999999695\n",
      "Error: 0.0004410000000012831 Prediction: 0.7789999999999695\n",
      "Error: 0.0004202500000012548 Prediction: 0.7794999999999694\n",
      "Error: 0.0004000000000012264 Prediction: 0.7799999999999694\n",
      "Error: 0.0003802500000011979 Prediction: 0.7804999999999693\n",
      "Error: 0.00036100000000116925 Prediction: 0.7809999999999693\n",
      "Error: 0.0003422500000011405 Prediction: 0.7814999999999692\n",
      "Error: 0.0003240000000011117 Prediction: 0.7819999999999692\n",
      "Error: 0.00030625000000108273 Prediction: 0.7824999999999691\n",
      "Error: 0.00028900000000105366 Prediction: 0.782999999999969\n",
      "Error: 0.0002722500000010245 Prediction: 0.783499999999969\n",
      "Error: 0.00025600000000099523 Prediction: 0.7839999999999689\n",
      "Error: 0.00024025000000096582 Prediction: 0.7844999999999689\n",
      "Error: 0.0002250000000009363 Prediction: 0.7849999999999688\n",
      "Error: 0.0002102500000009067 Prediction: 0.7854999999999688\n",
      "Error: 0.00019600000000087698 Prediction: 0.7859999999999687\n",
      "Error: 0.00018225000000084715 Prediction: 0.7864999999999687\n",
      "Error: 0.0001690000000008172 Prediction: 0.7869999999999686\n",
      "Error: 0.00015625000000078716 Prediction: 0.7874999999999686\n",
      "Error: 0.000144000000000757 Prediction: 0.7879999999999685\n",
      "Error: 0.0001322500000007267 Prediction: 0.7884999999999684\n",
      "Error: 0.00012100000000069633 Prediction: 0.7889999999999684\n",
      "Error: 0.00011025000000066583 Prediction: 0.7894999999999683\n",
      "Error: 0.00010000000000063523 Prediction: 0.7899999999999683\n",
      "Error: 9.025000000060451e-05 Prediction: 0.7904999999999682\n",
      "Error: 8.100000000057368e-05 Prediction: 0.7909999999999682\n",
      "Error: 7.225000000054275e-05 Prediction: 0.7914999999999681\n",
      "Error: 6.40000000005117e-05 Prediction: 0.7919999999999681\n",
      "Error: 5.625000000048055e-05 Prediction: 0.792499999999968\n",
      "Error: 4.9000000000449285e-05 Prediction: 0.792999999999968\n",
      "Error: 4.225000000041791e-05 Prediction: 0.7934999999999679\n",
      "Error: 3.6000000000386424e-05 Prediction: 0.7939999999999678\n",
      "Error: 3.0250000000354826e-05 Prediction: 0.7944999999999678\n",
      "Error: 2.500000000032312e-05 Prediction: 0.7949999999999677\n",
      "Error: 2.0250000000291302e-05 Prediction: 0.7954999999999677\n",
      "Error: 1.6000000000259378e-05 Prediction: 0.7959999999999676\n",
      "Error: 1.225000000022734e-05 Prediction: 0.7964999999999676\n",
      "Error: 9.000000000195194e-06 Prediction: 0.7969999999999675\n",
      "Error: 6.250000000162936e-06 Prediction: 0.7974999999999675\n",
      "Error: 4.000000000130569e-06 Prediction: 0.7979999999999674\n",
      "Error: 2.2500000000980924e-06 Prediction: 0.7984999999999673\n",
      "Error: 1.000000000065505e-06 Prediction: 0.7989999999999673\n",
      "Error: 2.5000000003280753e-07 Prediction: 0.7994999999999672\n",
      "Error: 1.0799505792475652e-27 Prediction: 0.7999999999999672\n"
     ]
    }
   ],
   "source": [
    "# Page 77\n",
    "weight = 0.5\n",
    "inp = 0.5\n",
    "goal_prediction = 0.8\n",
    "\n",
    "step_amount = 0.001\n",
    "\n",
    "for i in range(1101):\n",
    "    prediction = inp * weight\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    \n",
    "    print(\"Error: \" + str(error) + \" Prediction: \" + str(prediction))\n",
    "    \n",
    "    up_prediction = inp * (weight + step_amount)\n",
    "    up_error = (goal_prediction - up_prediction) ** 2\n",
    "    \n",
    "    down_prediction = inp * (weight - step_amount)\n",
    "    down_error = (goal_prediction - down_prediction) ** 2\n",
    "    \n",
    "    if down_error < up_error:\n",
    "        weight -= step_amount\n",
    "    elif up_error < down_error:\n",
    "        weight += step_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Weight:0.0\n",
      "Error:0.6400000000000001 Prediction:0.0\n",
      "Delta:-0.8 Weight Delta:-0.8800000000000001\n",
      "---------\n",
      "Weight:0.8800000000000001\n",
      "Error:0.02822400000000005 Prediction:0.9680000000000002\n",
      "Delta:0.16800000000000015 Weight Delta:0.1848000000000002\n",
      "---------\n",
      "Weight:0.6951999999999999\n",
      "Error:0.0012446784000000064 Prediction:0.76472\n",
      "Delta:-0.03528000000000009 Weight Delta:-0.0388080000000001\n",
      "---------\n",
      "Weight:0.734008\n",
      "Error:5.4890317439999896e-05 Prediction:0.8074088\n",
      "Delta:0.007408799999999993 Weight Delta:0.008149679999999992\n"
     ]
    }
   ],
   "source": [
    "# Page 86\n",
    "weight, goal_pred, inp = (0.0, 0.8, 1.1)\n",
    "    \n",
    "for iteration in range(4):\n",
    "    print(\"---------\\nWeight:\" + str(weight))\n",
    "    \n",
    "    pred = inp * weight\n",
    "    error = (pred - goal_pred) ** 2\n",
    "    delta = pred - goal_pred\n",
    "    weight_delta = delta * inp\n",
    "    weight -= weight_delta\n",
    "    \n",
    "    print(\"Error:\" + str(error) + \" Prediction:\" + str(pred))\n",
    "    print(\"Delta:\" + str(delta) + \" Weight Delta:\" + str(weight_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2420.6400000000003Prediction: 50.0\n",
      "Error: 24.20639999999992Prediction: 5.719999999999992\n",
      "Error: 0.24206399999999867Prediction: 1.2919999999999987\n",
      "Error: 0.0024206399999999913Prediction: 0.8492\n",
      "Error: 2.4206399999999257e-05Prediction: 0.80492\n",
      "Error: 2.420640000000472e-07Prediction: 0.8004920000000001\n",
      "Error: 2.4206399999971946e-09Prediction: 0.8000492\n",
      "Error: 2.4206399999644208e-11Prediction: 0.80000492\n",
      "Error: 2.420639999636683e-13Prediction: 0.800000492\n",
      "Number of iterations: 9\n"
     ]
    }
   ],
   "source": [
    "# My own gradient-boosting after Chapter 4\n",
    "inp = 100\n",
    "weight = 0.5\n",
    "goal_pred = 0.8\n",
    "\n",
    "error = 1\n",
    "alpha = 0.00009\n",
    "\n",
    "count_of_iterations = 0\n",
    "\n",
    "while error > 10 ** -12:\n",
    "    count_of_iterations += 1\n",
    "    \n",
    "    pred = inp * weight\n",
    "    error = (pred - goal_pred) ** 2\n",
    "    \n",
    "    boost = alpha * inp * (pred - goal_pred)\n",
    "    weight -= boost\n",
    "    \n",
    "    print(\"Error: \" + str(error) + \"Prediction: \" + str(pred))\n",
    "    \n",
    "print(\"Number of iterations: \" + str(count_of_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Github\n"
     ]
    }
   ],
   "source": [
    "#New practice\n",
    "hey_you = 'Hello Github'\n",
    "print(hey_you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2.6561231104\n",
      "Error: 0.9628701776715985\n",
      "Error: 0.5509165866836797\n",
      "Error: 0.36445836852222424\n",
      "Error: 0.2516768662079895\n",
      "Error: 0.17797575048089034\n",
      "Error: 0.12864460733422164\n",
      "Error: 0.09511036950476208\n",
      "Error: 0.07194564247043436\n",
      "Error: 0.05564914990717743\n",
      "Error: 0.04394763937673939\n",
      "Error: 0.035357967050948465\n",
      "Error: 0.02890700056547436\n",
      "Error: 0.023951660591138853\n",
      "Error: 0.020063105176016144\n",
      "Error: 0.016952094519447087\n",
      "Error: 0.014420818295271236\n",
      "Error: 0.012331739998443648\n",
      "Error: 0.010587393171639842\n",
      "Error: 0.009117233405426495\n",
      "Error: 0.00786904226904208\n",
      "Error: 0.006803273214640502\n",
      "Error: 0.005889303541837786\n",
      "Error: 0.0051029252561172675\n",
      "Error: 0.004424644608684828\n",
      "Error: 0.0038385124412518303\n",
      "Error: 0.0033313054558089675\n",
      "Error: 0.0028919416227737734\n",
      "Error: 0.002511053608117256\n",
      "Error: 0.0021806703520253884\n",
      "Error: 0.0018939739123713475\n",
      "Error: 0.0016451096996342332\n",
      "Error: 0.0014290353984827077\n",
      "Error: 0.0012413985592149145\n",
      "Error: 0.0010784359268087556\n",
      "Error: 0.0009368896209360312\n",
      "Error: 0.0008139366504753339\n",
      "Error: 0.0007071291752624441\n",
      "Error: 0.0006143435674831474\n",
      "Error: 0.00053373677328488\n"
     ]
    }
   ],
   "source": [
    "# Page 138\n",
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array( [[1, 0, 1],\n",
    "                          [0, 1, 1],\n",
    "                          [0, 0, 1],\n",
    "                          [1, 1, 1],\n",
    "                          [0, 1, 1], \n",
    "                          [1, 0, 1]] )\n",
    "\n",
    "walk_vs_stop = np.array([0, 1, 0, 1, 1, 0])\n",
    "\n",
    "inp = streetlights[0]\n",
    "goal_pred = walk_vs_stop[0]\n",
    "\n",
    "for i in range(40):\n",
    "    error_for_all_lights = 0\n",
    "    \n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        inp = streetlights[row_index]\n",
    "        goal_pred = walk_vs_stop[row_index]\n",
    "        \n",
    "        pred = inp.dot(weights)\n",
    "        \n",
    "        error = (goal_pred - pred) ** 2\n",
    "        error_for_all_lights += error\n",
    "        \n",
    "        delta = pred - goal_pred\n",
    "        weights -= alpha * (inp * delta)\n",
    "        # print('Prediction: {}'.format(pred))\n",
    "        \n",
    "    print('Error: {}'.format(error_for_all_lights))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.31298811341557736\n",
      "Error:0.043272385299784624\n",
      "Error:0.0034372657015086937\n",
      "Error:0.00019649599346773566\n",
      "Error:1.015593230102771e-05\n",
      "Error:5.122102420300414e-07\n"
     ]
    }
   ],
   "source": [
    "# Page 157 (162)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output > 0\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "streetlights = np.array( [[1, 0, 1],\n",
    "                          [0, 1, 1],\n",
    "                          [0, 0, 1],\n",
    "                          [1, 1, 1],\n",
    "                          [0, 1, 1], \n",
    "                          [1, 0, 1]] )\n",
    "\n",
    "walk_vs_stop = np.array([0, 1, 0, 1, 1, 0])\n",
    "\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "# print(weights_0_1)\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "        \n",
    "        layer_2_delta = layer_2 - walk_vs_stop[i:i+1]\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        \n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "    if (iteration % 10 == 9): # Эта строка вычисляет разность в слое\n",
    "        print (\"Error:\" + str(layer_2_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "# Test\n",
    "lights = np.array( [[1, 0, 1],\n",
    "                    [0, 1, 1],\n",
    "                    [0, 0, 1],\n",
    "                    [1, 1, 1],\n",
    "                    [0, 1, 1], \n",
    "                    [1, 0, 1]] )\n",
    "\n",
    "print(lights[4:5] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.718 Test-Acc:0.5418 Train-Err:0.885 Train-Err:0.289\n",
      "I:10 Test-Err:0.501 Test-Acc:0.7365 Train-Err:0.564 Train-Err:0.647\n",
      "I:20 Test-Err:0.478 Test-Acc:0.7621 Train-Err:0.530 Train-Err:0.681\n",
      "I:30 Test-Err:0.457 Test-Acc:0.7915 Train-Err:0.508 Train-Err:0.71\n",
      "I:40 Test-Err:0.445 Test-Acc:0.7998 Train-Err:0.492 Train-Err:0.719\n",
      "I:50 Test-Err:0.430 Test-Acc:0.8145 Train-Err:0.462 Train-Err:0.742\n",
      "I:60 Test-Err:0.446 Test-Acc:0.7974 Train-Err:0.472 Train-Err:0.746\n",
      "I:70 Test-Err:0.445 Test-Acc:0.7901 Train-Err:0.463 Train-Err:0.744\n",
      "I:80 Test-Err:0.426 Test-Acc:0.8105 Train-Err:0.461 Train-Err:0.764\n",
      "I:90 Test-Err:0.435 Test-Acc:0.7871 Train-Err:0.462 Train-Err:0.749\n",
      "I:100 Test-Err:0.433 Test-Acc:0.8039 Train-Err:0.452 Train-Err:0.769\n",
      "I:110 Test-Err:0.435 Test-Acc:0.8099 Train-Err:0.439 Train-Err:0.778\n",
      "I:120 Test-Err:0.442 Test-Acc:0.7871 Train-Err:0.451 Train-Err:0.778\n",
      "I:130 Test-Err:0.439 Test-Acc:0.811 Train-Err:0.452 Train-Err:0.783\n",
      "I:140 Test-Err:0.443 Test-Acc:0.8049 Train-Err:0.445 Train-Err:0.779\n",
      "I:150 Test-Err:0.446 Test-Acc:0.7918 Train-Err:0.457 Train-Err:0.783\n",
      "I:160 Test-Err:0.437 Test-Acc:0.81 Train-Err:0.456 Train-Err:0.774\n",
      "I:170 Test-Err:0.430 Test-Acc:0.7963 Train-Err:0.439 Train-Err:0.801\n",
      "I:180 Test-Err:0.432 Test-Acc:0.7955 Train-Err:0.453 Train-Err:0.782\n",
      "I:190 Test-Err:0.436 Test-Acc:0.7997 Train-Err:0.433 Train-Err:0.784\n",
      "I:200 Test-Err:0.436 Test-Acc:0.803 Train-Err:0.442 Train-Err:0.796\n",
      "I:210 Test-Err:0.434 Test-Acc:0.8031 Train-Err:0.441 Train-Err:0.79\n",
      "I:220 Test-Err:0.426 Test-Acc:0.8102 Train-Err:0.434 Train-Err:0.777\n",
      "I:230 Test-Err:0.429 Test-Acc:0.8058 Train-Err:0.431 Train-Err:0.803\n",
      "I:240 Test-Err:0.436 Test-Acc:0.8055 Train-Err:0.430 Train-Err:0.788\n",
      "I:250 Test-Err:0.421 Test-Acc:0.8053 Train-Err:0.433 Train-Err:0.789\n",
      "I:260 Test-Err:0.422 Test-Acc:0.8102 Train-Err:0.422 Train-Err:0.79\n",
      "I:270 Test-Err:0.438 Test-Acc:0.8062 Train-Err:0.430 Train-Err:0.803\n",
      "I:280 Test-Err:0.431 Test-Acc:0.7991 Train-Err:0.425 Train-Err:0.79\n",
      "I:290 Test-Err:0.433 Test-Acc:0.8028 Train-Err:0.428 Train-Err:0.792\n",
      "I:300 Test-Err:0.434 Test-Acc:0.7949 Train-Err:0.407 Train-Err:0.804\n",
      "I:310 Test-Err:0.428 Test-Acc:0.8036 Train-Err:0.415 Train-Err:0.793\n",
      "I:320 Test-Err:0.436 Test-Acc:0.8008 Train-Err:0.415 Train-Err:0.812\n",
      "I:330 Test-Err:0.419 Test-Acc:0.8134 Train-Err:0.418 Train-Err:0.817\n",
      "I:340 Test-Err:0.431 Test-Acc:0.8012 Train-Err:0.408 Train-Err:0.814"
     ]
    }
   ],
   "source": [
    "# Page 180 - 190\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "np.random.seed(1)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) /\\\n",
    "                                     255, y_train[0:1000])\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i, l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "    \n",
    "relu = lambda x: (x >= 0) * x\n",
    "relu2deriv = lambda output: output >= 0\n",
    "\n",
    "alpha, iterations, hidden_size = (0.005, 350, 40)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "\n",
    "        # Маска прореживания (регуляризация)\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        # Прореживаем первый слой\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        \n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                              np.argmax(labels[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = (labels[i:i+1] - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\\\n",
    "                                        * relu2deriv(layer_1)\n",
    "        # Добавляем прореживание в Back-Propagation\n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    if (j % 10 == 0):\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "        \n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "            \n",
    "            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                   np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write('\\n' + \\\n",
    "            'I:' + str(j) + \\\n",
    "            ' Test-Err:' + str(test_error/float(len(test_images)))[0:5] +\\\n",
    "            ' Test-Acc:' + str(test_correct_cnt/float(len(test_images))) +\\\n",
    "            ' Train-Err:' + str(error/float(len(images)))[0:5] +\\\n",
    "            ' Train-Err:' + str(correct_cnt/float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Activation Functions such as tanh, softmax and their backprop versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 212-214 - We are investigating new activation functions\n",
    "\n",
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28)\\\n",
    "                                                 / 255, y_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Page 212-214 - We are investigating new activation functions\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "for i, l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Page 212-214 - We are investigating new activation functions\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "    \n",
    "alpha, iterations, hidden_size = (2, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 100\n",
    "\n",
    "weights_0_1 = 0.02 * np.random.random((pixels_per_image, hidden_size)) - 0.01\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.394 Train-Acc:0.156\n",
      "I:10 Test-Acc:0.6867 Train-Acc:0.723\n",
      "I:20 Test-Acc:0.7025 Train-Acc:0.732\n",
      "I:30 Test-Acc:0.734 Train-Acc:0.763\n",
      "I:40 Test-Acc:0.7663 Train-Acc:0.794\n",
      "I:50 Test-Acc:0.7913 Train-Acc:0.819\n",
      "I:60 Test-Acc:0.8102 Train-Acc:0.849\n",
      "I:70 Test-Acc:0.8228 Train-Acc:0.864\n",
      "I:80 Test-Acc:0.831 Train-Acc:0.867\n",
      "I:90 Test-Acc:0.8364 Train-Acc:0.885\n",
      "I:100 Test-Acc:0.8407 Train-Acc:0.883\n",
      "I:110 Test-Acc:0.845 Train-Acc:0.891\n",
      "I:120 Test-Acc:0.8481 Train-Acc:0.901\n",
      "I:130 Test-Acc:0.8505 Train-Acc:0.901\n",
      "I:140 Test-Acc:0.8526 Train-Acc:0.905\n",
      "I:150 Test-Acc:0.8555 Train-Acc:0.914\n",
      "I:160 Test-Acc:0.8577 Train-Acc:0.925\n",
      "I:170 Test-Acc:0.8596 Train-Acc:0.918\n",
      "I:180 Test-Acc:0.8619 Train-Acc:0.933\n",
      "I:190 Test-Acc:0.863 Train-Acc:0.933\n",
      "I:200 Test-Acc:0.8642 Train-Acc:0.926\n",
      "I:210 Test-Acc:0.8653 Train-Acc:0.931\n",
      "I:220 Test-Acc:0.8668 Train-Acc:0.93\n",
      "I:230 Test-Acc:0.8672 Train-Acc:0.937\n",
      "I:240 Test-Acc:0.8681 Train-Acc:0.938\n",
      "I:250 Test-Acc:0.8687 Train-Acc:0.937\n",
      "I:260 Test-Acc:0.8684 Train-Acc:0.945\n",
      "I:270 Test-Acc:0.8703 Train-Acc:0.951\n",
      "I:280 Test-Acc:0.8699 Train-Acc:0.949\n",
      "I:290 Test-Acc:0.8701 Train-Acc:0.94"
     ]
    }
   ],
   "source": [
    "#4 Page 212-214 - We are investigating new activation functions\n",
    "\n",
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
    "        \n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == \\\n",
    "                              np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "        \n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2) \\\n",
    "                        / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) \\\n",
    "                        * tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    test_correct_cnt = 0\n",
    "    \n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i:i+1]\n",
    "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                              np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "    if (j % 10 == 0):\n",
    "        sys.stdout.write(\"\\n\"+ \"I:\" + str(j) + \\\n",
    "        \" Test-Acc:\"+str(test_correct_cnt/float(len(test_images)))+\\\n",
    "        \" Train-Acc:\" + str(correct_cnt/float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 221-223 - We are investigating convolutional neural networks\n",
    "\n",
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Page 221-223 - We are investigating convolutional neural networks\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255,\n",
    "                  y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "for i, l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Page 221-223 - We are investigating convolutional neural networks\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations = (2, 300)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 128\n",
    "\n",
    "input_rows = 28\n",
    "input_cols = 28\n",
    "\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernels = 16\n",
    "\n",
    "hidden_size = ((input_rows - kernel_rows) *\n",
    "              (input_cols - kernel_cols)) * num_kernels\n",
    "\n",
    "kernels = 0.02 * np.random.random((kernel_rows * kernel_cols,\n",
    "                                 num_kernels)) - 0.01\n",
    "\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size,\n",
    "                                     num_labels)) - 0.1\n",
    "\n",
    "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
    "    section = layer[:, row_from:row_to, col_from:col_to]\n",
    "    return section.reshape(-1, 1, row_to-row_from, col_to-col_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.4023 Train-Acc:0.209\n",
      "I:1 Test-Acc:0.4358 Train-Acc:0.238\n",
      "I:2 Test-Acc:0.4473 Train-Acc:0.286\n",
      "I:3 Test-Acc:0.4389 Train-Acc:0.274\n",
      "I:4 Test-Acc:0.3951 Train-Acc:0.257\n",
      "I:5 Test-Acc:0.2222 Train-Acc:0.243\n",
      "I:6 Test-Acc:0.0613 Train-Acc:0.112\n",
      "I:7 Test-Acc:0.0266 Train-Acc:0.035\n",
      "I:8 Test-Acc:0.0127 Train-Acc:0.026\n",
      "I:9 Test-Acc:0.0133 Train-Acc:0.022\n",
      "I:10 Test-Acc:0.0185 Train-Acc:0.038\n",
      "I:11 Test-Acc:0.0363 Train-Acc:0.038\n",
      "I:12 Test-Acc:0.0928 Train-Acc:0.067\n",
      "I:13 Test-Acc:0.1994 Train-Acc:0.081\n",
      "I:14 Test-Acc:0.3086 Train-Acc:0.154\n",
      "I:15 Test-Acc:0.4276 Train-Acc:0.204\n",
      "I:16 Test-Acc:0.5323 Train-Acc:0.256\n",
      "I:17 Test-Acc:0.5919 Train-Acc:0.305\n",
      "I:18 Test-Acc:0.6324 Train-Acc:0.341\n",
      "I:19 Test-Acc:0.6608 Train-Acc:0.426\n",
      "I:20 Test-Acc:0.6815 Train-Acc:0.439\n",
      "I:21 Test-Acc:0.7048 Train-Acc:0.462\n",
      "I:22 Test-Acc:0.7171 Train-Acc:0.484\n",
      "I:23 Test-Acc:0.7313 Train-Acc:0.505\n",
      "I:24 Test-Acc:0.7355 Train-Acc:0.53\n",
      "I:25 Test-Acc:0.7417 Train-Acc:0.548\n",
      "I:26 Test-Acc:0.747 Train-Acc:0.534\n",
      "I:27 Test-Acc:0.7491 Train-Acc:0.55\n",
      "I:28 Test-Acc:0.7459 Train-Acc:0.562\n",
      "I:29 Test-Acc:0.7352 Train-Acc:0.54\n",
      "I:30 Test-Acc:0.7082 Train-Acc:0.496\n",
      "I:31 Test-Acc:0.6487 Train-Acc:0.456\n",
      "I:32 Test-Acc:0.5209 Train-Acc:0.353\n",
      "I:33 Test-Acc:0.3305 Train-Acc:0.234\n",
      "I:34 Test-Acc:0.2052 Train-Acc:0.174\n",
      "I:35 Test-Acc:0.2149 Train-Acc:0.136\n",
      "I:36 Test-Acc:0.2679 Train-Acc:0.171\n",
      "I:37 Test-Acc:0.3237 Train-Acc:0.172\n",
      "I:38 Test-Acc:0.3581 Train-Acc:0.186\n",
      "I:39 Test-Acc:0.4202 Train-Acc:0.21\n",
      "I:40 Test-Acc:0.5165 Train-Acc:0.223\n",
      "I:41 Test-Acc:0.6007 Train-Acc:0.262\n",
      "I:42 Test-Acc:0.6476 Train-Acc:0.308\n",
      "I:43 Test-Acc:0.676 Train-Acc:0.363\n",
      "I:44 Test-Acc:0.696 Train-Acc:0.402\n",
      "I:45 Test-Acc:0.7077 Train-Acc:0.434\n",
      "I:46 Test-Acc:0.7204 Train-Acc:0.441\n",
      "I:47 Test-Acc:0.7303 Train-Acc:0.475\n",
      "I:48 Test-Acc:0.7359 Train-Acc:0.475\n",
      "I:49 Test-Acc:0.7401 Train-Acc:0.525\n",
      "I:50 Test-Acc:0.7493 Train-Acc:0.517\n",
      "I:51 Test-Acc:0.7533 Train-Acc:0.517\n",
      "I:52 Test-Acc:0.7606 Train-Acc:0.538\n",
      "I:53 Test-Acc:0.7644 Train-Acc:0.554\n",
      "I:54 Test-Acc:0.7724 Train-Acc:0.57\n",
      "I:55 Test-Acc:0.7788 Train-Acc:0.586\n",
      "I:56 Test-Acc:0.7855 Train-Acc:0.595\n",
      "I:57 Test-Acc:0.7853 Train-Acc:0.591\n",
      "I:58 Test-Acc:0.7925 Train-Acc:0.605\n",
      "I:59 Test-Acc:0.7973 Train-Acc:0.64\n",
      "I:60 Test-Acc:0.8013 Train-Acc:0.621\n",
      "I:61 Test-Acc:0.8029 Train-Acc:0.626\n",
      "I:62 Test-Acc:0.8092 Train-Acc:0.631\n",
      "I:63 Test-Acc:0.8099 Train-Acc:0.638\n",
      "I:64 Test-Acc:0.8156 Train-Acc:0.661\n",
      "I:65 Test-Acc:0.8156 Train-Acc:0.639\n",
      "I:66 Test-Acc:0.8184 Train-Acc:0.65\n",
      "I:67 Test-Acc:0.8216 Train-Acc:0.67\n",
      "I:68 Test-Acc:0.8246 Train-Acc:0.675\n",
      "I:69 Test-Acc:0.8237 Train-Acc:0.666\n",
      "I:70 Test-Acc:0.8273 Train-Acc:0.673\n",
      "I:71 Test-Acc:0.8273 Train-Acc:0.704\n",
      "I:72 Test-Acc:0.8314 Train-Acc:0.674\n",
      "I:73 Test-Acc:0.8292 Train-Acc:0.686\n",
      "I:74 Test-Acc:0.8335 Train-Acc:0.699\n",
      "I:75 Test-Acc:0.8359 Train-Acc:0.694\n",
      "I:76 Test-Acc:0.8375 Train-Acc:0.704\n",
      "I:77 Test-Acc:0.8373 Train-Acc:0.697\n",
      "I:78 Test-Acc:0.8398 Train-Acc:0.704\n",
      "I:79 Test-Acc:0.8393 Train-Acc:0.687\n",
      "I:80 Test-Acc:0.8436 Train-Acc:0.705\n",
      "I:81 Test-Acc:0.8437 Train-Acc:0.711\n",
      "I:82 Test-Acc:0.8446 Train-Acc:0.721\n",
      "I:83 Test-Acc:0.845 Train-Acc:0.719\n",
      "I:84 Test-Acc:0.8469 Train-Acc:0.724\n",
      "I:85 Test-Acc:0.8476 Train-Acc:0.726\n",
      "I:86 Test-Acc:0.848 Train-Acc:0.718\n",
      "I:87 Test-Acc:0.8496 Train-Acc:0.719\n",
      "I:88 Test-Acc:0.85 Train-Acc:0.73\n",
      "I:89 Test-Acc:0.8511 Train-Acc:0.737\n",
      "I:90 Test-Acc:0.8503 Train-Acc:0.73\n",
      "I:91 Test-Acc:0.8504 Train-Acc:0.717\n",
      "I:92 Test-Acc:0.8528 Train-Acc:0.74\n",
      "I:93 Test-Acc:0.8532 Train-Acc:0.733\n",
      "I:94 Test-Acc:0.8537 Train-Acc:0.73\n",
      "I:95 Test-Acc:0.8568 Train-Acc:0.721\n",
      "I:96 Test-Acc:0.857 Train-Acc:0.75\n",
      "I:97 Test-Acc:0.8558 Train-Acc:0.731\n",
      "I:98 Test-Acc:0.8578 Train-Acc:0.744\n",
      "I:99 Test-Acc:0.8588 Train-Acc:0.754\n",
      "I:100 Test-Acc:0.8579 Train-Acc:0.732\n",
      "I:101 Test-Acc:0.8582 Train-Acc:0.747\n",
      "I:102 Test-Acc:0.8593 Train-Acc:0.747\n",
      "I:103 Test-Acc:0.8598 Train-Acc:0.751\n",
      "I:104 Test-Acc:0.8603 Train-Acc:0.74\n",
      "I:105 Test-Acc:0.86 Train-Acc:0.753\n",
      "I:106 Test-Acc:0.8588 Train-Acc:0.746\n",
      "I:107 Test-Acc:0.861 Train-Acc:0.741\n",
      "I:108 Test-Acc:0.8616 Train-Acc:0.731\n",
      "I:109 Test-Acc:0.8629 Train-Acc:0.753\n",
      "I:110 Test-Acc:0.8609 Train-Acc:0.743\n",
      "I:111 Test-Acc:0.8627 Train-Acc:0.752\n",
      "I:112 Test-Acc:0.8646 Train-Acc:0.76\n",
      "I:113 Test-Acc:0.8649 Train-Acc:0.766\n",
      "I:114 Test-Acc:0.8659 Train-Acc:0.752\n",
      "I:115 Test-Acc:0.868 Train-Acc:0.756\n",
      "I:116 Test-Acc:0.8648 Train-Acc:0.767\n",
      "I:117 Test-Acc:0.8662 Train-Acc:0.747\n",
      "I:118 Test-Acc:0.8669 Train-Acc:0.753\n",
      "I:119 Test-Acc:0.8694 Train-Acc:0.753\n",
      "I:120 Test-Acc:0.8692 Train-Acc:0.76\n",
      "I:121 Test-Acc:0.8658 Train-Acc:0.756\n",
      "I:122 Test-Acc:0.8666 Train-Acc:0.769\n",
      "I:123 Test-Acc:0.8692 Train-Acc:0.77\n",
      "I:124 Test-Acc:0.8681 Train-Acc:0.757\n",
      "I:125 Test-Acc:0.8705 Train-Acc:0.77\n",
      "I:126 Test-Acc:0.8706 Train-Acc:0.77\n",
      "I:127 Test-Acc:0.8684 Train-Acc:0.768\n",
      "I:128 Test-Acc:0.8664 Train-Acc:0.774\n",
      "I:129 Test-Acc:0.8666 Train-Acc:0.756\n",
      "I:130 Test-Acc:0.8705 Train-Acc:0.783\n",
      "I:131 Test-Acc:0.87 Train-Acc:0.775\n",
      "I:132 Test-Acc:0.8729 Train-Acc:0.769\n",
      "I:133 Test-Acc:0.8725 Train-Acc:0.776\n",
      "I:134 Test-Acc:0.8721 Train-Acc:0.772\n",
      "I:135 Test-Acc:0.8718 Train-Acc:0.765\n",
      "I:136 Test-Acc:0.8746 Train-Acc:0.777\n",
      "I:137 Test-Acc:0.8746 Train-Acc:0.77\n",
      "I:138 Test-Acc:0.8734 Train-Acc:0.778\n",
      "I:139 Test-Acc:0.873 Train-Acc:0.785\n",
      "I:140 Test-Acc:0.8732 Train-Acc:0.76\n",
      "I:141 Test-Acc:0.8727 Train-Acc:0.779\n",
      "I:142 Test-Acc:0.8754 Train-Acc:0.772\n",
      "I:143 Test-Acc:0.8729 Train-Acc:0.773\n",
      "I:144 Test-Acc:0.8758 Train-Acc:0.784\n",
      "I:145 Test-Acc:0.8732 Train-Acc:0.774\n",
      "I:146 Test-Acc:0.8743 Train-Acc:0.782\n",
      "I:147 Test-Acc:0.8762 Train-Acc:0.772\n",
      "I:148 Test-Acc:0.8755 Train-Acc:0.79\n",
      "I:149 Test-Acc:0.8751 Train-Acc:0.774\n",
      "I:150 Test-Acc:0.8749 Train-Acc:0.782\n",
      "I:151 Test-Acc:0.8744 Train-Acc:0.78\n",
      "I:152 Test-Acc:0.8765 Train-Acc:0.782\n",
      "I:153 Test-Acc:0.8738 Train-Acc:0.796\n",
      "I:154 Test-Acc:0.8753 Train-Acc:0.798\n",
      "I:155 Test-Acc:0.8767 Train-Acc:0.794\n",
      "I:156 Test-Acc:0.8746 Train-Acc:0.784\n",
      "I:157 Test-Acc:0.8769 Train-Acc:0.796\n",
      "I:158 Test-Acc:0.8758 Train-Acc:0.789\n",
      "I:159 Test-Acc:0.8764 Train-Acc:0.79\n",
      "I:160 Test-Acc:0.873 Train-Acc:0.791\n",
      "I:161 Test-Acc:0.8765 Train-Acc:0.797\n",
      "I:162 Test-Acc:0.8772 Train-Acc:0.789\n",
      "I:163 Test-Acc:0.8778 Train-Acc:0.781\n",
      "I:164 Test-Acc:0.8758 Train-Acc:0.799\n",
      "I:165 Test-Acc:0.8773 Train-Acc:0.785\n",
      "I:166 Test-Acc:0.8766 Train-Acc:0.796\n",
      "I:167 Test-Acc:0.8782 Train-Acc:0.803\n",
      "I:168 Test-Acc:0.8789 Train-Acc:0.794\n",
      "I:169 Test-Acc:0.8778 Train-Acc:0.794\n",
      "I:170 Test-Acc:0.8778 Train-Acc:0.8\n",
      "I:171 Test-Acc:0.8785 Train-Acc:0.791\n",
      "I:172 Test-Acc:0.8777 Train-Acc:0.787\n",
      "I:173 Test-Acc:0.8769 Train-Acc:0.781\n",
      "I:174 Test-Acc:0.8765 Train-Acc:0.786\n",
      "I:175 Test-Acc:0.8765 Train-Acc:0.793\n",
      "I:176 Test-Acc:0.8785 Train-Acc:0.796\n",
      "I:177 Test-Acc:0.879 Train-Acc:0.789\n",
      "I:178 Test-Acc:0.8763 Train-Acc:0.79\n",
      "I:179 Test-Acc:0.8774 Train-Acc:0.787\n",
      "I:180 Test-Acc:0.8766 Train-Acc:0.782\n",
      "I:181 Test-Acc:0.8803 Train-Acc:0.798\n",
      "I:182 Test-Acc:0.8781 Train-Acc:0.789\n",
      "I:183 Test-Acc:0.8795 Train-Acc:0.785\n",
      "I:184 Test-Acc:0.8791 Train-Acc:0.807\n",
      "I:185 Test-Acc:0.8778 Train-Acc:0.796\n",
      "I:186 Test-Acc:0.8783 Train-Acc:0.801\n",
      "I:187 Test-Acc:0.8778 Train-Acc:0.81\n",
      "I:188 Test-Acc:0.8771 Train-Acc:0.784\n",
      "I:189 Test-Acc:0.8776 Train-Acc:0.792\n",
      "I:190 Test-Acc:0.8784 Train-Acc:0.794\n",
      "I:191 Test-Acc:0.8787 Train-Acc:0.795\n",
      "I:192 Test-Acc:0.8803 Train-Acc:0.781\n",
      "I:193 Test-Acc:0.8798 Train-Acc:0.804\n",
      "I:194 Test-Acc:0.8779 Train-Acc:0.779\n",
      "I:195 Test-Acc:0.8788 Train-Acc:0.792\n",
      "I:196 Test-Acc:0.8764 Train-Acc:0.793\n",
      "I:197 Test-Acc:0.8792 Train-Acc:0.792\n",
      "I:198 Test-Acc:0.8798 Train-Acc:0.803\n",
      "I:199 Test-Acc:0.8788 Train-Acc:0.804\n",
      "I:200 Test-Acc:0.8793 Train-Acc:0.797\n",
      "I:201 Test-Acc:0.8764 Train-Acc:0.791\n",
      "I:202 Test-Acc:0.8801 Train-Acc:0.801\n",
      "I:203 Test-Acc:0.8814 Train-Acc:0.799\n",
      "I:204 Test-Acc:0.8806 Train-Acc:0.79\n",
      "I:205 Test-Acc:0.8799 Train-Acc:0.8\n",
      "I:206 Test-Acc:0.8803 Train-Acc:0.802\n",
      "I:207 Test-Acc:0.8782 Train-Acc:0.807\n",
      "I:208 Test-Acc:0.8818 Train-Acc:0.797\n",
      "I:209 Test-Acc:0.8793 Train-Acc:0.799\n",
      "I:210 Test-Acc:0.8789 Train-Acc:0.815\n",
      "I:211 Test-Acc:0.8791 Train-Acc:0.816\n",
      "I:212 Test-Acc:0.8793 Train-Acc:0.809\n",
      "I:213 Test-Acc:0.8814 Train-Acc:0.795\n",
      "I:214 Test-Acc:0.8798 Train-Acc:0.799\n",
      "I:215 Test-Acc:0.8805 Train-Acc:0.806\n",
      "I:216 Test-Acc:0.88 Train-Acc:0.808\n",
      "I:217 Test-Acc:0.8782 Train-Acc:0.801\n",
      "I:218 Test-Acc:0.8802 Train-Acc:0.814\n",
      "I:219 Test-Acc:0.8807 Train-Acc:0.8\n",
      "I:220 Test-Acc:0.8809 Train-Acc:0.798\n",
      "I:221 Test-Acc:0.8805 Train-Acc:0.82\n",
      "I:222 Test-Acc:0.8795 Train-Acc:0.794\n",
      "I:223 Test-Acc:0.8807 Train-Acc:0.806\n",
      "I:224 Test-Acc:0.8806 Train-Acc:0.808\n",
      "I:225 Test-Acc:0.8787 Train-Acc:0.802\n",
      "I:226 Test-Acc:0.8796 Train-Acc:0.81\n",
      "I:227 Test-Acc:0.8766 Train-Acc:0.805\n",
      "I:228 Test-Acc:0.8781 Train-Acc:0.792\n",
      "I:229 Test-Acc:0.8787 Train-Acc:0.809\n",
      "I:230 Test-Acc:0.8762 Train-Acc:0.802\n",
      "I:231 Test-Acc:0.8775 Train-Acc:0.811\n",
      "I:232 Test-Acc:0.8804 Train-Acc:0.814\n",
      "I:233 Test-Acc:0.8794 Train-Acc:0.804\n",
      "I:234 Test-Acc:0.8788 Train-Acc:0.801\n",
      "I:235 Test-Acc:0.8777 Train-Acc:0.795\n",
      "I:236 Test-Acc:0.8785 Train-Acc:0.808\n",
      "I:237 Test-Acc:0.8788 Train-Acc:0.803\n",
      "I:238 Test-Acc:0.8773 Train-Acc:0.813\n",
      "I:239 Test-Acc:0.8786 Train-Acc:0.808\n",
      "I:240 Test-Acc:0.8787 Train-Acc:0.803\n",
      "I:241 Test-Acc:0.8789 Train-Acc:0.812\n",
      "I:242 Test-Acc:0.8792 Train-Acc:0.804\n",
      "I:243 Test-Acc:0.8779 Train-Acc:0.815\n",
      "I:244 Test-Acc:0.8796 Train-Acc:0.811\n",
      "I:245 Test-Acc:0.8798 Train-Acc:0.806\n",
      "I:246 Test-Acc:0.88 Train-Acc:0.803\n",
      "I:247 Test-Acc:0.8776 Train-Acc:0.795\n",
      "I:248 Test-Acc:0.8798 Train-Acc:0.803\n",
      "I:249 Test-Acc:0.8799 Train-Acc:0.805\n",
      "I:250 Test-Acc:0.8789 Train-Acc:0.807\n",
      "I:251 Test-Acc:0.8784 Train-Acc:0.804\n",
      "I:252 Test-Acc:0.8792 Train-Acc:0.806\n",
      "I:253 Test-Acc:0.8777 Train-Acc:0.796\n",
      "I:254 Test-Acc:0.8785 Train-Acc:0.821\n",
      "I:255 Test-Acc:0.8794 Train-Acc:0.81\n",
      "I:256 Test-Acc:0.8783 Train-Acc:0.816\n",
      "I:257 Test-Acc:0.8777 Train-Acc:0.812\n",
      "I:258 Test-Acc:0.8791 Train-Acc:0.812\n",
      "I:259 Test-Acc:0.878 Train-Acc:0.813\n",
      "I:260 Test-Acc:0.8784 Train-Acc:0.82\n",
      "I:261 Test-Acc:0.8792 Train-Acc:0.821\n",
      "I:262 Test-Acc:0.8781 Train-Acc:0.823\n",
      "I:263 Test-Acc:0.8788 Train-Acc:0.816\n",
      "I:264 Test-Acc:0.8793 Train-Acc:0.82\n",
      "I:265 Test-Acc:0.8781 Train-Acc:0.829\n",
      "I:266 Test-Acc:0.8795 Train-Acc:0.809\n",
      "I:267 Test-Acc:0.875 Train-Acc:0.806\n",
      "I:268 Test-Acc:0.8795 Train-Acc:0.813\n",
      "I:269 Test-Acc:0.88 Train-Acc:0.816\n",
      "I:270 Test-Acc:0.8796 Train-Acc:0.819\n",
      "I:271 Test-Acc:0.8802 Train-Acc:0.809\n",
      "I:272 Test-Acc:0.8804 Train-Acc:0.811\n",
      "I:273 Test-Acc:0.8779 Train-Acc:0.808\n",
      "I:274 Test-Acc:0.8816 Train-Acc:0.82\n",
      "I:275 Test-Acc:0.8792 Train-Acc:0.822\n",
      "I:276 Test-Acc:0.8791 Train-Acc:0.817\n",
      "I:277 Test-Acc:0.8769 Train-Acc:0.814\n",
      "I:278 Test-Acc:0.8785 Train-Acc:0.807\n",
      "I:279 Test-Acc:0.8778 Train-Acc:0.817\n",
      "I:280 Test-Acc:0.8794 Train-Acc:0.82\n",
      "I:281 Test-Acc:0.8804 Train-Acc:0.824\n",
      "I:282 Test-Acc:0.8779 Train-Acc:0.812\n",
      "I:283 Test-Acc:0.8784 Train-Acc:0.816\n",
      "I:284 Test-Acc:0.877 Train-Acc:0.817\n",
      "I:285 Test-Acc:0.8767 Train-Acc:0.826\n",
      "I:286 Test-Acc:0.8774 Train-Acc:0.816\n",
      "I:287 Test-Acc:0.8774 Train-Acc:0.804\n",
      "I:288 Test-Acc:0.8774 Train-Acc:0.814\n",
      "I:289 Test-Acc:0.8776 Train-Acc:0.806\n",
      "I:290 Test-Acc:0.8787 Train-Acc:0.822\n",
      "I:291 Test-Acc:0.8781 Train-Acc:0.817\n",
      "I:292 Test-Acc:0.878 Train-Acc:0.823\n",
      "I:293 Test-Acc:0.877 Train-Acc:0.825\n",
      "I:294 Test-Acc:0.8759 Train-Acc:0.826\n",
      "I:295 Test-Acc:0.8769 Train-Acc:0.824\n",
      "I:296 Test-Acc:0.8781 Train-Acc:0.821\n",
      "I:297 Test-Acc:0.8794 Train-Acc:0.81\n",
      "I:298 Test-Acc:0.8791 Train-Acc:0.81\n",
      "I:299 Test-Acc:0.8772 Train-Acc:0.825"
     ]
    }
   ],
   "source": [
    "#4 Page 221-223 - We are investigating convolutional neural networks\n",
    "\n",
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end = ((i*batch_size), ((i+1)*batch_size))\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
    "        layer_0.shape\n",
    "        \n",
    "        sects = list()\n",
    "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_rows):\n",
    "                sect = get_image_section(layer_0,\n",
    "                                        row_start,\n",
    "                                        row_start+kernel_rows,\n",
    "                                        col_start,\n",
    "                                        col_start+kernel_cols)\n",
    "                sects.append(sect)\n",
    "        \n",
    "        expanded_input = np.concatenate(sects, axis=1)\n",
    "        es = expanded_input.shape\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1], -1)\n",
    "        \n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            labelset = labels[batch_start+k:batch_start+k+1]\n",
    "            _inc = int(np.argmax(layer_2[k:k+1]) == \n",
    "                      np.argmax(labelset))\n",
    "            correct_cnt += _inc\n",
    "            \n",
    "        layer_2_delta = (labels[batch_start:batch_end] - layer_2) \\\n",
    "                        / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * \\\n",
    "                        tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "        k_update = flattened_input.T.dot(l1d_reshape)\n",
    "        kernels -= alpha * k_update\n",
    "        \n",
    "    \n",
    "    test_correct_cnt = 0\n",
    "    \n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i:i+1]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
    "        layer_0.shape\n",
    "        \n",
    "        sects = list()\n",
    "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_rows):\n",
    "                sect = get_image_section(layer_0,\n",
    "                                        row_start,\n",
    "                                        row_start+kernel_rows,\n",
    "                                        col_start,\n",
    "                                        col_start+kernel_cols)\n",
    "                sects.append(sect)\n",
    "        \n",
    "        expanded_input = np.concatenate(sects, axis=1)\n",
    "        es = expanded_input.shape\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1], -1)\n",
    "        \n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        test_correct_cnt += int(np.argmax(layer_2) == \n",
    "                              np.argmax(test_labels[i:i+1]))\n",
    "    \n",
    "    if (j % 1 == 0):\n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "        \"I:\" + str(j) + \\\n",
    "        \" Test-Acc:\" + str(test_correct_cnt/float(len(test_images)))+\\\n",
    "        \" Train-Acc:\" + str(correct_cnt/float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preprocessing work not in the book\n",
    "filenames = ['reviews #1.txt', 'reviews #2.txt']\n",
    "with open('reviews.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 214 - preprocessing\n",
    "\n",
    "import sys\n",
    "\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:set(x.split(' ')), raw_reviews))\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if (len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            ''\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "    \n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:95.95% Training Accuracy:0.7971654856190079%%\n",
      "Iter:1 Progress:95.95% Training Accuracy:0.8195211402613098%\n",
      "Test Accuracy:0.843\n"
     ]
    }
   ],
   "source": [
    "# Page 216\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1\n",
    "\n",
    "correct,total = (0,0)\n",
    "for iter in range(iterations):\n",
    "    for i in range(len(input_dataset)-1000):\n",
    "        \n",
    "        x, y = (input_dataset[i], target_dataset[i])\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "        \n",
    "        layer_2_delta = layer_2 - y\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "        \n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "        \n",
    "        if(np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if(i % 10 == 9):\n",
    "            progress = str(i/float(len(input_dataset)))\n",
    "            sys.stdout.write('\\rIter:'+str(iter)\\\n",
    "                            +' Progress:'+progress[2:4]\\\n",
    "                            +'.'+progress[4:6]\\\n",
    "                            +'% Training Accuracy:'\\\n",
    "                            + str(correct/float(total)) + '%')\n",
    "    print()\n",
    "correct, total = (0,0)\n",
    "for i in range(len(input_dataset)-1000,len(input_dataset)):\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "    \n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substitution of missing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 243 - preprocessing step\n",
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:(x.split(\" \")),raw_reviews))\n",
    "wordcnt = Counter()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))\n",
    "                                    \n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "                                    \n",
    "  \n",
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.9999799991999689 [('terrible', -0.0), ('horrible', -2.52559168953348), ('fantastic', -3.93465090925655), ('bad', -4.051182051722724), ('brilliant', -4.154160608588459), ('pathetic', -4.211284441468548), ('dreadful', -4.283776633630338), ('poor', -4.380962727928457), ('magnificent', -4.382881495844147), ('great', -4.401655404772276)]5)]58)]9)]]5)])])]]]]])])][('terrible', -0.0), ('horrible', -2.6096546786637247), ('bad', -3.767183518137911), ('fantastic', -3.875075043677378), ('brilliant', -3.991796014301944), ('pathetic', -4.10884763219456), ('dreadful', -4.243173950738017), ('stupid', -4.338180291179504), ('poor', -4.344969143714373), ('great', -4.373188116606074)]\n"
     ]
    }
   ],
   "source": [
    "# Page 243-244\n",
    "\n",
    "random.shuffle(input_dataset)\n",
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size, window, negative = (50, 2, 5)\n",
    "\n",
    "weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2\n",
    "weights_1_2 = np.random.rand(len(vocab),hidden_size) * 0.2\n",
    "\n",
    "layer_2_target = np.zeros(negative+1)\n",
    "layer_2_target[0] = 1\n",
    "\n",
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target]\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = - math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "for rev_i, review in enumerate(input_dataset * iterations):\n",
    "    for target_i in range(len(review)):\n",
    "\n",
    "        target_samples = [review[target_i]]+list(concatenated\\\n",
    "            [(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
    "\n",
    "        left_context = review[max(0,target_i-window):target_i]\n",
    "        right_context = review[target_i+1:min(len(review),target_i+window)]\n",
    "\n",
    "        layer_1 = np.mean(weights_0_1[left_context+right_context],axis=0)\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "\n",
    "        weights_0_1[left_context+right_context] -= layer_1_delta * alpha\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1)*alpha\n",
    "\n",
    "    if(rev_i % 250 == 0):\n",
    "        sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\n",
    "            *iterations)) + \" \" + str(similar('terrible')))\n",
    "    sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\n",
    "            *iterations)))\n",
    "        \n",
    "print(similar('terrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks that write like Shakespeare: recurrent layers for variable-length data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i don  t know why i like this movie so w',\n",
       " 'this movie is so bad  i knew how it ends',\n",
       " 'probably new zealands worst movie ever m']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 256\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "normed_weights = weights_0_1 * norms\n",
    "\n",
    "def make_sent_vect(words):\n",
    "    indices = list(map(lambda x:word2index[x], \\\n",
    "                filter(lambda x: x in word2index, words)))\n",
    "    return np.mean(normed_weights[indices], axis=0)\n",
    "\n",
    "reviews2vectors = list()\n",
    "for review in tokens:\n",
    "    reviews2vectors.append(make_sent_vect(review))\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Counter()\n",
    "    for i, val in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    most_similar = list()\n",
    "    for idx, score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx][0:40])\n",
    "    return most_similar\n",
    "\n",
    "most_similar_reviews(['boring','awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[1. 2. 3.]\n",
      "[0.1 0.2 0.3]\n",
      "[-1.  -0.5  0. ]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Page 262-263 identity matrix Practice\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([0.1, 0.2, 0.3])\n",
    "c = np.array([-1, -0.5, 0])\n",
    "d = np.array([0, 0, 0])\n",
    "\n",
    "identity = np.eye(3)\n",
    "print(identity)\n",
    "\n",
    "print(a.dot(identity))\n",
    "print(b.dot(identity))\n",
    "print(c.dot(identity))\n",
    "print(d.dot(identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 17]\n",
      "[13. 15. 17.]\n"
     ]
    }
   ],
   "source": [
    "# Page 263\n",
    "\n",
    "this = np.array([2, 4, 6])\n",
    "movie = np.array([10, 10, 10])\n",
    "rocks = np.array([1, 1, 1])\n",
    "\n",
    "print(this + movie + rocks)\n",
    "print((this.dot(identity) + movie).dot(identity) + rocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "# Page 267\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_)\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0., 0., 0.]])\n",
    "word_vects['bears'] = np.array([[0., 0., 0.]])\n",
    "word_vects['braves'] = np.array([[0., 0., 0.]])\n",
    "word_vects['red'] = np.array([[0., 0., 0.]])\n",
    "word_vects['sox'] = np.array([[0., 0., 0.]])\n",
    "word_vects['lose'] = np.array([[0., 0., 0.]])\n",
    "word_vects['defeat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['beat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['tie'] = np.array([[0., 0., 0.]])\n",
    "\n",
    "sent2output = np.random.rand(3, len(word_vects))\n",
    "\n",
    "identity = np.eye(3)\n",
    "\n",
    "# Page 267 - creating layers for forward prop.\n",
    "\n",
    "layer_0 = word_vects['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "\n",
    "pred = softmax(layer_2.dot(sent2output))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 268 - back prop.\n",
    "\n",
    "y = np.array([1,0,0,0,0,0,0,0,0])\n",
    "\n",
    "pred_delta = pred - y\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "defeat_delta = layer_2_delta * 1\n",
    "\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "sox_delta = layer_1_delta * 1\n",
    "\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "alpha = 0.01\n",
    "\n",
    "word_vects['red'] -= layer_0_delta * alpha\n",
    "word_vects['sox'] -= sox_delta * alpha\n",
    "word_vects['defeat'] -= defeat_delta * alpha\n",
    "\n",
    "identity -= np.outer(layer_0, layer_1_delta) * alpha\n",
    "identity -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "sent2output -= np.outer(layer_2, pred_delta) * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
     ]
    }
   ],
   "source": [
    "# Page 269 - Dataset Preparation\n",
    "\n",
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "print(tokens[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 270-271 - Preparation\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "        \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "vocab = list(vocab)\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "embed_size = 10\n",
    "\n",
    "embed = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1\n",
    "recurrent = np.eye(embed_size)\n",
    "start = np.zeros(embed_size)\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "one_hot = np.eye(len(vocab))\n",
    "\n",
    "def predict(sent):\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "    loss = 0\n",
    "    preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "        layer = {}\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) +\\\n",
    "                                        embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "\n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:82.23016500268825\n",
      "Perplexity:82.07006016564391\n",
      "Perplexity:81.87303409675972\n",
      "Perplexity:81.56239217794138\n",
      "Perplexity:80.9955360735282\n",
      "Perplexity:79.85498833959106\n",
      "Perplexity:77.26929888448917\n",
      "Perplexity:69.75689813233382\n",
      "Perplexity:40.77681694649174\n",
      "Perplexity:23.91861080014992\n",
      "Perplexity:19.783707801402613\n",
      "Perplexity:18.485391226622106\n",
      "Perplexity:17.193757735948697\n",
      "Perplexity:15.395801012734276\n",
      "Perplexity:12.824689868163784\n",
      "Perplexity:9.75639585113182\n",
      "Perplexity:7.667802759720692\n",
      "Perplexity:6.526055747627111\n",
      "Perplexity:5.750989431161865\n",
      "Perplexity:5.231317103283213\n",
      "Perplexity:4.92664720187029\n",
      "Perplexity:4.728488388337021\n",
      "Perplexity:4.6031394242954935\n",
      "Perplexity:4.5302259748415885\n",
      "Perplexity:4.479805191231206\n",
      "Perplexity:4.426527404276067\n",
      "Perplexity:4.360690860013493\n",
      "Perplexity:4.284494163731166\n",
      "Perplexity:4.204018971019123\n",
      "Perplexity:4.117761121882188\n"
     ]
    }
   ],
   "source": [
    "# Page 273 - Forward and Back propogations\n",
    "\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter % len(tokens)][1:])\n",
    "    \n",
    "    layers, loss = predict(sent)\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "        \n",
    "        if (layer_idx > 0):\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "            if (layer_idx == len(layers) - 1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                    layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else:\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "            \n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "        \n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], \\\n",
    "                           layer['output_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sent))\n",
    "        \n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], \\\n",
    "                             layer['hidden_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "    if (iter % 1000 == 0):\n",
    "        print('Perplexity:' + str(np.exp(loss/len(sent))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Optimization - Frameworks Chapter 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 280-282 - Creating Tensor Class\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    def __init__(self, data, creators=None, creation_op=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        self.grad = grad\n",
    "        \n",
    "        if (self.creation_op == 'add'):\n",
    "            self.creators[0].backward(grad)\n",
    "            self.creators[1].backward(grad)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data,\n",
    "                      creators=[self,other], \n",
    "                      creation_op='add')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "x = Tensor([1,2,3,4,5])\n",
    "y = Tensor([2,2,2,2,2])\n",
    "\n",
    "z = x + y\n",
    "z.backward(Tensor(np.array([1,1,1,1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
      "add\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.creators)\n",
    "print(z.creation_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[1 2 3]\n",
      " [4 5 6]]\n",
      "Summing over first dimension and then second:\n",
      "[5 7 9] \t [ 6 15] \n",
      "\n",
      "Expanding over first dimension:\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]] \n",
      "\n",
      "Expanding over last dimension:\n",
      "[[[1 1 1 1 1 1]\n",
      "  [2 2 2 2 2 2]\n",
      "  [3 3 3 3 3 3]]\n",
      "\n",
      " [[4 4 4 4 4 4]\n",
      "  [5 5 5 5 5 5]\n",
      "  [6 6 6 6 6 6]]]\n"
     ]
    }
   ],
   "source": [
    "# Couple of experiments to understand the code better\n",
    "\n",
    "x = Tensor(np.array([[1,2,3],[4,5,6]]))\n",
    "print('X = ', x)\n",
    "print('Summing over first dimension and then second:')\n",
    "print(x.sum(0), '\\t', x.sum(1), '\\n')\n",
    "\n",
    "print('Expanding over first dimension:')\n",
    "print(x.expand(dim=0, copies=4), '\\n')\n",
    "\n",
    "print('Expanding over last dimension:')\n",
    "print(x.expand(dim=2, copies=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "[0.58128304]\n",
      "\n",
      "#1\n",
      "[0.48988149]\n",
      "\n",
      "#2\n",
      "[0.41375111]\n",
      "\n",
      "#3\n",
      "[0.34489412]\n",
      "\n",
      "#4\n",
      "[0.28210124]\n",
      "\n",
      "#5\n",
      "[0.2254484]\n",
      "\n",
      "#6\n",
      "[0.17538853]\n",
      "\n",
      "#7\n",
      "[0.1324231]\n",
      "\n",
      "#8\n",
      "[0.09682769]\n",
      "\n",
      "#9\n",
      "[0.06849361]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Page 296 - start of using autograd\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    loss = ((pred - target) * (pred - target)).sum(0)\n",
    "    print('#{}'.format(i))\n",
    "    print(loss.data)\n",
    "    # print('-')\n",
    "    # print(np.ones_like(loss.data))\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    \n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.grad.data *= 0\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 297 - SGD (stochastic gradient descent) Optimizer\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "            \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if (zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58128304]\n",
      "[0.48988149]\n",
      "[0.41375111]\n",
      "[0.34489412]\n",
      "[0.28210124]\n",
      "[0.2254484]\n",
      "[0.17538853]\n",
      "[0.1324231]\n",
      "[0.09682769]\n",
      "[0.06849361]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "optim = SGD(parameters=w, alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33428272]\n",
      "[0.06743796]\n",
      "[0.0521849]\n",
      "[0.04079507]\n",
      "[0.03184365]\n",
      "[0.02479336]\n",
      "[0.01925443]\n",
      "[0.01491699]\n",
      "[0.01153118]\n",
      "[0.00889602]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 303 - Activation Function Objects\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06372865]\n",
      "[0.51186358]\n",
      "[0.40813918]\n",
      "[0.28195888]\n",
      "[0.23630241]\n",
      "[0.17087349]\n",
      "[0.1422059]\n",
      "[0.09038856]\n",
      "[0.07491572]\n",
      "[0.05888053]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=3)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.eye(5), autograd=True)\n",
    "x.index_select(Tensor([[1,2,3], \n",
    "                       [2,3,4]])).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00075326]\n",
      "[0.76615654]\n",
      "[0.49045342]\n",
      "[0.32114743]\n",
      "[0.22623086]\n",
      "[0.16717823]\n",
      "[0.12764907]\n",
      "[0.10004686]\n",
      "[0.08023261]\n",
      "[0.06569254]\n"
     ]
    }
   ],
   "source": [
    "# Page 304-307\n",
    "\n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor((weight), autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "    \n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Embedding(5,3), Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.5)\n",
    "\n",
    "for i in range(50):\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if (i % 5 == 0):\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3885032434928422\n",
      "0.6337245077659619\n",
      "0.4970254774441115\n",
      "0.24235608130790934\n",
      "0.12739972300219915\n",
      "0.08666598450574696\n",
      "0.06633924052699045\n",
      "0.05364788527801873\n",
      "0.04497450105863754\n",
      "0.03867751471128533\n"
     ]
    }
   ],
   "source": [
    "# Page 308\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0,1,0,1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Embedding(3,3), Tanh(), Linear(3,4)])\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tensor\n",
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    def __init__(self, data, \n",
    "                 autograd=False,\n",
    "                 creators=None, \n",
    "                 creation_op=None, \n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        \n",
    "        if (creators is not None):\n",
    "            for c in creators:\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if (self.autograd):\n",
    "            # Так как backward первый раз вызывается именно у ошибки нет смысла передавать туда что-то в качестве grad\n",
    "            if (grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            \n",
    "            if (grad_origin is not None):\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    raise Exception('cannot backprop more than once')\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "        \n",
    "            if (self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "\n",
    "            if (self.creators is not None and \n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "                if (self.creation_op == 'add'):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                \n",
    "                if (self.creation_op == 'neg'):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                \n",
    "                if (self.creation_op == 'sub'):\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if (self.creation_op == 'mul'):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                \n",
    "                if (self.creation_op == 'mm'):\n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                    \n",
    "                if (self.creation_op == 'transpose'):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                    \n",
    "                if ('sum' in self.creation_op):\n",
    "                    dim = int(self.creation_op.split('_')[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "                    \n",
    "                if ('expand' in self.creation_op):\n",
    "                    try:\n",
    "                        dim = int(self.creation_op.split('_')[1])\n",
    "                        self.creators[0].backward(self.grad.sum(dim))\n",
    "                    except:\n",
    "                        print(self.creation_op)\n",
    "                        raise ValueError\n",
    "                        \n",
    "                if (self.creation_op == 'index_select'):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if (self.creation_op == 'cross_entropy'):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                        \n",
    "                if (self.creation_op == 'sigmoid' or self.creation_op == 'tanh'):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other], \n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data * -1, \n",
    "                          autograd=True,\n",
    "                          creators=[self], \n",
    "                          creation_op='neg')\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other], \n",
    "                          creation_op='sub')\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other], \n",
    "                          creation_op='mul')\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dim):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.sum(dim), \n",
    "                          autograd=True,\n",
    "                          creators=[self], \n",
    "                          creation_op='sum_'+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if (self.autograd):\n",
    "            return Tensor(new_data, \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='expand_'+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.transpose(), \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='transpose')\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.dot(x.data), \n",
    "                          autograd=True, \n",
    "                          creators=[self, x], \n",
    "                          creation_op='mm')\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)), \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='sigmoid')\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "    \n",
    "    def tanh(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(np.tanh(self.data), \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='tanh')\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if (self.autograd):\n",
    "            new = Tensor(self.data[indices.data], \n",
    "                         autograd=True, \n",
    "                         creators=[self], \n",
    "                         creation_op='index_select')\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, \n",
    "                                       axis=len(self.data.shape)-1, \n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        \n",
    "        if (self.autograd):\n",
    "            out = Tensor(loss, \n",
    "                         autograd=True, \n",
    "                         creators=[self], \n",
    "                         creation_op='cross_entropy')\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        \n",
    "        return Tensor(loss)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 298-300 - Layers\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand(0, len(input.data))\n",
    "    \n",
    "class Sequential(object):\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "    \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor((weight), autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if (activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif (activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception('Non-linearity not found')\n",
    "            \n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 311 (with added RNN Cell) - Data Preprocessing\n",
    "\n",
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[:1000]:\n",
    "    tokens.append(line.lower().replace('\\n','').split(' ')[1:])\n",
    "    \n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "tokens = new_tokens\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "        \n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "def word2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "indices = list()\n",
    "for line in tokens:\n",
    "    idx = list()\n",
    "    for w in line:\n",
    "        idx.append(word2index[w])\n",
    "    indices.append(idx)\n",
    "    \n",
    "data = np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.462270832181715 % Correct:  0.0\n",
      "Loss:  0.17826032937131292 % Correct:  0.24\n",
      "Loss:  0.15595782143260267 % Correct:  0.3\n",
      "Loss:  0.14053575187514772 % Correct:  0.37\n",
      "Loss:  0.13753627193304369 % Correct:  0.35\n"
     ]
    }
   ],
   "source": [
    "# Page 312 - Building NN for prev data\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab), dim=16)\n",
    "model = RNNCell(n_inputs=16, n_hidden=16, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "params = model.get_parameters() + embed.get_parameters()\n",
    "optim = SGD(parameters=params, alpha=0.05)\n",
    "\n",
    "for iter in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "    \n",
    "    for t in range(5):\n",
    "        input = Tensor(data[:batch_size,t], autograd=True)\n",
    "        rnn_input = embed.forward(input=input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        \n",
    "    target = Tensor(data[:batch_size,t+1], autograd=True)\n",
    "    loss = criterion.forward(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.data\n",
    "    \n",
    "    if (iter % 200 == 0):\n",
    "        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()\n",
    "        print_loss = total_loss / (len(data)/batch_size)\n",
    "        print('Loss: ', print_loss, '% Correct: ', p_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  - mary moved to the \n",
      "Pred:  office.\n"
     ]
    }
   ],
   "source": [
    "# Page 313\n",
    "\n",
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size=batch_size)\n",
    "for t in range(5):\n",
    "    input = Tensor(data[:batch_size,t], autograd=True)\n",
    "    rnn_input = embed.forward(input=input)\n",
    "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "    \n",
    "target = Tensor(data[:batch_size,t+1], autograd=True)\n",
    "loss = criterion.forward(output, target)\n",
    "\n",
    "ctx = ''\n",
    "for idx in data[:batch_size][0][0:-1]:\n",
    "    ctx += vocab[idx] + ' '\n",
    "print('Context: ', ctx)\n",
    "print('Pred: ', vocab[output.data.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 14 - Page 314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "f = open('shakespear.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=100):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        \n",
    "        for batch_i in range(len(input_batches)):\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, \n",
    "                                               hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if (t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss += batch_loss\n",
    "            for loss in losses:\n",
    "                ''\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = '\\r Iter: ' + str(iter)\n",
    "            log += ' - Batch ' + str(batch_i+1) + '/' + str(len(input_batches))\n",
    "            log += ' - Loss: ' + str(np.exp(total_loss / (batch_i + 1)))\n",
    "            if (batch_i == 0):\n",
    "                log += ' - ' + generate_sample(70, '\\n').replace('\\n', ' ')\n",
    "            if (batch_i % 10 == 0 or batch_i - 1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = ''\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 0 - Batch 191/195 - Loss: 53.052569018934765 ikikkakaikiiaskkkkikioaaikiikaikkkikkkikkkikiskkkkakkaikkakkkkikkkkkak\n",
      " Iter: 1 - Batch 191/195 - Loss: 18.398294772746728 etht the thek thek thithk the the the the the the the the the the thek\n",
      " Iter: 2 - Batch 191/195 - Loss: 14.406136655827039 otht the the the the the the the the the the the the the the the the t\n",
      " Iter: 3 - Batch 191/195 - Loss: 12.508222770817097ot the the the the the the thk the the the the the thk the the the the\n",
      " Iter: 4 - Batch 191/195 - Loss: 11.420288817626586ot the the the thkke the the the the the the the the the the the the t\n",
      " Iter: 5 - Batch 191/195 - Loss: 10.666042842379589ot the the the the the the thitk the the the the the the the the the t\n",
      " Iter: 6 - Batch 191/195 - Loss: 10.088801466330803 At the the thith the the the the the the the the the the the the the t\n",
      " Iter: 7 - Batch 191/195 - Loss: 9.613042168735708 At the the the the the the the the the the the the the the the the the\n",
      " Iter: 8 - Batch 191/195 - Loss: 9.204613289687519  the the the the the the the the the the the the the the the the the t\n",
      " Iter: 9 - Batch 191/195 - Loss: 8.846888654230754  the the the the the the the the the the the the the the the the the t\n",
      " Iter: 10 - Batch 191/195 - Loss: 8.529241737298543ket the the the the the the the the the the the the the the the the t\n",
      " Iter: 11 - Batch 191/195 - Loss: 8.239854198025226 Ake the the the the the the the the the the the the the the the the th\n",
      " Iter: 12 - Batch 191/195 - Loss: 7.973221732240711 's the the the the the the the the the the the the the the the the the\n",
      " Iter: 13 - Batch 191/195 - Loss: 7.7268979401639925 's the the the the the the the the the the the the the the the the the\n",
      " Iter: 14 - Batch 191/195 - Loss: 7.4970764592053345's the the the the the the the the the the the the the the the the the\n",
      " Iter: 15 - Batch 191/195 - Loss: 7.2808213875043775's the the sto the the the the the the the sto the the the the the the\n",
      " Iter: 16 - Batch 191/195 - Loss: 7.075683336900541 's the the the the the the the the the the the the sto the the the the\n",
      " Iter: 17 - Batch 191/195 - Loss: 6.878685627664891 's the the kith the the the the the the the the the the the the the th\n",
      " Iter: 18 - Batch 191/195 - Loss: 6.6883825496857535'kike the the the the the the the the the sto the the kith the the the\n",
      " Iter: 19 - Batch 191/195 - Loss: 6.503366592815761 'll the the the king the the the the the sto the the the the the sto t\n",
      " Iter: 20 - Batch 191/195 - Loss: 6.3221533266122825s the kike the sto the king the stke the king the sto the king the ki\n",
      " Iter: 21 - Batch 191/195 - Loss: 6.1424000715999575'Whake the sto the sto the king the sto the king the sto the sto the s\n",
      " Iter: 22 - Batch 191/195 - Loss: 5.9625039689699025's, the stoke the king the stort the stoke the sto the king the stort \n",
      " Iter: 23 - Batch 191/195 - Loss: 5.7829925066665335's, the stoke the king the stort the stoke the stort the king the stor\n",
      " Iter: 24 - Batch 191/195 - Loss: 5.6051746211873795's, the stort the storth the storth sirterk the kink the stort the kin\n",
      " Iter: 25 - Batch 191/195 - Loss: 5.429243431225521 's, the stoor the storth the storth the kint the kint the stoke and th\n",
      " Iter: 26 - Batch 191/195 - Loss: 5.2557299050582375es, the stoor the storth the storth the stoor the kint the kint the st\n",
      " Iter: 27 - Batch 191/195 - Loss: 5.0850360892370175 es, the kint the kint the kint the stoke and the the storth the kint t\n",
      " Iter: 28 - Batch 191/195 - Loss: 4.9169225242885126es, the storth the the storth the storth sirters, the storth the stort\n",
      " Iter: 29 - Batch 191/195 - Loss: 4.7516028964157355es, the the sked the kink kind the the stke the kint the the sked the \n",
      " Iter: 30 - Batch 191/195 - Loss: 4.5894789584057175ALE: That the the sked the wke the the sked the wke the warrom the ki\n",
      " Iter: 31 - Batch 191/195 - Loss: 4.4306977411079394MALE: That the the sked the the sked the wkeking the sked the kink wot\n",
      " Iter: 32 - Batch 191/195 - Loss: 4.2753446258146255MALE: That the the sked the the shat the kink kind the the shat the th\n",
      " Iter: 33 - Batch 191/195 - Loss: 4.1232151815097165MALE: That the the shat the kink worse kike and kist the storth the wk\n",
      " Iter: 34 - Batch 191/195 - Loss: 3.9737610462265724MALE: That the the kike and the the sked the the kikk the kind the the\n",
      " Iter: 35 - Batch 191/195 - Loss: 3.8275248563408413MALE: That the the sked the the shat the the sked the the sked the the\n",
      " Iter: 36 - Batch 191/195 - Loss: 3.6840099727776865MALE: That the thkerking kind the this stall, the thkers, the kith the\n",
      " Iter: 37 - Batch 191/195 - Loss: 3.5435440371168427MALE: That the thkerking kind the thkers, this stall, the this stall, \n",
      " Iter: 38 - Batch 191/195 - Loss: 3.4057804662222906MALE: That the this stall, the thkers, the kitt a pent the kitk the st\n",
      " Iter: 39 - Batch 191/195 - Loss: 3.2703416793009163 MALE: There the this stall, the this stall, the this stall, the this s\n",
      " Iter: 40 - Batch 191/195 - Loss: 3.1390213960465427MALE: There kind skech the this stall, the this stall, the this stall,\n",
      " Iter: 41 - Batch 191/195 - Loss: 3.0122978736674373MALE: There wot the tkes, this to the tkes, this kind kith the this st\n",
      " Iter: 42 - Batch 191/195 - Loss: 2.8898021322963934 MALE: There won the kith the tkes to the tampet the this stall, the ta\n",
      " Iter: 43 - Batch 191/195 - Loss: 2.7739771308396832 MAke and skech the this stall, the tampet the kith a king the tkesk th\n",
      " Iter: 44 - Batch 191/195 - Loss: 2.6618476855025746 MAnd some to the tampet the tampet the tkesk this stall, the tamn the \n",
      " Iter: 45 - Batch 191/195 - Loss: 2.5549726249948206And some to the tampet the kith a wastaind speak tkemkers, this to th\n",
      " Iter: 46 - Batch 191/195 - Loss: 2.4526041719339315 MAnd some kind same to the tampet the mand the tampet the this stall, \n",
      " Iter: 47 - Batch 191/195 - Loss: 2.3565246139436695 MAnd some himsers, this stall, the tamn to the tamn to the tamk, which\n",
      " Iter: 48 - Batch 191/195 - Loss: 2.2677358715231444 MAnd the tamn to the tamn to the tamn to-mand the mand the tamn to the\n",
      " Iter: 49 - Batch 191/195 - Loss: 2.1831316861868215MAnd the tamn to the tamn to the tamk, which the tamn toke and the tam\n",
      " Iter: 50 - Batch 191/195 - Loss: 2.1015190078224717 MAnd the tamn toke and skech the tamn toke and some himsersek to the t\n",
      " Iter: 51 - Batch 191/195 - Loss: 2.0226416583198152 MAnd the tamn toorts to the most all the mand the kith a good the kith\n",
      " Iter: 52 - Batch 191/195 - Loss: 1.9452951509697418MAnd the tamn toorts to the mk to the tamn toorts to the mk to the tam\n",
      " Iter: 53 - Batch 191/195 - Loss: 1.8729306144221753 MAnd kith a good kingson kingson was a kingson was a spake ake of the \n",
      " Iter: 54 - Batch 191/195 - Loss: 1.8068016560327658MAnd say wonlow kingson was a spaid were to the mkingson was a spaid w\n",
      " Iter: 55 - Batch 191/195 - Loss: 1.7438206681621127 MAnd say wonlow kingson was a spaid to the tamn toorts to the kith a f\n",
      " Iter: 56 - Batch 191/195 - Loss: 1.6847172353908917 MAnd say wonlow not and the mkingson was a spaid to the tamines,'d to \n",
      " Iter: 57 - Batch 191/195 - Loss: 1.6292058795130502MAnd say wonlow not and the man! Ben makes, and the mkingson was a spa\n",
      " Iter: 58 - Batch 191/195 - Loss: 1.5807385111882593MAnd say wo kings.  kememers, and thy rent this stall the most this st\n",
      " Iter: 59 - Batch 191/195 - Loss: 1.5349896098314892 MAnd say wo kings.  kememers, and say wo no from thy tark, tormas.  MA\n",
      " Iter: 60 - Batch 191/195 - Loss: 1.4932643724781252 MAnd sarsh the tamines,'d tkem stall, thy rent this stall, thy rent th\n",
      " Iter: 61 - Batch 191/195 - Loss: 1.4591089147757625 MAnd sarsh thy tarkks,'dis takekniagh, and thy rentost rept this stall\n",
      " Iter: 62 - Batch 191/195 - Loss: 1.4251053800645914MAnd same kings.  kememers, and same to the tamn, kemn this stkeson wa\n",
      " Iter: 63 - Batch 191/195 - Loss: 1.3887649752342552 MAnd say wo no fringstainkked to the tamk, which the tamn this stall t\n",
      " Iter: 64 - Batch 191/195 - Loss: 1.3519819702136053 MAnd same to the kith a king this stkessader are, my sirsk to the tami\n",
      " Iter: 65 - Batch 191/195 - Loss: 1.3253173325580005 MAnd same, my sirthis stod thy her this stod thy her this stod thy her\n",
      " Iter: 66 - Batch 191/195 - Loss: 1.2973553126208868 MAnd sat he princely steatkers to the tammst toould kith her fackingme\n",
      " Iter: 67 - Batch 191/195 - Loss: 1.2741869165462834 MAnd sake thy hathk this stod thy hathere kind all the man! Ben king t\n",
      " Iter: 68 - Batch 191/195 - Loss: 1.2536327946819572MAnd sat him sich the worse on The princely steat, Soothknkersek to th\n",
      " Iter: 69 - Batch 191/195 - Loss: 1.2351865168405016 MAnd same to thy rent and same to thy rent and same to thy rent and sa\n",
      " Iter: 70 - Batch 191/195 - Loss: 1.2183742422576713MAnd sake thy hath in this stok thk thy rent an the stiring from the k\n",
      " Iter: 71 - Batch 191/195 - Loss: 1.2050388072065397MAnd same him skect fits aid were to thy rent ake, To my whow he stiri\n",
      " Iter: 72 - Batch 191/195 - Loss: 1.1900103415122283 MAnd same and same and same and same and same and same and same and sa\n",
      " Iter: 73 - Batch 191/195 - Loss: 1.1761004287468613 MAnd same king this stolt my his desich the worse on Antonis, and same\n",
      " Iter: 74 - Batch 191/195 - Loss: 1.1627312202669005 MAnd same and empress of thy rented hk her, Took king ours fookk to th\n",
      " Iter: 75 - Batch 191/195 - Loss: 1.1480955463249668MAnd same and empress of thy hath in thy rented him, mint restruck wer\n",
      " Iter: 76 - Batch 191/195 - Loss: 1.1338878130269439 MAnd same and empress of thy hath in thy rented himselt kine rent an I\n",
      " Iter: 77 - Batch 191/195 - Loss: 1.1203263838687585 MAnd same and empress of thy hath in thy this stolt my hath skeconto t\n",
      " Iter: 78 - Batch 191/195 - Loss: 1.1089761138549186 MAnd same and empress of thy hath in thy this stolt my hath see, minds\n",
      " Iter: 79 - Batch 191/195 - Loss: 1.1019309908767427MAnd same and empress of thy hath in thy tk-s. am the to day, skeconfl\n",
      " Iter: 80 - Batch 191/195 - Loss: 1.0931284534708512 MAnd same and empress usoke my whould notk to the tamn thek trus repte\n",
      " Iter: 81 - Batch 191/195 - Loss: 1.0866428084601278 MAnd same and empress uson will that I will that I will that I will th\n",
      " Iter: 82 - Batch 191/195 - Loss: 1.0801247967316647MAnd same and empress of thy hath see, king os copt hik, Inakes to the\n",
      " Iter: 83 - Batch 191/195 - Loss: 1.0757445647347164 MAnd same and empress uson will that I will that I will that I will th\n",
      " Iter: 84 - Batch 191/195 - Loss: 1.0695592173802768MAnd same and empress usome my who in may, such leggoors well ek with \n",
      " Iter: 85 - Batch 191/195 - Loss: 1.0653527298110312 MAnd same and empress, and kish the worses all the tas a fring from th\n",
      " Iter: 86 - Batch 191/195 - Loss: 1.0618301134685188 MAnd skecont my hath see, give rester mine riph the tamn this tay, suc\n",
      " Iter: 87 - Batch 191/195 - Loss: 1.0581132358237408MAnd skeconto be gaon my did, my ath the chat and empress uson warried\n",
      " Iter: 88 - Batch 191/195 - Loss: 1.0547483763722412MAnd shat comess of thy hath see, out mastad.  CHESTIkms: and fell of \n",
      " Iter: 89 - Batch 191/195 - Loss: 1.0515300294734926 MAnd felly knavon mastad Thruk king os king this stolt my did fook sha\n",
      " Iter: 90 - Batch 191/195 - Loss: 1.0483475041744066MAnd felly knavon mastad Thriek voins: Why, such the wkeconflakess uso\n",
      " Iter: 91 - Batch 191/195 - Loss: 1.0459562193659966MAnd felly that comess of thy hath see, out mastad.  CHESTIO: When I'l\n",
      " Iter: 92 - Batch 191/195 - Loss: 1.0436281244683456 MAnd felly that comess of thy hath see, out makes to the tamn this tas\n",
      " Iter: 93 - Batch 191/195 - Loss: 1.0416239269626321 MAnd felly that comess of thy hath see, out mastad.  CHESTIO: When I'l\n",
      " Iter: 94 - Batch 191/195 - Loss: 1.0396977163350363 MAnd felly that comess of thy hath see, out king this stolt my did foo\n",
      " Iter: 95 - Batch 191/195 - Loss: 1.0379553637629755 MAnd felly that comess of thy hath see, out mastkious see, my knows, t\n",
      " Iter: 96 - Batch 191/195 - Loss: 1.0363394085265458 MAnd felly that comess of thy hath see, out king os is, is tas not you\n",
      " Iter: 97 - Batch 191/195 - Loss: 1.0349280616331078MAnd kitk my sirsh, ours, To go, Why, such the wkky 's my anfecs tooul\n",
      " Iter: 98 - Batch 191/195 - Loss: 1.0335230060069371 MAnd I conflace.  KINIPIk my farch their he and thum for by this, thou\n",
      " Iter: 99 - Batch 191/195 - Loss: 1.0323282635946258MAnd I conflace.  KINI: Though! and felly druk king our are, and felly\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAnd I conflace.\n",
      "\n",
      "KINI:\n",
      "Though! and fellkess of thy hath see's seemnke himsek and fellow, and fellk\n",
      "to your justinguk my shakes to the tamn tkruchiwith hon was a betterk,\n",
      "That cower\n",
      "Befe their he kits keath, and felly drusars desicho to the chat there shall sery sted my flakk with thy allsing of her for this see, out master our king os is, is tas not your ded my face;\n",
      "The day, such their hear:\n",
      "Though! and fellkess of thy hath see, out makes to the tamn thin tith a fring fill sirsel-\n",
      "A kiar the conto beat on the tamn this tas not your just boins:\n",
      "Why, such their hear:\n",
      "Thath that comess usome lays if you this see, out master our now, sirsely lors, and felly drum flakes to kis desichked my flakes,\n",
      "The day, such their hear:\n",
      "Thath that comess usome lays if you this see, out master our now, sirsel-\n",
      "Command?\n",
      "\n",
      "All, sirsel- mine riph the tamn toldist res this stolt were\n",
      "Wear:\n",
      "\n",
      "OOR: Shat I will that I will that comess usome lays if you this see, out master our now, sirsely lors, and felly druk the tamn this seemn tith a day, such thek to and fellow, and fello, betwer king himsets that comess usome landad\n",
      "That cower\n",
      "Befe their he kits are: sing his restruck with the ablace;\n",
      "The day, suco thy hath kiscolt in they sars in thkeeming him, my lor wkecont my knowselses it it king our are, and felly did fkeld.\n",
      "\n",
      "TIMI'I\n",
      "\n",
      "First Lart the tamn to forch the worses ake:\n",
      "Why, such their hear:\n",
      "Thk the monn my flakes offent on the to the tamn thik trus rent I will that I will that I will that comess usome landad\n",
      "That cower\n",
      "Befe their he kits keath, and felly druk the tamn this seemn tith a fring him, my lor wiflace.\n",
      "\n",
      "KINIPIUShe'er, the tamn thin tien with the cont my hath is tas not your ded my face;\n",
      "The day, suco thy hath kiscolt in they sars in they sars if you this see, out master our now, sirsely lors, and felly druk the tamn thin tith a fring fill sirsel-\n",
      "A good and felly drusars desichked my flakes,\n",
      "The day, suck with the and fellkess of thy hath seek;\n",
      "That came and felly drusars kear as\n"
     ]
    }
   ],
   "source": [
    "print(generate_sampleate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activations\n",
      "[0.93940638 0.96852968]\n",
      "[0.9919462  0.99121735]\n",
      "[0.99301385 0.99302901]\n",
      "[0.9930713  0.99307098]\n",
      "[0.99307285 0.99307285]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "\n",
      "Sigmoid Gradients\n",
      "[0.03439552 0.03439552]\n",
      "[0.00118305 0.00118305]\n",
      "[4.06916726e-05 4.06916726e-05]\n",
      "[1.39961115e-06 1.39961115e-06]\n",
      "[4.81403643e-08 4.81403637e-08]\n",
      "[1.65582672e-09 1.65582765e-09]\n",
      "[5.69682675e-11 5.69667160e-11]\n",
      "[1.97259346e-12 1.97517920e-12]\n",
      "[8.45387597e-14 8.02306381e-14]\n",
      "[1.45938177e-14 2.16938983e-14]\n",
      "Activations\n",
      "[4.8135251  4.72615519]\n",
      "[23.71814585 23.98025559]\n",
      "[119.63916823 118.852839  ]\n",
      "[595.05052421 597.40951192]\n",
      "[2984.68857188 2977.61160877]\n",
      "[14895.13500696 14916.36589628]\n",
      "[74560.59859209 74496.90592414]\n",
      "[372548.22228863 372739.30029248]\n",
      "[1863505.42345854 1862932.18944699]\n",
      "[9315234.18124649 9316953.88328115]\n",
      "\n",
      "Gradients\n",
      "[5. 5.]\n",
      "[25. 25.]\n",
      "[125. 125.]\n",
      "[625. 625.]\n",
      "[3125. 3125.]\n",
      "[15625. 15625.]\n",
      "[78125. 78125.]\n",
      "[390625. 390625.]\n",
      "[1953125. 1953125.]\n",
      "[9765625. 9765625.]\n"
     ]
    }
   ],
   "source": [
    "(sigmoid, relu) = (lambda x: 1/(1+np.exp(-x)), lambda x: (x>0).astype(float)*x)\n",
    "weights = np.array([[1,4],[4,1]])\n",
    "activation = sigmoid(np.array([1,0.01]))\n",
    "\n",
    "print('Sigmoid Activations')\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = sigmoid(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "print('\\nSigmoid Gradients')\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = (activation * (1 - activation) * gradient)\n",
    "    gradient = gradient.dot(weights.transpose())\n",
    "    print(gradient)\n",
    "\n",
    "print('Activations')\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = relu(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "print('\\nGradients')\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tensor\n",
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    def __init__(self, data, \n",
    "                 autograd=False,\n",
    "                 creators=None, \n",
    "                 creation_op=None, \n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        \n",
    "        if (creators is not None):\n",
    "            for c in creators:\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if (self.autograd):\n",
    "            # Так как backward первый раз вызывается именно у ошибки нет смысла передавать туда что-то в качестве grad\n",
    "            if (grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            \n",
    "            if (grad_origin is not None):\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    raise Exception('cannot backprop more than once')\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "        \n",
    "            if (self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "\n",
    "            if (self.creators is not None and \n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "                if (self.creation_op == 'add'):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                \n",
    "                if (self.creation_op == 'neg'):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                \n",
    "                if (self.creation_op == 'sub'):\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if (self.creation_op == 'mul'):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                \n",
    "                if (self.creation_op == 'mm'):\n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                    \n",
    "                if (self.creation_op == 'transpose'):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                    \n",
    "                if ('sum' in self.creation_op):\n",
    "                    dim = int(self.creation_op.split('_')[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "                    \n",
    "                if ('expand' in self.creation_op):\n",
    "                    try:\n",
    "                        dim = int(self.creation_op.split('_')[1])\n",
    "                        self.creators[0].backward(self.grad.sum(dim))\n",
    "                    except:\n",
    "                        print(self.creation_op)\n",
    "                        raise ValueError\n",
    "                        \n",
    "                if (self.creation_op == 'index_select'):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if (self.creation_op == 'cross_entropy'):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                        \n",
    "                if (self.creation_op == 'sigmoid' or self.creation_op == 'tanh'):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other], \n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data * -1, \n",
    "                          autograd=True,\n",
    "                          creators=[self], \n",
    "                          creation_op='neg')\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other], \n",
    "                          creation_op='sub')\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other], \n",
    "                          creation_op='mul')\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dim):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.sum(dim), \n",
    "                          autograd=True,\n",
    "                          creators=[self], \n",
    "                          creation_op='sum_'+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if (self.autograd):\n",
    "            return Tensor(new_data, \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='expand_'+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.transpose(), \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='transpose')\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.dot(x.data), \n",
    "                          autograd=True, \n",
    "                          creators=[self, x], \n",
    "                          creation_op='mm')\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)), \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='sigmoid')\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "    \n",
    "    def tanh(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(np.tanh(self.data), \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='tanh')\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def softmax(self):\n",
    "        e_x = np.exp(self.data - np.max(self.data))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "#      def softmax(self):\n",
    "#         e_x = np.exp(self.data - np.max(self.data))\n",
    "#         if (self.autograd):\n",
    "#             return Tensor(e_x / e_x.sum(), \n",
    "#                           autograd=True, \n",
    "#                           creators=[self], \n",
    "#                           creation_op='softmax')\n",
    "#         return Tensor(e_x / e_x.sum())\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if (self.autograd):\n",
    "            new = Tensor(self.data[indices.data], \n",
    "                         autograd=True, \n",
    "                         creators=[self], \n",
    "                         creation_op='index_select')\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, \n",
    "                                       axis=len(self.data.shape)-1, \n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        \n",
    "        if (self.autograd):\n",
    "            out = Tensor(loss, \n",
    "                         autograd=True, \n",
    "                         creators=[self], \n",
    "                         creation_op='cross_entropy')\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        \n",
    "        return Tensor(loss)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "# Page 298-300 - Layers\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        if bias:\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        else:\n",
    "            self.bias = 0\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.bias:\n",
    "            return input.mm(self.weight) + self.bias.expand(0, len(input.data))\n",
    "        return input.mm(self.weight)\n",
    "    \n",
    "class Sequential(object):\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "    \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor((weight), autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if (activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif (activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception('Non-linearity not found')\n",
    "            \n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "    \n",
    "# Page 303 - Activation Function Objects\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "# Page 297 - SGD (stochastic gradient descent) Optimizer\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "            \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if (zero):\n",
    "                p.grad.data *= 0\n",
    "                \n",
    "# Page 327 - LSTM Cell\n",
    "\n",
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)\n",
    "        \n",
    "        self.w_ho = Linear(n_inputs, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "        \n",
    "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()\n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()\n",
    "        c = (f * prev_cell) + (i * g)\n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        h.data[:, 0] += 1\n",
    "        c.data[:, 0] += 1\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "f = open('shakespear.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), \n",
    "            alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
    "min_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=100, min_loss=1000):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "        \n",
    "        for batch_i in range(batches_to_train):\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), \n",
    "                      Tensor(hidden[1].data, autograd=True))\n",
    "            losses = list()\n",
    "            \n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, \n",
    "                                               hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                \n",
    "                if (t == 0):\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "            loss = losses[-1]\n",
    "                                  \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "                                  \n",
    "            total_loss += loss.data / bptt\n",
    "            epoch_loss = np.exp(total_loss / (batch_i + 1))\n",
    "            if (epoch_loss < min_loss):\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "            log = '\\r Iter: ' + str(iter)\n",
    "            log += ' - Alpha: ' + str(optim.alpha)[:5]\n",
    "            log += ' - Batch ' + str(batch_i+1) + '/' + str(len(input_batches))\n",
    "            log += ' - Min Loss: ' + str(min_loss)[:5]\n",
    "            log += ' - Loss: ' + str(epoch_loss)\n",
    "            if (batch_i == 0):\n",
    "                s = generate_sample(70, '\\n').replace('\\n', ' ')\n",
    "                log += ' - ' + s\n",
    "            sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = ''\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iter: 0 - Alpha: 0.05 - Batch 3/249 - Min Loss: 61.75 - Loss: 61.816466210708166 -  fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n",
      " Iter: 0 - Alpha: 0.05 - Batch 4/249 - Min Loss: 61.73 - Loss: 61.730354865900416\n",
      " Iter: 0 - Alpha: 0.05 - Batch 5/249 - Min Loss: 61.54 - Loss: 61.543135304092104\n",
      " Iter: 0 - Alpha: 0.05 - Batch 6/249 - Min Loss: 61.16 - Loss: 61.16903741143134\n",
      " Iter: 0 - Alpha: 0.05 - Batch 7/249 - Min Loss: 60.53 - Loss: 60.5379099220656\n",
      " Iter: 0 - Alpha: 0.05 - Batch 8/249 - Min Loss: 59.07 - Loss: 59.07154765875954\n",
      " Iter: 0 - Alpha: 0.05 - Batch 9/249 - Min Loss: 56.30 - Loss: 56.30236278709842\n",
      " Iter: 0 - Alpha: 0.05 - Batch 10/249 - Min Loss: 54.51 - Loss: 54.515487143562126\n",
      " Iter: 0 - Alpha: 0.05 - Batch 11/249 - Min Loss: 52.89 - Loss: 52.898574270180525\n",
      " Iter: 0 - Alpha: 0.05 - Batch 12/249 - Min Loss: 50.92 - Loss: 50.926585060198775\n",
      " Iter: 0 - Alpha: 0.05 - Batch 13/249 - Min Loss: 48.37 - Loss: 48.37324892766472\n",
      " Iter: 0 - Alpha: 0.05 - Batch 14/249 - Min Loss: 47.39 - Loss: 47.39754673613307\n",
      " Iter: 0 - Alpha: 0.05 - Batch 15/249 - Min Loss: 46.00 - Loss: 46.00189384186587\n",
      " Iter: 0 - Alpha: 0.05 - Batch 16/249 - Min Loss: 44.67 - Loss: 44.67682178314435\n",
      " Iter: 0 - Alpha: 0.05 - Batch 17/249 - Min Loss: 43.46 - Loss: 43.46276404237218\n",
      " Iter: 0 - Alpha: 0.05 - Batch 18/249 - Min Loss: 42.60 - Loss: 42.6065696077509\n",
      " Iter: 0 - Alpha: 0.05 - Batch 19/249 - Min Loss: 41.91 - Loss: 41.91218759823981\n",
      " Iter: 0 - Alpha: 0.05 - Batch 20/249 - Min Loss: 41.04 - Loss: 41.043099546988735\n",
      " Iter: 0 - Alpha: 0.05 - Batch 21/249 - Min Loss: 40.09 - Loss: 40.09521237253795\n",
      " Iter: 0 - Alpha: 0.05 - Batch 23/249 - Min Loss: 40.01 - Loss: 40.18628631870866\n",
      " Iter: 0 - Alpha: 0.05 - Batch 24/249 - Min Loss: 39.84 - Loss: 39.84514606702577\n",
      " Iter: 0 - Alpha: 0.05 - Batch 25/249 - Min Loss: 39.18 - Loss: 39.18754643189411\n",
      " Iter: 0 - Alpha: 0.05 - Batch 26/249 - Min Loss: 38.37 - Loss: 38.37544055955982\n",
      " Iter: 0 - Alpha: 0.05 - Batch 27/249 - Min Loss: 37.91 - Loss: 37.91009967068573\n",
      " Iter: 0 - Alpha: 0.05 - Batch 28/249 - Min Loss: 37.30 - Loss: 37.30609940339396\n",
      " Iter: 0 - Alpha: 0.05 - Batch 29/249 - Min Loss: 36.84 - Loss: 36.84914091252351\n",
      " Iter: 0 - Alpha: 0.05 - Batch 30/249 - Min Loss: 36.38 - Loss: 36.380964044565005\n",
      " Iter: 0 - Alpha: 0.05 - Batch 31/249 - Min Loss: 36.08 - Loss: 36.081645760919464\n",
      " Iter: 0 - Alpha: 0.05 - Batch 32/249 - Min Loss: 35.69 - Loss: 35.69501903411247\n",
      " Iter: 0 - Alpha: 0.05 - Batch 33/249 - Min Loss: 35.33 - Loss: 35.331906214381085\n",
      " Iter: 0 - Alpha: 0.05 - Batch 34/249 - Min Loss: 34.96 - Loss: 34.96604185118353\n",
      " Iter: 0 - Alpha: 0.05 - Batch 35/249 - Min Loss: 34.78 - Loss: 34.78916194643538\n",
      " Iter: 0 - Alpha: 0.05 - Batch 36/249 - Min Loss: 34.58 - Loss: 34.58840621276491\n",
      " Iter: 0 - Alpha: 0.05 - Batch 37/249 - Min Loss: 34.29 - Loss: 34.29342707795922\n",
      " Iter: 0 - Alpha: 0.05 - Batch 38/249 - Min Loss: 34.05 - Loss: 34.05042079824697\n",
      " Iter: 0 - Alpha: 0.05 - Batch 39/249 - Min Loss: 33.96 - Loss: 33.96556449017779\n",
      " Iter: 0 - Alpha: 0.05 - Batch 40/249 - Min Loss: 33.62 - Loss: 33.62984873128283\n",
      " Iter: 0 - Alpha: 0.05 - Batch 42/249 - Min Loss: 33.53 - Loss: 33.55363165195085\n",
      " Iter: 0 - Alpha: 0.05 - Batch 43/249 - Min Loss: 33.24 - Loss: 33.24230435484701\n",
      " Iter: 0 - Alpha: 0.05 - Batch 44/249 - Min Loss: 33.02 - Loss: 33.025560777302644\n",
      " Iter: 0 - Alpha: 0.05 - Batch 45/249 - Min Loss: 32.90 - Loss: 32.90518030980501\n",
      " Iter: 0 - Alpha: 0.05 - Batch 48/249 - Min Loss: 32.86 - Loss: 32.92903508704471\n",
      " Iter: 0 - Alpha: 0.05 - Batch 49/249 - Min Loss: 32.75 - Loss: 32.75989645276635\n",
      " Iter: 0 - Alpha: 0.05 - Batch 50/249 - Min Loss: 32.47 - Loss: 32.47124890329307\n",
      " Iter: 0 - Alpha: 0.05 - Batch 51/249 - Min Loss: 32.18 - Loss: 32.18550715770568\n",
      " Iter: 0 - Alpha: 0.05 - Batch 52/249 - Min Loss: 31.99 - Loss: 31.994571989202527\n",
      " Iter: 0 - Alpha: 0.05 - Batch 53/249 - Min Loss: 31.85 - Loss: 31.85636678842091\n",
      " Iter: 0 - Alpha: 0.05 - Batch 54/249 - Min Loss: 31.67 - Loss: 31.677645697756464\n",
      " Iter: 0 - Alpha: 0.05 - Batch 55/249 - Min Loss: 31.48 - Loss: 31.482044897634708\n",
      " Iter: 0 - Alpha: 0.05 - Batch 56/249 - Min Loss: 31.30 - Loss: 31.30504825237426\n",
      " Iter: 0 - Alpha: 0.05 - Batch 57/249 - Min Loss: 31.08 - Loss: 31.088577166066493\n",
      " Iter: 0 - Alpha: 0.05 - Batch 58/249 - Min Loss: 30.92 - Loss: 30.92623059944504\n",
      " Iter: 0 - Alpha: 0.05 - Batch 59/249 - Min Loss: 30.72 - Loss: 30.724867313400647\n",
      " Iter: 0 - Alpha: 0.05 - Batch 60/249 - Min Loss: 30.54 - Loss: 30.549461941921976\n",
      " Iter: 0 - Alpha: 0.05 - Batch 61/249 - Min Loss: 30.32 - Loss: 30.324548629617556\n",
      " Iter: 0 - Alpha: 0.05 - Batch 62/249 - Min Loss: 30.14 - Loss: 30.14326875923591\n",
      " Iter: 0 - Alpha: 0.05 - Batch 63/249 - Min Loss: 29.91 - Loss: 29.915974266097294\n",
      " Iter: 0 - Alpha: 0.05 - Batch 64/249 - Min Loss: 29.74 - Loss: 29.745774174364804\n",
      " Iter: 0 - Alpha: 0.05 - Batch 66/249 - Min Loss: 29.74 - Loss: 29.785101874727243\n",
      " Iter: 0 - Alpha: 0.05 - Batch 67/249 - Min Loss: 29.70 - Loss: 29.701981257207834\n",
      " Iter: 0 - Alpha: 0.05 - Batch 68/249 - Min Loss: 29.48 - Loss: 29.48333834549568\n",
      " Iter: 0 - Alpha: 0.05 - Batch 69/249 - Min Loss: 29.37 - Loss: 29.372220518021987\n",
      " Iter: 0 - Alpha: 0.05 - Batch 70/249 - Min Loss: 29.20 - Loss: 29.209331691919616\n",
      " Iter: 0 - Alpha: 0.05 - Batch 71/249 - Min Loss: 29.10 - Loss: 29.105019846427872\n",
      " Iter: 0 - Alpha: 0.05 - Batch 72/249 - Min Loss: 29.04 - Loss: 29.041191372671417\n",
      " Iter: 0 - Alpha: 0.05 - Batch 73/249 - Min Loss: 28.93 - Loss: 28.937371354731546\n",
      " Iter: 0 - Alpha: 0.05 - Batch 74/249 - Min Loss: 28.75 - Loss: 28.758688478730882\n",
      " Iter: 0 - Alpha: 0.05 - Batch 75/249 - Min Loss: 28.63 - Loss: 28.631436661030335\n",
      " Iter: 0 - Alpha: 0.05 - Batch 76/249 - Min Loss: 28.53 - Loss: 28.532607632573807\n",
      " Iter: 0 - Alpha: 0.05 - Batch 77/249 - Min Loss: 28.39 - Loss: 28.39280415634042\n",
      " Iter: 0 - Alpha: 0.05 - Batch 78/249 - Min Loss: 28.30 - Loss: 28.307807175208985\n",
      " Iter: 0 - Alpha: 0.05 - Batch 79/249 - Min Loss: 28.18 - Loss: 28.187760934037026\n",
      " Iter: 0 - Alpha: 0.05 - Batch 80/249 - Min Loss: 28.07 - Loss: 28.075890837059617\n",
      " Iter: 0 - Alpha: 0.05 - Batch 81/249 - Min Loss: 27.92 - Loss: 27.922003176200196\n",
      " Iter: 0 - Alpha: 0.05 - Batch 82/249 - Min Loss: 27.83 - Loss: 27.830760952280034\n",
      " Iter: 0 - Alpha: 0.05 - Batch 83/249 - Min Loss: 27.73 - Loss: 27.734827031711678\n",
      " Iter: 0 - Alpha: 0.05 - Batch 86/249 - Min Loss: 27.70 - Loss: 27.736297097731665\n",
      " Iter: 0 - Alpha: 0.05 - Batch 87/249 - Min Loss: 27.56 - Loss: 27.565454915044157\n",
      " Iter: 0 - Alpha: 0.05 - Batch 88/249 - Min Loss: 27.42 - Loss: 27.424004789794473\n",
      " Iter: 0 - Alpha: 0.05 - Batch 89/249 - Min Loss: 27.28 - Loss: 27.28130965808894\n",
      " Iter: 0 - Alpha: 0.05 - Batch 90/249 - Min Loss: 27.15 - Loss: 27.154966213595863\n",
      " Iter: 0 - Alpha: 0.05 - Batch 91/249 - Min Loss: 27.04 - Loss: 27.049204984994127\n",
      " Iter: 0 - Alpha: 0.05 - Batch 92/249 - Min Loss: 27.00 - Loss: 27.004487974447954\n",
      " Iter: 0 - Alpha: 0.05 - Batch 93/249 - Min Loss: 26.84 - Loss: 26.849740558857185\n",
      " Iter: 0 - Alpha: 0.05 - Batch 94/249 - Min Loss: 26.73 - Loss: 26.732858894182744\n",
      " Iter: 0 - Alpha: 0.05 - Batch 95/249 - Min Loss: 26.61 - Loss: 26.611621650732268\n",
      " Iter: 0 - Alpha: 0.05 - Batch 96/249 - Min Loss: 26.56 - Loss: 26.561966484627273\n",
      " Iter: 0 - Alpha: 0.05 - Batch 97/249 - Min Loss: 26.44 - Loss: 26.44893737560759\n",
      " Iter: 0 - Alpha: 0.05 - Batch 98/249 - Min Loss: 26.34 - Loss: 26.343391948862244\n",
      " Iter: 0 - Alpha: 0.05 - Batch 99/249 - Min Loss: 26.21 - Loss: 26.213021099608838\n",
      " Iter: 0 - Alpha: 0.05 - Batch 100/249 - Min Loss: 26.10 - Loss: 26.107895483641094\n",
      " Iter: 0 - Alpha: 0.05 - Batch 101/249 - Min Loss: 26.02 - Loss: 26.02800534031781\n",
      " Iter: 0 - Alpha: 0.05 - Batch 102/249 - Min Loss: 25.91 - Loss: 25.91812005230684\n",
      " Iter: 0 - Alpha: 0.05 - Batch 103/249 - Min Loss: 25.80 - Loss: 25.805946274622272\n",
      " Iter: 0 - Alpha: 0.05 - Batch 104/249 - Min Loss: 25.71 - Loss: 25.717549936420856\n",
      " Iter: 0 - Alpha: 0.05 - Batch 105/249 - Min Loss: 25.63 - Loss: 25.6395137557101\n",
      " Iter: 0 - Alpha: 0.05 - Batch 106/249 - Min Loss: 25.54 - Loss: 25.544962938985915\n",
      " Iter: 0 - Alpha: 0.05 - Batch 107/249 - Min Loss: 25.48 - Loss: 25.48883972150984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 0 - Alpha: 0.05 - Batch 108/249 - Min Loss: 25.38 - Loss: 25.381193237232704\n",
      " Iter: 0 - Alpha: 0.05 - Batch 109/249 - Min Loss: 25.28 - Loss: 25.289942704084417\n",
      " Iter: 0 - Alpha: 0.05 - Batch 110/249 - Min Loss: 25.21 - Loss: 25.218444557075593\n",
      " Iter: 0 - Alpha: 0.05 - Batch 111/249 - Min Loss: 25.15 - Loss: 25.156905351159654\n",
      " Iter: 0 - Alpha: 0.05 - Batch 112/249 - Min Loss: 25.06 - Loss: 25.0618011198572\n",
      " Iter: 0 - Alpha: 0.05 - Batch 113/249 - Min Loss: 24.95 - Loss: 24.959235194229176\n",
      " Iter: 0 - Alpha: 0.05 - Batch 114/249 - Min Loss: 24.87 - Loss: 24.873807193663595\n",
      " Iter: 0 - Alpha: 0.05 - Batch 115/249 - Min Loss: 24.78 - Loss: 24.789342475809136\n",
      " Iter: 0 - Alpha: 0.05 - Batch 116/249 - Min Loss: 24.70 - Loss: 24.70049731434871\n",
      " Iter: 0 - Alpha: 0.05 - Batch 117/249 - Min Loss: 24.63 - Loss: 24.630266426284877\n",
      " Iter: 0 - Alpha: 0.05 - Batch 118/249 - Min Loss: 24.53 - Loss: 24.537973084409742\n",
      " Iter: 0 - Alpha: 0.05 - Batch 119/249 - Min Loss: 24.47 - Loss: 24.47731199660316\n",
      " Iter: 0 - Alpha: 0.05 - Batch 120/249 - Min Loss: 24.38 - Loss: 24.387348935737215\n",
      " Iter: 0 - Alpha: 0.05 - Batch 121/249 - Min Loss: 24.30 - Loss: 24.304028383423834\n",
      " Iter: 0 - Alpha: 0.05 - Batch 122/249 - Min Loss: 24.21 - Loss: 24.211241961031835\n",
      " Iter: 0 - Alpha: 0.05 - Batch 123/249 - Min Loss: 24.11 - Loss: 24.116063864793286\n",
      " Iter: 0 - Alpha: 0.05 - Batch 124/249 - Min Loss: 24.02 - Loss: 24.0276297523877\n",
      " Iter: 0 - Alpha: 0.05 - Batch 125/249 - Min Loss: 23.95 - Loss: 23.955204899590097\n",
      " Iter: 0 - Alpha: 0.05 - Batch 126/249 - Min Loss: 23.89 - Loss: 23.892166250739347\n",
      " Iter: 0 - Alpha: 0.05 - Batch 127/249 - Min Loss: 23.81 - Loss: 23.81504565738678\n",
      " Iter: 0 - Alpha: 0.05 - Batch 128/249 - Min Loss: 23.71 - Loss: 23.714337405564685\n",
      " Iter: 0 - Alpha: 0.05 - Batch 129/249 - Min Loss: 23.62 - Loss: 23.62658473402046\n",
      " Iter: 0 - Alpha: 0.05 - Batch 130/249 - Min Loss: 23.55 - Loss: 23.55989644194876\n",
      " Iter: 0 - Alpha: 0.05 - Batch 131/249 - Min Loss: 23.47 - Loss: 23.471104598980304\n",
      " Iter: 0 - Alpha: 0.05 - Batch 132/249 - Min Loss: 23.37 - Loss: 23.379947045506732\n",
      " Iter: 0 - Alpha: 0.05 - Batch 133/249 - Min Loss: 23.29 - Loss: 23.297342094606865\n",
      " Iter: 0 - Alpha: 0.05 - Batch 134/249 - Min Loss: 23.21 - Loss: 23.2102955783964\n",
      " Iter: 0 - Alpha: 0.05 - Batch 135/249 - Min Loss: 23.16 - Loss: 23.1644511729716\n",
      " Iter: 0 - Alpha: 0.05 - Batch 136/249 - Min Loss: 23.15 - Loss: 23.154899811197286\n",
      " Iter: 0 - Alpha: 0.05 - Batch 137/249 - Min Loss: 23.09 - Loss: 23.09031401274478\n",
      " Iter: 0 - Alpha: 0.05 - Batch 138/249 - Min Loss: 23.01 - Loss: 23.01063330935238\n",
      " Iter: 0 - Alpha: 0.05 - Batch 141/249 - Min Loss: 23.00 - Loss: 23.014364545298424\n",
      " Iter: 0 - Alpha: 0.05 - Batch 142/249 - Min Loss: 22.93 - Loss: 22.932300504211057\n",
      " Iter: 0 - Alpha: 0.05 - Batch 143/249 - Min Loss: 22.85 - Loss: 22.856347463454895\n",
      " Iter: 0 - Alpha: 0.05 - Batch 144/249 - Min Loss: 22.78 - Loss: 22.78395780746694\n",
      " Iter: 0 - Alpha: 0.05 - Batch 145/249 - Min Loss: 22.72 - Loss: 22.72506244002572\n",
      " Iter: 0 - Alpha: 0.05 - Batch 146/249 - Min Loss: 22.71 - Loss: 22.71443975455018\n",
      " Iter: 0 - Alpha: 0.05 - Batch 147/249 - Min Loss: 22.65 - Loss: 22.657748896520523\n",
      " Iter: 0 - Alpha: 0.05 - Batch 148/249 - Min Loss: 22.59 - Loss: 22.591349167105044\n",
      " Iter: 0 - Alpha: 0.05 - Batch 149/249 - Min Loss: 22.51 - Loss: 22.517863150873406\n",
      " Iter: 0 - Alpha: 0.05 - Batch 150/249 - Min Loss: 22.45 - Loss: 22.45305728238209\n",
      " Iter: 0 - Alpha: 0.05 - Batch 151/249 - Min Loss: 22.40 - Loss: 22.409457256659728\n",
      " Iter: 0 - Alpha: 0.05 - Batch 152/249 - Min Loss: 22.34 - Loss: 22.348499327772725\n",
      " Iter: 0 - Alpha: 0.05 - Batch 153/249 - Min Loss: 22.26 - Loss: 22.267188075163084\n",
      " Iter: 0 - Alpha: 0.05 - Batch 154/249 - Min Loss: 22.20 - Loss: 22.202368305068212\n",
      " Iter: 0 - Alpha: 0.05 - Batch 155/249 - Min Loss: 22.16 - Loss: 22.165086608700335\n",
      " Iter: 0 - Alpha: 0.05 - Batch 156/249 - Min Loss: 22.14 - Loss: 22.14536582832697\n",
      " Iter: 0 - Alpha: 0.05 - Batch 157/249 - Min Loss: 22.10 - Loss: 22.10064592288289\n",
      " Iter: 0 - Alpha: 0.05 - Batch 158/249 - Min Loss: 22.04 - Loss: 22.046575555044488\n",
      " Iter: 0 - Alpha: 0.05 - Batch 159/249 - Min Loss: 22.01 - Loss: 22.014179403958483\n",
      " Iter: 0 - Alpha: 0.05 - Batch 160/249 - Min Loss: 21.95 - Loss: 21.955831982570036\n",
      " Iter: 0 - Alpha: 0.05 - Batch 161/249 - Min Loss: 21.89 - Loss: 21.891915316939713\n",
      " Iter: 0 - Alpha: 0.05 - Batch 162/249 - Min Loss: 21.83 - Loss: 21.83800919263669\n",
      " Iter: 0 - Alpha: 0.05 - Batch 163/249 - Min Loss: 21.78 - Loss: 21.78143751403725\n",
      " Iter: 0 - Alpha: 0.05 - Batch 164/249 - Min Loss: 21.73 - Loss: 21.73489495167019\n",
      " Iter: 0 - Alpha: 0.05 - Batch 165/249 - Min Loss: 21.69 - Loss: 21.69294736661139\n",
      " Iter: 0 - Alpha: 0.05 - Batch 166/249 - Min Loss: 21.64 - Loss: 21.642834783164687\n",
      " Iter: 0 - Alpha: 0.05 - Batch 167/249 - Min Loss: 21.61 - Loss: 21.61611561288436\n",
      " Iter: 0 - Alpha: 0.05 - Batch 168/249 - Min Loss: 21.58 - Loss: 21.58399055468839\n",
      " Iter: 0 - Alpha: 0.05 - Batch 169/249 - Min Loss: 21.52 - Loss: 21.528075160865434\n",
      " Iter: 0 - Alpha: 0.05 - Batch 170/249 - Min Loss: 21.48 - Loss: 21.482123509309055\n",
      " Iter: 0 - Alpha: 0.05 - Batch 171/249 - Min Loss: 21.44 - Loss: 21.442483198188174\n",
      " Iter: 0 - Alpha: 0.05 - Batch 172/249 - Min Loss: 21.40 - Loss: 21.408855902204966\n",
      " Iter: 0 - Alpha: 0.05 - Batch 173/249 - Min Loss: 21.37 - Loss: 21.370986311171524\n",
      " Iter: 0 - Alpha: 0.05 - Batch 174/249 - Min Loss: 21.33 - Loss: 21.33817953675507\n",
      " Iter: 0 - Alpha: 0.05 - Batch 175/249 - Min Loss: 21.31 - Loss: 21.31972520128034\n",
      " Iter: 0 - Alpha: 0.05 - Batch 176/249 - Min Loss: 21.28 - Loss: 21.28186373588462\n",
      " Iter: 0 - Alpha: 0.05 - Batch 177/249 - Min Loss: 21.23 - Loss: 21.23735372190975\n",
      " Iter: 0 - Alpha: 0.05 - Batch 178/249 - Min Loss: 21.19 - Loss: 21.195053963564455\n",
      " Iter: 0 - Alpha: 0.05 - Batch 179/249 - Min Loss: 21.17 - Loss: 21.176681630868732\n",
      " Iter: 0 - Alpha: 0.05 - Batch 180/249 - Min Loss: 21.13 - Loss: 21.13410295722146\n",
      " Iter: 0 - Alpha: 0.05 - Batch 181/249 - Min Loss: 21.08 - Loss: 21.088145084152597\n",
      " Iter: 0 - Alpha: 0.05 - Batch 182/249 - Min Loss: 21.04 - Loss: 21.040920039330356\n",
      " Iter: 0 - Alpha: 0.05 - Batch 183/249 - Min Loss: 20.98 - Loss: 20.98994925831061\n",
      " Iter: 0 - Alpha: 0.05 - Batch 184/249 - Min Loss: 20.95 - Loss: 20.95195276331143\n",
      " Iter: 0 - Alpha: 0.05 - Batch 185/249 - Min Loss: 20.90 - Loss: 20.908797748918925\n",
      " Iter: 0 - Alpha: 0.05 - Batch 186/249 - Min Loss: 20.87 - Loss: 20.87801303842851\n",
      " Iter: 0 - Alpha: 0.05 - Batch 188/249 - Min Loss: 20.85 - Loss: 20.854869674483925\n",
      " Iter: 0 - Alpha: 0.05 - Batch 189/249 - Min Loss: 20.82 - Loss: 20.82143052356004\n",
      " Iter: 0 - Alpha: 0.05 - Batch 190/249 - Min Loss: 20.79 - Loss: 20.794692263634534\n",
      " Iter: 0 - Alpha: 0.05 - Batch 191/249 - Min Loss: 20.76 - Loss: 20.7657426001051\n",
      " Iter: 0 - Alpha: 0.05 - Batch 192/249 - Min Loss: 20.73 - Loss: 20.737662927911828\n",
      " Iter: 0 - Alpha: 0.05 - Batch 193/249 - Min Loss: 20.67 - Loss: 20.67744015694803\n",
      " Iter: 0 - Alpha: 0.05 - Batch 194/249 - Min Loss: 20.66 - Loss: 20.6610383087056\n",
      " Iter: 0 - Alpha: 0.05 - Batch 195/249 - Min Loss: 20.65 - Loss: 20.65566644466359\n",
      " Iter: 0 - Alpha: 0.05 - Batch 196/249 - Min Loss: 20.61 - Loss: 20.61247858322741\n",
      " Iter: 0 - Alpha: 0.05 - Batch 197/249 - Min Loss: 20.58 - Loss: 20.58329232638155\n",
      " Iter: 0 - Alpha: 0.05 - Batch 198/249 - Min Loss: 20.55 - Loss: 20.55528082372374\n",
      " Iter: 0 - Alpha: 0.05 - Batch 199/249 - Min Loss: 20.51 - Loss: 20.514277752366297\n",
      " Iter: 0 - Alpha: 0.05 - Batch 200/249 - Min Loss: 20.46 - Loss: 20.46707154601533\n",
      " Iter: 0 - Alpha: 0.05 - Batch 201/249 - Min Loss: 20.43 - Loss: 20.43815175197105\n",
      " Iter: 0 - Alpha: 0.05 - Batch 202/249 - Min Loss: 20.41 - Loss: 20.411470602348963\n",
      " Iter: 0 - Alpha: 0.05 - Batch 203/249 - Min Loss: 20.37 - Loss: 20.37583949465653\n",
      " Iter: 0 - Alpha: 0.05 - Batch 204/249 - Min Loss: 20.33 - Loss: 20.339789928755\n",
      " Iter: 0 - Alpha: 0.05 - Batch 205/249 - Min Loss: 20.30 - Loss: 20.308204501801374\n",
      " Iter: 0 - Alpha: 0.05 - Batch 206/249 - Min Loss: 20.26 - Loss: 20.26646386828338\n",
      " Iter: 0 - Alpha: 0.05 - Batch 207/249 - Min Loss: 20.23 - Loss: 20.230598531080364\n",
      " Iter: 0 - Alpha: 0.05 - Batch 208/249 - Min Loss: 20.19 - Loss: 20.194703834092895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 0 - Alpha: 0.05 - Batch 209/249 - Min Loss: 20.14 - Loss: 20.148583143494243\n",
      " Iter: 0 - Alpha: 0.05 - Batch 210/249 - Min Loss: 20.10 - Loss: 20.104223190216725\n",
      " Iter: 0 - Alpha: 0.05 - Batch 211/249 - Min Loss: 20.05 - Loss: 20.050921123374064\n",
      " Iter: 0 - Alpha: 0.05 - Batch 212/249 - Min Loss: 20.00 - Loss: 20.008330641458862\n",
      " Iter: 0 - Alpha: 0.05 - Batch 213/249 - Min Loss: 19.97 - Loss: 19.97447239730344\n",
      " Iter: 0 - Alpha: 0.05 - Batch 214/249 - Min Loss: 19.94 - Loss: 19.94157023212201\n",
      " Iter: 0 - Alpha: 0.05 - Batch 215/249 - Min Loss: 19.89 - Loss: 19.89702209582327\n",
      " Iter: 0 - Alpha: 0.05 - Batch 216/249 - Min Loss: 19.85 - Loss: 19.858437168004794\n",
      " Iter: 0 - Alpha: 0.05 - Batch 217/249 - Min Loss: 19.83 - Loss: 19.833068725912796\n",
      " Iter: 0 - Alpha: 0.05 - Batch 218/249 - Min Loss: 19.79 - Loss: 19.790286930962004\n",
      " Iter: 0 - Alpha: 0.05 - Batch 219/249 - Min Loss: 19.73 - Loss: 19.738668093577964\n",
      " Iter: 0 - Alpha: 0.05 - Batch 220/249 - Min Loss: 19.71 - Loss: 19.715726506074358\n",
      " Iter: 0 - Alpha: 0.05 - Batch 221/249 - Min Loss: 19.70 - Loss: 19.706343617457936\n",
      " Iter: 0 - Alpha: 0.05 - Batch 222/249 - Min Loss: 19.67 - Loss: 19.673825603957265\n",
      " Iter: 0 - Alpha: 0.05 - Batch 223/249 - Min Loss: 19.64 - Loss: 19.641651921491764\n",
      " Iter: 0 - Alpha: 0.05 - Batch 224/249 - Min Loss: 19.61 - Loss: 19.613142721951466\n",
      " Iter: 0 - Alpha: 0.05 - Batch 225/249 - Min Loss: 19.58 - Loss: 19.581719645146798\n",
      " Iter: 0 - Alpha: 0.05 - Batch 226/249 - Min Loss: 19.54 - Loss: 19.544781934461152\n",
      " Iter: 0 - Alpha: 0.05 - Batch 227/249 - Min Loss: 19.51 - Loss: 19.516833175643317\n",
      " Iter: 0 - Alpha: 0.05 - Batch 228/249 - Min Loss: 19.50 - Loss: 19.502940383510236\n",
      " Iter: 0 - Alpha: 0.05 - Batch 229/249 - Min Loss: 19.47 - Loss: 19.472732526170823\n",
      " Iter: 0 - Alpha: 0.05 - Batch 230/249 - Min Loss: 19.44 - Loss: 19.443423159566112\n",
      " Iter: 0 - Alpha: 0.05 - Batch 231/249 - Min Loss: 19.42 - Loss: 19.426117816756072\n",
      " Iter: 0 - Alpha: 0.05 - Batch 232/249 - Min Loss: 19.39 - Loss: 19.394873470035105\n",
      " Iter: 0 - Alpha: 0.05 - Batch 233/249 - Min Loss: 19.37 - Loss: 19.375936500280318\n",
      " Iter: 0 - Alpha: 0.05 - Batch 234/249 - Min Loss: 19.34 - Loss: 19.34478093858626\n",
      " Iter: 0 - Alpha: 0.05 - Batch 235/249 - Min Loss: 19.31 - Loss: 19.310217660569183\n",
      " Iter: 0 - Alpha: 0.05 - Batch 236/249 - Min Loss: 19.27 - Loss: 19.272043096301626\n",
      " Iter: 0 - Alpha: 0.05 - Batch 237/249 - Min Loss: 19.24 - Loss: 19.246016283362806\n",
      " Iter: 0 - Alpha: 0.05 - Batch 238/249 - Min Loss: 19.22 - Loss: 19.223013514548082\n",
      " Iter: 0 - Alpha: 0.05 - Batch 239/249 - Min Loss: 19.20 - Loss: 19.207059768110877\n",
      " Iter: 0 - Alpha: 0.05 - Batch 240/249 - Min Loss: 19.19 - Loss: 19.19361717513393\n",
      " Iter: 0 - Alpha: 0.05 - Batch 241/249 - Min Loss: 19.15 - Loss: 19.155857336426955\n",
      " Iter: 0 - Alpha: 0.05 - Batch 242/249 - Min Loss: 19.12 - Loss: 19.125497497147382\n",
      " Iter: 0 - Alpha: 0.05 - Batch 244/249 - Min Loss: 19.09 - Loss: 19.095312294320685\n",
      " Iter: 0 - Alpha: 0.05 - Batch 245/249 - Min Loss: 19.07 - Loss: 19.078232879151805\n",
      " Iter: 0 - Alpha: 0.05 - Batch 246/249 - Min Loss: 19.06 - Loss: 19.061099427886372\n",
      " Iter: 0 - Alpha: 0.05 - Batch 247/249 - Min Loss: 19.03 - Loss: 19.035251988945014\n",
      " Iter: 0 - Alpha: 0.05 - Batch 248/249 - Min Loss: 19.00 - Loss: 19.005394436998657\n",
      " Iter: 0 - Alpha: 0.05 - Batch 249/249 - Min Loss: 18.98 - Loss: 18.980816521703687\n",
      " Iter: 1 - Alpha: 0.049 - Batch 1/249 - Min Loss: 13.12 - Loss: 13.121032185263042 - Ther ther ther ther ther ther ther ther ther ther therf ther ther ther\n",
      " Iter: 1 - Alpha: 0.049 - Batch 3/249 - Min Loss: 12.87 - Loss: 12.953971150245167\n",
      " Iter: 1 - Alpha: 0.049 - Batch 4/249 - Min Loss: 12.81 - Loss: 12.818318741170255\n",
      " Iter: 2 - Alpha: 0.049 - Batch 4/249 - Min Loss: 12.68 - Loss: 12.822802842401094 - Therend therrend thefrend therfrend therres, Thend therrend therrend t\n",
      " Iter: 2 - Alpha: 0.049 - Batch 205/249 - Min Loss: 12.56 - Loss: 12.569659563032384\n",
      " Iter: 2 - Alpha: 0.049 - Batch 206/249 - Min Loss: 12.56 - Loss: 12.563618169232297\n",
      " Iter: 2 - Alpha: 0.049 - Batch 207/249 - Min Loss: 12.55 - Loss: 12.555913404546166\n",
      " Iter: 2 - Alpha: 0.049 - Batch 208/249 - Min Loss: 12.54 - Loss: 12.549635234844208\n",
      " Iter: 2 - Alpha: 0.049 - Batch 209/249 - Min Loss: 12.54 - Loss: 12.540921221137785\n",
      " Iter: 2 - Alpha: 0.049 - Batch 210/249 - Min Loss: 12.53 - Loss: 12.531984144122736\n",
      " Iter: 2 - Alpha: 0.049 - Batch 211/249 - Min Loss: 12.51 - Loss: 12.515427193505479\n",
      " Iter: 2 - Alpha: 0.049 - Batch 212/249 - Min Loss: 12.50 - Loss: 12.507297822779227\n",
      " Iter: 2 - Alpha: 0.049 - Batch 213/249 - Min Loss: 12.50 - Loss: 12.502529726340542\n",
      " Iter: 2 - Alpha: 0.049 - Batch 214/249 - Min Loss: 12.49 - Loss: 12.499239675446757\n",
      " Iter: 2 - Alpha: 0.049 - Batch 215/249 - Min Loss: 12.48 - Loss: 12.489175059946339\n",
      " Iter: 2 - Alpha: 0.049 - Batch 217/249 - Min Loss: 12.47 - Loss: 12.482975125484865\n",
      " Iter: 2 - Alpha: 0.049 - Batch 218/249 - Min Loss: 12.47 - Loss: 12.474584334674422\n",
      " Iter: 3 - Alpha: 0.048 - Batch 3/249 - Min Loss: 12.46 - Loss: 12.586181103879552 - Thend theres, and theseres, Thend theferes, and theseres,fand therfefr\n",
      " Iter: 3 - Alpha: 0.048 - Batch 4/249 - Min Loss: 12.26 - Loss: 12.26916198802295\n",
      " Iter: 3 - Alpha: 0.048 - Batch 34/249 - Min Loss: 12.06 - Loss: 12.080260199458175\n",
      " Iter: 3 - Alpha: 0.048 - Batch 105/249 - Min Loss: 12.04 - Loss: 12.052077272571283\n",
      " Iter: 3 - Alpha: 0.048 - Batch 107/249 - Min Loss: 12.04 - Loss: 12.044887705800996\n",
      " Iter: 3 - Alpha: 0.048 - Batch 210/249 - Min Loss: 12.03 - Loss: 12.041481508577252\n",
      " Iter: 3 - Alpha: 0.048 - Batch 211/249 - Min Loss: 12.02 - Loss: 12.022428532020207\n",
      " Iter: 3 - Alpha: 0.048 - Batch 212/249 - Min Loss: 12.01 - Loss: 12.013876733105473\n",
      " Iter: 3 - Alpha: 0.048 - Batch 213/249 - Min Loss: 12.00 - Loss: 12.009306782967913\n",
      " Iter: 3 - Alpha: 0.048 - Batch 214/249 - Min Loss: 12.00 - Loss: 12.00563728944089\n",
      " Iter: 3 - Alpha: 0.048 - Batch 215/249 - Min Loss: 11.99 - Loss: 11.99498841699632\n",
      " Iter: 3 - Alpha: 0.048 - Batch 217/249 - Min Loss: 11.98 - Loss: 11.990164709116733\n",
      " Iter: 3 - Alpha: 0.048 - Batch 218/249 - Min Loss: 11.98 - Loss: 11.982037158245138\n",
      " Iter: 3 - Alpha: 0.048 - Batch 219/249 - Min Loss: 11.96 - Loss: 11.967208805564757\n",
      " Iter: 3 - Alpha: 0.048 - Batch 223/249 - Min Loss: 11.96 - Loss: 11.968899050070041\n",
      " Iter: 3 - Alpha: 0.048 - Batch 224/249 - Min Loss: 11.96 - Loss: 11.965256147901854\n",
      " Iter: 3 - Alpha: 0.048 - Batch 225/249 - Min Loss: 11.96 - Loss: 11.963068795139312\n",
      " Iter: 3 - Alpha: 0.048 - Batch 234/249 - Min Loss: 11.96 - Loss: 11.968856240485449\n",
      " Iter: 3 - Alpha: 0.048 - Batch 235/249 - Min Loss: 11.96 - Loss: 11.96030411212566\n",
      " Iter: 3 - Alpha: 0.048 - Batch 236/249 - Min Loss: 11.95 - Loss: 11.95139389000233\n",
      " Iter: 4 - Alpha: 0.048 - Batch 23/249 - Min Loss: 11.94 - Loss: 12.013532808610957- Theseres,  Beates,fers, fend theseres,fers,  Beates, Theseres,  Beates\n",
      " Iter: 4 - Alpha: 0.048 - Batch 24/249 - Min Loss: 11.92 - Loss: 11.928719047717554\n",
      " Iter: 4 - Alpha: 0.048 - Batch 25/249 - Min Loss: 11.83 - Loss: 11.836845997072567\n",
      " Iter: 4 - Alpha: 0.048 - Batch 33/249 - Min Loss: 11.72 - Loss: 11.751609258736986\n",
      " Iter: 4 - Alpha: 0.048 - Batch 34/249 - Min Loss: 11.71 - Loss: 11.712479257843366\n",
      " Iter: 4 - Alpha: 0.048 - Batch 56/249 - Min Loss: 11.68 - Loss: 11.704306126458496\n",
      " Iter: 4 - Alpha: 0.048 - Batch 92/249 - Min Loss: 11.67 - Loss: 11.697784261363225\n",
      " Iter: 4 - Alpha: 0.048 - Batch 93/249 - Min Loss: 11.66 - Loss: 11.666068717153443\n",
      " Iter: 4 - Alpha: 0.048 - Batch 94/249 - Min Loss: 11.64 - Loss: 11.647323399132159\n",
      " Iter: 4 - Alpha: 0.048 - Batch 97/249 - Min Loss: 11.63 - Loss: 11.637567228225475\n",
      " Iter: 4 - Alpha: 0.048 - Batch 100/249 - Min Loss: 11.62 - Loss: 11.625453449522553\n",
      " Iter: 4 - Alpha: 0.048 - Batch 101/249 - Min Loss: 11.61 - Loss: 11.61532802080231\n",
      " Iter: 4 - Alpha: 0.048 - Batch 102/249 - Min Loss: 11.60 - Loss: 11.601710099576286\n",
      " Iter: 4 - Alpha: 0.048 - Batch 103/249 - Min Loss: 11.59 - Loss: 11.599373007925191\n",
      " Iter: 4 - Alpha: 0.048 - Batch 104/249 - Min Loss: 11.59 - Loss: 11.599157350887797\n",
      " Iter: 4 - Alpha: 0.048 - Batch 105/249 - Min Loss: 11.59 - Loss: 11.59636804953803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 4 - Alpha: 0.048 - Batch 107/249 - Min Loss: 11.58 - Loss: 11.585366901454583\n",
      " Iter: 4 - Alpha: 0.048 - Batch 126/249 - Min Loss: 11.57 - Loss: 11.577137637529955\n",
      " Iter: 4 - Alpha: 0.048 - Batch 127/249 - Min Loss: 11.56 - Loss: 11.56718687077202\n",
      " Iter: 4 - Alpha: 0.048 - Batch 128/249 - Min Loss: 11.56 - Loss: 11.563548283870238\n",
      " Iter: 4 - Alpha: 0.048 - Batch 129/249 - Min Loss: 11.55 - Loss: 11.553978120383896\n",
      " Iter: 4 - Alpha: 0.048 - Batch 130/249 - Min Loss: 11.55 - Loss: 11.550158691497899\n",
      " Iter: 4 - Alpha: 0.048 - Batch 131/249 - Min Loss: 11.53 - Loss: 11.530289911015212\n",
      " Iter: 4 - Alpha: 0.048 - Batch 133/249 - Min Loss: 11.51 - Loss: 11.518453686426744\n",
      " Iter: 4 - Alpha: 0.048 - Batch 135/249 - Min Loss: 11.50 - Loss: 11.512033018025228\n",
      " Iter: 4 - Alpha: 0.048 - Batch 136/249 - Min Loss: 11.49 - Loss: 11.49798828600684\n",
      " Iter: 4 - Alpha: 0.048 - Batch 137/249 - Min Loss: 11.49 - Loss: 11.496059505763963\n",
      " Iter: 4 - Alpha: 0.048 - Batch 151/249 - Min Loss: 11.47 - Loss: 11.482910548664078\n",
      " Iter: 4 - Alpha: 0.048 - Batch 152/249 - Min Loss: 11.47 - Loss: 11.477426549563878\n",
      " Iter: 4 - Alpha: 0.048 - Batch 153/249 - Min Loss: 11.46 - Loss: 11.465250985104994\n",
      " Iter: 4 - Alpha: 0.048 - Batch 209/249 - Min Loss: 11.45 - Loss: 11.456146807918849\n",
      " Iter: 4 - Alpha: 0.048 - Batch 210/249 - Min Loss: 11.44 - Loss: 11.448252704340074\n",
      " Iter: 4 - Alpha: 0.048 - Batch 211/249 - Min Loss: 11.42 - Loss: 11.42991188858774\n",
      " Iter: 4 - Alpha: 0.048 - Batch 212/249 - Min Loss: 11.42 - Loss: 11.423131022207743\n",
      " Iter: 4 - Alpha: 0.048 - Batch 213/249 - Min Loss: 11.41 - Loss: 11.418505492171887\n",
      " Iter: 4 - Alpha: 0.048 - Batch 214/249 - Min Loss: 11.41 - Loss: 11.41667546726961\n",
      " Iter: 4 - Alpha: 0.048 - Batch 215/249 - Min Loss: 11.40 - Loss: 11.405565813333352\n",
      " Iter: 4 - Alpha: 0.048 - Batch 217/249 - Min Loss: 11.39 - Loss: 11.40087057498982\n",
      " Iter: 4 - Alpha: 0.048 - Batch 218/249 - Min Loss: 11.39 - Loss: 11.393369652850593\n",
      " Iter: 4 - Alpha: 0.048 - Batch 222/249 - Min Loss: 11.38 - Loss: 11.381701323615124\n",
      " Iter: 4 - Alpha: 0.048 - Batch 223/249 - Min Loss: 11.37 - Loss: 11.378567379750967\n",
      " Iter: 4 - Alpha: 0.048 - Batch 224/249 - Min Loss: 11.37 - Loss: 11.372703362249228\n",
      " Iter: 4 - Alpha: 0.048 - Batch 225/249 - Min Loss: 11.36 - Loss: 11.367882046444576\n",
      " Iter: 4 - Alpha: 0.048 - Batch 226/249 - Min Loss: 11.36 - Loss: 11.363591919990917\n",
      " Iter: 4 - Alpha: 0.048 - Batch 227/249 - Min Loss: 11.35 - Loss: 11.359389559392937\n",
      " Iter: 4 - Alpha: 0.048 - Batch 234/249 - Min Loss: 11.35 - Loss: 11.355947006140783\n",
      " Iter: 4 - Alpha: 0.048 - Batch 235/249 - Min Loss: 11.34 - Loss: 11.348648296882601\n",
      " Iter: 4 - Alpha: 0.048 - Batch 236/249 - Min Loss: 11.34 - Loss: 11.340420017985652\n",
      " Iter: 5 - Alpha: 0.047 - Batch 33/249 - Min Loss: 11.33 - Loss: 11.373770010324247 Theest, Buthesteres, and seates,  Buthest, Theestfers, and sefers,ferf\n",
      " Iter: 5 - Alpha: 0.047 - Batch 34/249 - Min Loss: 11.32 - Loss: 11.329473579827988\n",
      " Iter: 5 - Alpha: 0.047 - Batch 93/249 - Min Loss: 11.30 - Loss: 11.324909090064523\n",
      " Iter: 5 - Alpha: 0.047 - Batch 94/249 - Min Loss: 11.30 - Loss: 11.30398187276068\n",
      " Iter: 5 - Alpha: 0.047 - Batch 96/249 - Min Loss: 11.28 - Loss: 11.285844972182142\n",
      " Iter: 5 - Alpha: 0.047 - Batch 97/249 - Min Loss: 11.27 - Loss: 11.27441038930495\n",
      " Iter: 5 - Alpha: 0.047 - Batch 99/249 - Min Loss: 11.26 - Loss: 11.261747022147075\n",
      " Iter: 5 - Alpha: 0.047 - Batch 100/249 - Min Loss: 11.25 - Loss: 11.251824148391368\n",
      " Iter: 5 - Alpha: 0.047 - Batch 101/249 - Min Loss: 11.23 - Loss: 11.23459265769634\n",
      " Iter: 5 - Alpha: 0.047 - Batch 102/249 - Min Loss: 11.21 - Loss: 11.216814467360154\n",
      " Iter: 5 - Alpha: 0.047 - Batch 103/249 - Min Loss: 11.21 - Loss: 11.210605680089857\n",
      " Iter: 5 - Alpha: 0.047 - Batch 104/249 - Min Loss: 11.20 - Loss: 11.207358422307772\n",
      " Iter: 5 - Alpha: 0.047 - Batch 105/249 - Min Loss: 11.20 - Loss: 11.203619784031533\n",
      " Iter: 5 - Alpha: 0.047 - Batch 106/249 - Min Loss: 11.18 - Loss: 11.189323335428348\n",
      " Iter: 5 - Alpha: 0.047 - Batch 107/249 - Min Loss: 11.18 - Loss: 11.18837731587401\n",
      " Iter: 5 - Alpha: 0.047 - Batch 127/249 - Min Loss: 11.18 - Loss: 11.188093494832762\n",
      " Iter: 5 - Alpha: 0.047 - Batch 128/249 - Min Loss: 11.18 - Loss: 11.181062463121657\n",
      " Iter: 5 - Alpha: 0.047 - Batch 129/249 - Min Loss: 11.17 - Loss: 11.172281178699773\n",
      " Iter: 5 - Alpha: 0.047 - Batch 130/249 - Min Loss: 11.16 - Loss: 11.167531892623128\n",
      " Iter: 5 - Alpha: 0.047 - Batch 131/249 - Min Loss: 11.14 - Loss: 11.149684049564534\n",
      " Iter: 5 - Alpha: 0.047 - Batch 132/249 - Min Loss: 11.13 - Loss: 11.132773809790402\n",
      " Iter: 5 - Alpha: 0.047 - Batch 133/249 - Min Loss: 11.13 - Loss: 11.131620228850375\n",
      " Iter: 5 - Alpha: 0.047 - Batch 135/249 - Min Loss: 11.11 - Loss: 11.122339886161187\n",
      " Iter: 5 - Alpha: 0.047 - Batch 137/249 - Min Loss: 11.11 - Loss: 11.111204818461408\n",
      " Iter: 5 - Alpha: 0.047 - Batch 138/249 - Min Loss: 11.09 - Loss: 11.095193018052424\n",
      " Iter: 5 - Alpha: 0.047 - Batch 144/249 - Min Loss: 11.09 - Loss: 11.098132481204386\n",
      " Iter: 5 - Alpha: 0.047 - Batch 151/249 - Min Loss: 11.09 - Loss: 11.097156704652328\n",
      " Iter: 5 - Alpha: 0.047 - Batch 152/249 - Min Loss: 11.09 - Loss: 11.092200273859248\n",
      " Iter: 5 - Alpha: 0.047 - Batch 153/249 - Min Loss: 11.08 - Loss: 11.082827801071083\n",
      " Iter: 5 - Alpha: 0.047 - Batch 210/249 - Min Loss: 11.07 - Loss: 11.087775167099853\n",
      " Iter: 5 - Alpha: 0.047 - Batch 211/249 - Min Loss: 11.06 - Loss: 11.068648800583086\n",
      " Iter: 5 - Alpha: 0.047 - Batch 212/249 - Min Loss: 11.06 - Loss: 11.062998578798657\n",
      " Iter: 5 - Alpha: 0.047 - Batch 213/249 - Min Loss: 11.05 - Loss: 11.056464465453088\n",
      " Iter: 5 - Alpha: 0.047 - Batch 214/249 - Min Loss: 11.05 - Loss: 11.054792622082447\n",
      " Iter: 5 - Alpha: 0.047 - Batch 215/249 - Min Loss: 11.04 - Loss: 11.044146354140743\n",
      " Iter: 5 - Alpha: 0.047 - Batch 217/249 - Min Loss: 11.03 - Loss: 11.038798454989841\n",
      " Iter: 5 - Alpha: 0.047 - Batch 218/249 - Min Loss: 11.03 - Loss: 11.032320298206503\n",
      " Iter: 5 - Alpha: 0.047 - Batch 221/249 - Min Loss: 11.02 - Loss: 11.025173032923048\n",
      " Iter: 5 - Alpha: 0.047 - Batch 222/249 - Min Loss: 11.01 - Loss: 11.018962364811527\n",
      " Iter: 5 - Alpha: 0.047 - Batch 223/249 - Min Loss: 11.01 - Loss: 11.014145989503271\n",
      " Iter: 5 - Alpha: 0.047 - Batch 224/249 - Min Loss: 11.00 - Loss: 11.008840137865752\n",
      " Iter: 5 - Alpha: 0.047 - Batch 225/249 - Min Loss: 11.00 - Loss: 11.004025733571504\n",
      " Iter: 5 - Alpha: 0.047 - Batch 226/249 - Min Loss: 11.00 - Loss: 11.000022299031082\n",
      " Iter: 5 - Alpha: 0.047 - Batch 227/249 - Min Loss: 10.99 - Loss: 10.9949918047774\n",
      " Iter: 5 - Alpha: 0.047 - Batch 230/249 - Min Loss: 10.99 - Loss: 10.995747727514248\n",
      " Iter: 5 - Alpha: 0.047 - Batch 231/249 - Min Loss: 10.99 - Loss: 10.992005024366001\n",
      " Iter: 5 - Alpha: 0.047 - Batch 233/249 - Min Loss: 10.99 - Loss: 10.99653820655546\n",
      " Iter: 5 - Alpha: 0.047 - Batch 234/249 - Min Loss: 10.98 - Loss: 10.98858920522252\n",
      " Iter: 5 - Alpha: 0.047 - Batch 235/249 - Min Loss: 10.98 - Loss: 10.98052036308039\n",
      " Iter: 5 - Alpha: 0.047 - Batch 236/249 - Min Loss: 10.97 - Loss: 10.971130525999842\n",
      " Iter: 5 - Alpha: 0.047 - Batch 237/249 - Min Loss: 10.96 - Loss: 10.963102767814364\n",
      " Iter: 5 - Alpha: 0.047 - Batch 240/249 - Min Loss: 10.95 - Loss: 10.963939107226068\n",
      " Iter: 6 - Alpha: 0.047 - Batch 33/249 - Min Loss: 10.95 - Loss: 10.986452744420898- Theat, Theat, Thefers, and seers, and seesteres, and seesteres, and se\n",
      " Iter: 6 - Alpha: 0.047 - Batch 34/249 - Min Loss: 10.94 - Loss: 10.943107009144796\n",
      " Iter: 6 - Alpha: 0.047 - Batch 35/249 - Min Loss: 10.92 - Loss: 10.92148983300861\n",
      " Iter: 6 - Alpha: 0.047 - Batch 37/249 - Min Loss: 10.91 - Loss: 10.918669443192849\n",
      " Iter: 6 - Alpha: 0.047 - Batch 135/249 - Min Loss: 10.89 - Loss: 10.906303116715334\n",
      " Iter: 6 - Alpha: 0.047 - Batch 137/249 - Min Loss: 10.89 - Loss: 10.896781701096342\n",
      " Iter: 6 - Alpha: 0.047 - Batch 138/249 - Min Loss: 10.88 - Loss: 10.882872349346062\n",
      " Iter: 6 - Alpha: 0.047 - Batch 143/249 - Min Loss: 10.88 - Loss: 10.888905638755124\n",
      " Iter: 6 - Alpha: 0.047 - Batch 144/249 - Min Loss: 10.87 - Loss: 10.879635013643934\n",
      " Iter: 6 - Alpha: 0.047 - Batch 152/249 - Min Loss: 10.87 - Loss: 10.876863508290757\n",
      " Iter: 6 - Alpha: 0.047 - Batch 153/249 - Min Loss: 10.87 - Loss: 10.870124933464808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 6 - Alpha: 0.047 - Batch 210/249 - Min Loss: 10.86 - Loss: 10.866298504270977\n",
      " Iter: 6 - Alpha: 0.047 - Batch 211/249 - Min Loss: 10.84 - Loss: 10.846925342841716\n",
      " Iter: 6 - Alpha: 0.047 - Batch 212/249 - Min Loss: 10.84 - Loss: 10.842922488268236\n",
      " Iter: 6 - Alpha: 0.047 - Batch 213/249 - Min Loss: 10.83 - Loss: 10.834556386407353\n",
      " Iter: 6 - Alpha: 0.047 - Batch 214/249 - Min Loss: 10.83 - Loss: 10.832621570305616\n",
      " Iter: 6 - Alpha: 0.047 - Batch 215/249 - Min Loss: 10.82 - Loss: 10.82201604283038\n",
      " Iter: 6 - Alpha: 0.047 - Batch 217/249 - Min Loss: 10.81 - Loss: 10.817733838282743\n",
      " Iter: 6 - Alpha: 0.047 - Batch 218/249 - Min Loss: 10.81 - Loss: 10.812353340847514\n",
      " Iter: 6 - Alpha: 0.047 - Batch 219/249 - Min Loss: 10.80 - Loss: 10.800709262460657\n",
      " Iter: 6 - Alpha: 0.047 - Batch 221/249 - Min Loss: 10.79 - Loss: 10.802679268985246\n",
      " Iter: 6 - Alpha: 0.047 - Batch 222/249 - Min Loss: 10.79 - Loss: 10.796321266517808\n",
      " Iter: 6 - Alpha: 0.047 - Batch 223/249 - Min Loss: 10.79 - Loss: 10.79085584803634\n",
      " Iter: 6 - Alpha: 0.047 - Batch 224/249 - Min Loss: 10.78 - Loss: 10.786234356603043\n",
      " Iter: 6 - Alpha: 0.047 - Batch 225/249 - Min Loss: 10.78 - Loss: 10.781608548343366\n",
      " Iter: 6 - Alpha: 0.047 - Batch 226/249 - Min Loss: 10.77 - Loss: 10.7782145072127\n",
      " Iter: 6 - Alpha: 0.047 - Batch 227/249 - Min Loss: 10.77 - Loss: 10.774126437188857\n",
      " Iter: 6 - Alpha: 0.047 - Batch 234/249 - Min Loss: 10.77 - Loss: 10.772931185390455\n",
      " Iter: 6 - Alpha: 0.047 - Batch 235/249 - Min Loss: 10.76 - Loss: 10.766088828638035\n",
      " Iter: 6 - Alpha: 0.047 - Batch 236/249 - Min Loss: 10.75 - Loss: 10.757409107686943\n",
      " Iter: 6 - Alpha: 0.047 - Batch 237/249 - Min Loss: 10.75 - Loss: 10.750550933000248\n",
      " Iter: 7 - Alpha: 0.046 - Batch 137/249 - Min Loss: 10.74 - Loss: 10.756582381656179 That thers,fer Thesteres, and seesteres, That seesteres, But thers, an\n",
      " Iter: 7 - Alpha: 0.046 - Batch 138/249 - Min Loss: 10.74 - Loss: 10.742105479284646\n",
      " Iter: 7 - Alpha: 0.046 - Batch 142/249 - Min Loss: 10.73 - Loss: 10.739644957839074\n",
      " Iter: 7 - Alpha: 0.046 - Batch 143/249 - Min Loss: 10.72 - Loss: 10.728352003306144\n",
      " Iter: 7 - Alpha: 0.046 - Batch 144/249 - Min Loss: 10.71 - Loss: 10.717310615794364\n",
      " Iter: 7 - Alpha: 0.046 - Batch 152/249 - Min Loss: 10.71 - Loss: 10.713902631925119\n",
      " Iter: 7 - Alpha: 0.046 - Batch 153/249 - Min Loss: 10.70 - Loss: 10.707055209037634\n",
      " Iter: 7 - Alpha: 0.046 - Batch 208/249 - Min Loss: 10.69 - Loss: 10.703767694582039\n",
      " Iter: 7 - Alpha: 0.046 - Batch 209/249 - Min Loss: 10.69 - Loss: 10.698136381517495\n",
      " Iter: 7 - Alpha: 0.046 - Batch 210/249 - Min Loss: 10.69 - Loss: 10.690796604567598\n",
      " Iter: 7 - Alpha: 0.046 - Batch 211/249 - Min Loss: 10.67 - Loss: 10.672062675827066\n",
      " Iter: 7 - Alpha: 0.046 - Batch 212/249 - Min Loss: 10.66 - Loss: 10.667529697247737\n",
      " Iter: 7 - Alpha: 0.046 - Batch 213/249 - Min Loss: 10.65 - Loss: 10.658905054367256\n",
      " Iter: 7 - Alpha: 0.046 - Batch 214/249 - Min Loss: 10.65 - Loss: 10.65804293025807\n",
      " Iter: 7 - Alpha: 0.046 - Batch 215/249 - Min Loss: 10.64 - Loss: 10.648053620191058\n",
      " Iter: 7 - Alpha: 0.046 - Batch 217/249 - Min Loss: 10.64 - Loss: 10.645921408635544\n",
      " Iter: 7 - Alpha: 0.046 - Batch 218/249 - Min Loss: 10.64 - Loss: 10.640807645886504\n",
      " Iter: 7 - Alpha: 0.046 - Batch 219/249 - Min Loss: 10.63 - Loss: 10.630177351359626\n",
      " Iter: 7 - Alpha: 0.046 - Batch 221/249 - Min Loss: 10.62 - Loss: 10.629920950073716\n",
      " Iter: 7 - Alpha: 0.046 - Batch 222/249 - Min Loss: 10.62 - Loss: 10.623391827496583\n",
      " Iter: 7 - Alpha: 0.046 - Batch 223/249 - Min Loss: 10.61 - Loss: 10.617796556247365\n",
      " Iter: 7 - Alpha: 0.046 - Batch 224/249 - Min Loss: 10.61 - Loss: 10.61384444311609\n",
      " Iter: 7 - Alpha: 0.046 - Batch 225/249 - Min Loss: 10.61 - Loss: 10.61020515296059\n",
      " Iter: 7 - Alpha: 0.046 - Batch 226/249 - Min Loss: 10.60 - Loss: 10.608519744783127\n",
      " Iter: 7 - Alpha: 0.046 - Batch 235/249 - Min Loss: 10.60 - Loss: 10.609300973646843\n",
      " Iter: 7 - Alpha: 0.046 - Batch 236/249 - Min Loss: 10.59 - Loss: 10.598939095857322\n",
      " Iter: 7 - Alpha: 0.046 - Batch 237/249 - Min Loss: 10.59 - Loss: 10.59284133858429\n",
      " Iter: 7 - Alpha: 0.046 - Batch 240/249 - Min Loss: 10.58 - Loss: 10.595341560457937\n",
      " Iter: 7 - Alpha: 0.046 - Batch 241/249 - Min Loss: 10.58 - Loss: 10.587721153920848\n",
      " Iter: 7 - Alpha: 0.046 - Batch 242/249 - Min Loss: 10.58 - Loss: 10.586079414549662\n",
      " Iter: 8 - Alpha: 0.046 - Batch 33/249 - Min Loss: 10.58 - Loss: 10.619638283315041- That therferes, and seest and seest and seest as ates, But theres, and\n",
      " Iter: 8 - Alpha: 0.046 - Batch 34/249 - Min Loss: 10.58 - Loss: 10.58152128910036\n",
      " Iter: 8 - Alpha: 0.046 - Batch 35/249 - Min Loss: 10.56 - Loss: 10.56272465496398\n",
      " Iter: 8 - Alpha: 0.046 - Batch 36/249 - Min Loss: 10.54 - Loss: 10.544589192241862\n",
      " Iter: 8 - Alpha: 0.046 - Batch 37/249 - Min Loss: 10.53 - Loss: 10.531117426248489\n",
      " Iter: 8 - Alpha: 0.046 - Batch 40/249 - Min Loss: 10.49 - Loss: 10.533244945712818\n",
      " Iter: 8 - Alpha: 0.046 - Batch 41/249 - Min Loss: 10.48 - Loss: 10.480595705303253\n",
      " Iter: 8 - Alpha: 0.046 - Batch 42/249 - Min Loss: 10.47 - Loss: 10.478337948715257\n",
      " Iter: 8 - Alpha: 0.046 - Batch 234/249 - Min Loss: 10.47 - Loss: 10.476385214569413\n",
      " Iter: 8 - Alpha: 0.046 - Batch 235/249 - Min Loss: 10.46 - Loss: 10.468993344486353\n",
      " Iter: 8 - Alpha: 0.046 - Batch 236/249 - Min Loss: 10.46 - Loss: 10.46272949323079\n",
      " Iter: 9 - Alpha: 0.045 - Batch 40/249 - Min Loss: 10.46 - Loss: 10.505696370843078- That therferert, feat thefferert, feat thefferferert, Bet theer That t\n",
      " Iter: 9 - Alpha: 0.045 - Batch 41/249 - Min Loss: 10.44 - Loss: 10.44781839163438\n",
      " Iter: 9 - Alpha: 0.045 - Batch 42/249 - Min Loss: 10.44 - Loss: 10.441666221185736\n",
      " Iter: 9 - Alpha: 0.045 - Batch 50/249 - Min Loss: 10.43 - Loss: 10.452924319766614\n",
      " Iter: 9 - Alpha: 0.045 - Batch 56/249 - Min Loss: 10.41 - Loss: 10.431758134313824\n",
      " Iter: 9 - Alpha: 0.045 - Batch 57/249 - Min Loss: 10.40 - Loss: 10.408671546220377\n",
      " Iter: 9 - Alpha: 0.045 - Batch 151/249 - Min Loss: 10.40 - Loss: 10.404639878163593\n",
      " Iter: 9 - Alpha: 0.045 - Batch 152/249 - Min Loss: 10.40 - Loss: 10.401886350841707\n",
      " Iter: 9 - Alpha: 0.045 - Batch 153/249 - Min Loss: 10.39 - Loss: 10.39537270502187\n",
      " Iter: 9 - Alpha: 0.045 - Batch 215/249 - Min Loss: 10.38 - Loss: 10.389718557365056\n",
      " Iter: 9 - Alpha: 0.045 - Batch 217/249 - Min Loss: 10.38 - Loss: 10.38819479690048\n",
      " Iter: 9 - Alpha: 0.045 - Batch 218/249 - Min Loss: 10.38 - Loss: 10.381185306188247\n",
      " Iter: 9 - Alpha: 0.045 - Batch 219/249 - Min Loss: 10.37 - Loss: 10.372800920305178\n",
      " Iter: 9 - Alpha: 0.045 - Batch 221/249 - Min Loss: 10.37 - Loss: 10.372480690041352\n",
      " Iter: 9 - Alpha: 0.045 - Batch 222/249 - Min Loss: 10.36 - Loss: 10.367773491928927\n",
      " Iter: 9 - Alpha: 0.045 - Batch 223/249 - Min Loss: 10.36 - Loss: 10.361872373866886\n",
      " Iter: 9 - Alpha: 0.045 - Batch 224/249 - Min Loss: 10.35 - Loss: 10.357724526896812\n",
      " Iter: 9 - Alpha: 0.045 - Batch 225/249 - Min Loss: 10.35 - Loss: 10.354160790654731\n",
      " Iter: 9 - Alpha: 0.045 - Batch 226/249 - Min Loss: 10.35 - Loss: 10.35356648139704\n",
      " Iter: 9 - Alpha: 0.045 - Batch 235/249 - Min Loss: 10.35 - Loss: 10.351331459664506\n",
      " Iter: 9 - Alpha: 0.045 - Batch 236/249 - Min Loss: 10.34 - Loss: 10.343809924276027\n",
      " Iter: 9 - Alpha: 0.045 - Batch 237/249 - Min Loss: 10.33 - Loss: 10.337919695808655\n",
      " Iter: 9 - Alpha: 0.045 - Batch 240/249 - Min Loss: 10.33 - Loss: 10.341196278994598\n",
      " Iter: 9 - Alpha: 0.045 - Batch 241/249 - Min Loss: 10.33 - Loss: 10.333305156051251\n",
      " Iter: 9 - Alpha: 0.045 - Batch 242/249 - Min Loss: 10.33 - Loss: 10.331701811936673\n",
      " Iter: 10 - Alpha: 0.045 - Batch 52/249 - Min Loss: 10.32 - Loss: 10.330126260967193 That ther theeffererf theefffererf theer That ther theer That ther the\n",
      " Iter: 10 - Alpha: 0.045 - Batch 54/249 - Min Loss: 10.31 - Loss: 10.325357265158557\n",
      " Iter: 10 - Alpha: 0.045 - Batch 55/249 - Min Loss: 10.31 - Loss: 10.312315766640832\n",
      " Iter: 10 - Alpha: 0.045 - Batch 56/249 - Min Loss: 10.28 - Loss: 10.289527446769945\n",
      " Iter: 10 - Alpha: 0.045 - Batch 57/249 - Min Loss: 10.27 - Loss: 10.271376725401213\n",
      " Iter: 10 - Alpha: 0.045 - Batch 150/249 - Min Loss: 10.27 - Loss: 10.271246704885957\n",
      " Iter: 10 - Alpha: 0.045 - Batch 151/249 - Min Loss: 10.26 - Loss: 10.267236052369064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 10 - Alpha: 0.045 - Batch 152/249 - Min Loss: 10.26 - Loss: 10.263607573233973\n",
      " Iter: 10 - Alpha: 0.045 - Batch 153/249 - Min Loss: 10.25 - Loss: 10.257945986492956\n",
      " Iter: 10 - Alpha: 0.045 - Batch 212/249 - Min Loss: 10.25 - Loss: 10.260385440876602\n",
      " Iter: 10 - Alpha: 0.045 - Batch 214/249 - Min Loss: 10.25 - Loss: 10.253597499303641\n",
      " Iter: 10 - Alpha: 0.045 - Batch 215/249 - Min Loss: 10.24 - Loss: 10.243117692118542\n",
      " Iter: 10 - Alpha: 0.045 - Batch 217/249 - Min Loss: 10.23 - Loss: 10.239965837480952\n",
      " Iter: 10 - Alpha: 0.045 - Batch 218/249 - Min Loss: 10.23 - Loss: 10.231948811898656\n",
      " Iter: 10 - Alpha: 0.045 - Batch 219/249 - Min Loss: 10.22 - Loss: 10.22363493118894\n",
      " Iter: 10 - Alpha: 0.045 - Batch 221/249 - Min Loss: 10.22 - Loss: 10.224309623481847\n",
      " Iter: 10 - Alpha: 0.045 - Batch 222/249 - Min Loss: 10.22 - Loss: 10.220053919845013\n",
      " Iter: 10 - Alpha: 0.045 - Batch 223/249 - Min Loss: 10.21 - Loss: 10.214408286137623\n",
      " Iter: 10 - Alpha: 0.045 - Batch 224/249 - Min Loss: 10.21 - Loss: 10.210208055016563\n",
      " Iter: 10 - Alpha: 0.045 - Batch 225/249 - Min Loss: 10.20 - Loss: 10.206234493491719\n",
      " Iter: 10 - Alpha: 0.045 - Batch 226/249 - Min Loss: 10.20 - Loss: 10.206161433007853\n",
      " Iter: 10 - Alpha: 0.045 - Batch 227/249 - Min Loss: 10.20 - Loss: 10.201119937836431\n",
      " Iter: 10 - Alpha: 0.045 - Batch 228/249 - Min Loss: 10.20 - Loss: 10.20077010324659\n",
      " Iter: 10 - Alpha: 0.045 - Batch 234/249 - Min Loss: 10.19 - Loss: 10.200848751344724\n",
      " Iter: 10 - Alpha: 0.045 - Batch 235/249 - Min Loss: 10.19 - Loss: 10.19514681829138\n",
      " Iter: 10 - Alpha: 0.045 - Batch 236/249 - Min Loss: 10.19 - Loss: 10.190865848056006\n",
      " Iter: 10 - Alpha: 0.045 - Batch 237/249 - Min Loss: 10.18 - Loss: 10.187170694377338\n",
      " Iter: 10 - Alpha: 0.045 - Batch 242/249 - Min Loss: 10.18 - Loss: 10.190283766229056\n",
      " Iter: 11 - Alpha: 0.044 - Batch 3/249 - Min Loss: 10.18 - Loss: 10.255473340695318 - That seeffer That ther thement and sees,fefer That seent and sees, and\n",
      " Iter: 11 - Alpha: 0.044 - Batch 217/249 - Min Loss: 10.18 - Loss: 10.191799209310174\n",
      " Iter: 11 - Alpha: 0.044 - Batch 218/249 - Min Loss: 10.18 - Loss: 10.183503839608143\n",
      " Iter: 11 - Alpha: 0.044 - Batch 219/249 - Min Loss: 10.17 - Loss: 10.174638225065813\n",
      " Iter: 11 - Alpha: 0.044 - Batch 221/249 - Min Loss: 10.17 - Loss: 10.175257506339214\n",
      " Iter: 11 - Alpha: 0.044 - Batch 222/249 - Min Loss: 10.17 - Loss: 10.171540570300824\n",
      " Iter: 11 - Alpha: 0.044 - Batch 223/249 - Min Loss: 10.16 - Loss: 10.16630846917967\n",
      " Iter: 11 - Alpha: 0.044 - Batch 224/249 - Min Loss: 10.16 - Loss: 10.161957198739126\n",
      " Iter: 11 - Alpha: 0.044 - Batch 226/249 - Min Loss: 10.15 - Loss: 10.158809983263065\n",
      " Iter: 11 - Alpha: 0.044 - Batch 228/249 - Min Loss: 10.15 - Loss: 10.154356554495781\n",
      " Iter: 11 - Alpha: 0.044 - Batch 236/249 - Min Loss: 10.15 - Loss: 10.153628198049711\n",
      " Iter: 11 - Alpha: 0.044 - Batch 237/249 - Min Loss: 10.15 - Loss: 10.15097943709616\n",
      " Iter: 14 - Alpha: 0.043 - Batch 2/249 - Min Loss: 10.14 - Loss: 10.253164647838325 - That seer That fres, seest fant ffeer That seer That seeffes, shat see\n",
      " Iter: 14 - Alpha: 0.043 - Batch 3/249 - Min Loss: 10.11 - Loss: 10.115823621139418\n",
      " Iter: 51 - Alpha: 0.029 - Batch 245/249 - Min Loss: 10.07 - Loss: 15.534339431403238 That makes enterestas an every en the dabftart stan peat I kn makes en"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-0d284b3c8005>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations, min_loss)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cross_entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"index_select\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b295ed38fa1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"transpose\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Creator's Github\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,1000000000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                    print(self.id)\n",
    "                    print(self.creation_op)\n",
    "                    print(len(self.creators))\n",
    "                    for c in self.creators:\n",
    "                        print(c.creation_op)\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_bias = bias\n",
    "        \n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        if(self.use_bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "        if(self.use_bias):        \n",
    "            self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if(self.use_bias):\n",
    "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "        return input.mm(self.weight)\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "\n",
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "class MSELoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        dif = input - target\n",
    "        return (dif * dif).sum(0)\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "    \n",
    "class LSTMCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "\n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)        \n",
    "        self.xc = Linear(n_inputs, n_hidden)        \n",
    "        \n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()        \n",
    "        self.parameters += self.ho.get_parameters()        \n",
    "        self.parameters += self.hc.get_parameters()                \n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        prev_hidden = hidden[0]        \n",
    "        prev_cell = hidden[1]\n",
    "        \n",
    "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
    "        c = (f * prev_cell) + (i * g)\n",
    "\n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_hidden.data[:,0] += 1\n",
    "        init_cell.data[:,0] += 1\n",
    "        return (init_hidden, init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Редактировать метаданные",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
